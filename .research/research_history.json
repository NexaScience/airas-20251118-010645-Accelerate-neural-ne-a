{
  "research_topic": "Accelerate neural network training",
  "queries": [
    "mixed precision training",
    "gradient accumulation",
    "distributed data parallel",
    "learning rate scheduling",
    "neural network pruning"
  ],
  "research_study_list": [
    {
      "title": "Collage: Light-Weight Low-Precision Strategy for LLM Training",
      "abstract": "Large models training is plagued by the intense compute cost and limited hardware memory. A practical solution is low-precision representation but is troubled by loss in numerical accuracy and unstable training rendering the model less useful. We argue that low-precision floating points can perform well provided the error is properly compensated at the critical locations in the training process. We propose Collage which utilizes multi-component float representation in low-precision to accurately perform operations with numerical errors accounted. To understand the impact of imprecision to training, we propose a simple and novel metric which tracks the lost information during training as well as differentiates various precision strategies. Our method works with commonly used low-precision such as half-precision ($16$-bit floating points) and can be naturally extended to work with even lower precision such as $8$-bit. Experimental results show that pre-training using Collage removes the requirement of using $32$-bit floating-point copies of the model and attains similar/better training performance compared to $(16, 32)$-bit mixed-precision strategy, with up to $3.7\\times$ speedup and $\\sim 15\\%$ to $23\\%$ less memory usage in practice.",
      "meta_data": {
        "arxiv_id": "2405.03637v1",
        "authors": [
          "Tao Yu",
          "Gaurav Gupta",
          "Karthick Gopalswamy",
          "Amith Mamidala",
          "Hao Zhou",
          "Jeffrey Huynh",
          "Youngsuk Park",
          "Ron Diamant",
          "Anoop Deoras",
          "Luke Huan"
        ],
        "published_date": "2024-05-06T16:55:30Z",
        "pdf_url": "https://arxiv.org/pdf/2405.03637v1.pdf"
      }
    },
    {
      "title": "Guaranteed Approximation Bounds for Mixed-Precision Neural Operators",
      "abstract": "Neural operators, such as Fourier Neural Operators (FNO), form a principled approach for learning solution operators for PDEs and other mappings between function spaces. However, many real-world problems require high-resolution training data, and the training time and limited GPU memory pose big barriers. One solution is to train neural operators in mixed precision to reduce the memory requirement and increase training speed. However, existing mixed-precision training techniques are designed for standard neural networks, and we find that their direct application to FNO leads to numerical overflow and poor memory efficiency. Further, at first glance, it may appear that mixed precision in FNO will lead to drastic accuracy degradation since reducing the precision of the Fourier transform yields poor results in classical numerical solvers. We show that this is not the case; in fact, we prove that reducing the precision in FNO still guarantees a good approximation bound, when done in a targeted manner. Specifically, we build on the intuition that neural operator learning inherently induces an approximation error, arising from discretizing the infinite-dimensional ground-truth input function, implying that training in full precision is not needed. We formalize this intuition by rigorously characterizing the approximation and precision errors of FNO and bounding these errors for general input functions. We prove that the precision error is asymptotically comparable to the approximation error. Based on this, we design a simple method to optimize the memory-intensive half-precision tensor contractions by greedily finding the optimal contraction order. Through extensive experiments on different state-of-the-art neural operators, datasets, and GPUs, we demonstrate that our approach reduces GPU memory usage by up to 50% and improves throughput by 58% with little or no reduction in accuracy.",
      "meta_data": {
        "arxiv_id": "2307.15034v3",
        "authors": [
          "Renbo Tu",
          "Colin White",
          "Jean Kossaifi",
          "Boris Bonev",
          "Nikola Kovachki",
          "Gennady Pekhimenko",
          "Kamyar Azizzadenesheli",
          "Anima Anandkumar"
        ],
        "published_date": "2023-07-27T17:42:06Z",
        "pdf_url": "https://arxiv.org/pdf/2307.15034v3.pdf"
      }
    },
    {
      "title": "Shifted and Squeezed 8-bit Floating Point format for Low-Precision Training of Deep Neural Networks",
      "abstract": "Training with larger number of parameters while keeping fast iterations is an increasingly adopted strategy and trend for developing better performing Deep Neural Network (DNN) models. This necessitates increased memory footprint and computational requirements for training. Here we introduce a novel methodology for training deep neural networks using 8-bit floating point (FP8) numbers. Reduced bit precision allows for a larger effective memory and increased computational speed. We name this method Shifted and Squeezed FP8 (S2FP8). We show that, unlike previous 8-bit precision training methods, the proposed method works out-of-the-box for representative models: ResNet-50, Transformer and NCF. The method can maintain model accuracy without requiring fine-tuning loss scaling parameters or keeping certain layers in single precision. We introduce two learnable statistics of the DNN tensors - shifted and squeezed factors that are used to optimally adjust the range of the tensors in 8-bits, thus minimizing the loss in information due to quantization.",
      "meta_data": {
        "arxiv_id": "2001.05674v1",
        "authors": [
          "Léopold Cambier",
          "Anahita Bhiwandiwalla",
          "Ting Gong",
          "Mehran Nekuii",
          "Oguz H Elibol",
          "Hanlin Tang"
        ],
        "published_date": "2020-01-16T06:38:27Z",
        "pdf_url": "https://arxiv.org/pdf/2001.05674v1.pdf"
      }
    },
    {
      "title": "Structured Inverse-Free Natural Gradient Descent: Memory-Efficient & Numerically-Stable KFAC",
      "abstract": "Second-order methods such as KFAC can be useful for neural net training. However, they are often memory-inefficient since their preconditioning Kronecker factors are dense, and numerically unstable in low precision as they require matrix inversion or decomposition. These limitations render such methods unpopular for modern mixed-precision training. We address them by (i) formulating an inverse-free KFAC update and (ii) imposing structures in the Kronecker factors, resulting in structured inverse-free natural gradient descent (SINGD). On modern neural networks, we show that SINGD is memory-efficient and numerically robust, in contrast to KFAC, and often outperforms AdamW even in half precision. Our work closes a gap between first- and second-order methods in modern low-precision training.",
      "meta_data": {
        "arxiv_id": "2312.05705v4",
        "authors": [
          "Wu Lin",
          "Felix Dangel",
          "Runa Eschenhagen",
          "Kirill Neklyudov",
          "Agustinus Kristiadi",
          "Richard E. Turner",
          "Alireza Makhzani"
        ],
        "published_date": "2023-12-09T23:13:32Z",
        "pdf_url": "https://arxiv.org/pdf/2312.05705v4.pdf"
      }
    },
    {
      "title": "Multi-Precision Policy Enforced Training (MuPPET) : A Precision-Switching Strategy for Quantised Fixed-Point Training of CNNs",
      "abstract": "Large-scale convolutional neural networks (CNNs) suffer from very long training times, spanning from hours to weeks, limiting the productivity and experimentation of deep learning practitioners. As networks grow in size and complexity, training time can be reduced through low-precision data representations and computations. However, in doing so the final accuracy suffers due to the problem of vanishing gradients. Existing state-of-the-art methods combat this issue by means of a mixed-precision approach utilising two different precision levels, FP32 (32-bit floating-point) and FP16/FP8 (16-/8-bit floating-point), leveraging the hardware support of recent GPU architectures for FP16 operations to obtain performance gains. This work pushes the boundary of quantised training by employing a multilevel optimisation approach that utilises multiple precisions including low-precision fixed-point representations. The novel training strategy, MuPPET, combines the use of multiple number representation regimes together with a precision-switching mechanism that decides at run time the transition point between precision regimes. Overall, the proposed strategy tailors the training process to the hardware-level capabilities of the target hardware architecture and yields improvements in training time and energy efficiency compared to state-of-the-art approaches. Applying MuPPET on the training of AlexNet, ResNet18 and GoogLeNet on ImageNet (ILSVRC12) and targeting an NVIDIA Turing GPU, MuPPET achieves the same accuracy as standard full-precision training with training-time speedup of up to 1.84$\\times$ and an average speedup of 1.58$\\times$ across the networks.",
      "meta_data": {
        "arxiv_id": "2006.09049v1",
        "authors": [
          "Aditya Rajagopal",
          "Diederik Adriaan Vink",
          "Stylianos I. Venieris",
          "Christos-Savvas Bouganis"
        ],
        "published_date": "2020-06-16T10:14:36Z",
        "pdf_url": "https://arxiv.org/pdf/2006.09049v1.pdf"
      }
    },
    {
      "title": "Towards Cheaper Inference in Deep Networks with Lower Bit-Width Accumulators",
      "abstract": "The majority of the research on the quantization of Deep Neural Networks (DNNs) is focused on reducing the precision of tensors visible by high-level frameworks (e.g., weights, activations, and gradients). However, current hardware still relies on high-accuracy core operations. Most significant is the operation of accumulating products. This high-precision accumulation operation is gradually becoming the main computational bottleneck. This is because, so far, the usage of low-precision accumulators led to a significant degradation in performance. In this work, we present a simple method to train and fine-tune high-end DNNs, to allow, for the first time, utilization of cheaper, $12$-bits accumulators, with no significant degradation in accuracy. Lastly, we show that as we decrease the accumulation precision further, using fine-grained gradient approximations can improve the DNN accuracy.",
      "meta_data": {
        "arxiv_id": "2401.14110v1",
        "authors": [
          "Yaniv Blumenfeld",
          "Itay Hubara",
          "Daniel Soudry"
        ],
        "published_date": "2024-01-25T11:46:01Z",
        "pdf_url": "https://arxiv.org/pdf/2401.14110v1.pdf"
      }
    },
    {
      "title": "Accumulated Decoupled Learning with Gradient Staleness Mitigation for Convolutional Neural Networks",
      "abstract": "Decoupled learning is a branch of model parallelism which parallelizes the training of a network by splitting it depth-wise into multiple modules. Techniques from decoupled learning usually lead to stale gradient effect because of their asynchronous implementation, thereby causing performance degradation. In this paper, we propose an accumulated decoupled learning (ADL) which incorporates the gradient accumulation technique to mitigate the stale gradient effect. We give both theoretical and empirical evidences regarding how the gradient staleness can be reduced. We prove that the proposed method can converge to critical points, i.e., the gradients converge to 0, in spite of its asynchronous nature. Empirical validation is provided by training deep convolutional neural networks to perform classification tasks on CIFAR-10 and ImageNet datasets. The ADL is shown to outperform several state-of-the-arts in the classification tasks, and is the fastest among the compared methods.",
      "meta_data": {
        "arxiv_id": "2012.03747v1",
        "authors": [
          "Huiping Zhuang",
          "Zhiping Lin",
          "Kar-Ann Toh"
        ],
        "published_date": "2020-12-03T11:52:55Z",
        "pdf_url": "https://arxiv.org/pdf/2012.03747v1.pdf"
      }
    },
    {
      "title": "Neural gradients are near-lognormal: improved quantized  and sparse training",
      "abstract": "While training can mostly be accelerated by reducing the time needed to propagate neural gradients back throughout the model, most previous works focus on the quantization/pruning of weights and activations. These methods are often not applicable to neural gradients, which have very different statistical properties. Distinguished from weights and activations, we find that the distribution of neural gradients is approximately lognormal. Considering this, we suggest two closed-form analytical methods to reduce the computational and memory burdens of neural gradients. The first method optimizes the floating-point format and scale of the gradients. The second method accurately sets sparsity thresholds for gradient pruning. Each method achieves state-of-the-art results on ImageNet. To the best of our knowledge, this paper is the first to (1) quantize the gradients to 6-bit floating-point formats, or (2) achieve up to 85% gradient sparsity -- in each case without accuracy degradation. Reference implementation accompanies the paper.",
      "meta_data": {
        "arxiv_id": "2006.08173v3",
        "authors": [
          "Brian Chmiel",
          "Liad Ben-Uri",
          "Moran Shkolnik",
          "Elad Hoffer",
          "Ron Banner",
          "Daniel Soudry"
        ],
        "published_date": "2020-06-15T07:00:15Z",
        "pdf_url": "https://arxiv.org/pdf/2006.08173v3.pdf"
      }
    },
    {
      "title": "Stabilizing Backpropagation Through Time to Learn Complex Physics",
      "abstract": "Of all the vector fields surrounding the minima of recurrent learning setups, the gradient field with its exploding and vanishing updates appears a poor choice for optimization, offering little beyond efficient computability. We seek to improve this suboptimal practice in the context of physics simulations, where backpropagating feedback through many unrolled time steps is considered crucial to acquiring temporally coherent behavior. The alternative vector field we propose follows from two principles: physics simulators, unlike neural networks, have a balanced gradient flow, and certain modifications to the backpropagation pass leave the positions of the original minima unchanged. As any modification of backpropagation decouples forward and backward pass, the rotation-free character of the gradient field is lost. Therefore, we discuss the negative implications of using such a rotational vector field for optimization and how to counteract them. Our final procedure is easily implementable via a sequence of gradient stopping and component-wise comparison operations, which do not negatively affect scalability. Our experiments on three control problems show that especially as we increase the complexity of each task, the unbalanced updates from the gradient can no longer provide the precise control signals necessary while our method still solves the tasks. Our code can be found at https://github.com/tum-pbs/StableBPTT.",
      "meta_data": {
        "arxiv_id": "2405.02041v1",
        "authors": [
          "Patrick Schnell",
          "Nils Thuerey"
        ],
        "published_date": "2024-05-03T12:20:08Z",
        "pdf_url": "https://arxiv.org/pdf/2405.02041v1.pdf"
      }
    },
    {
      "title": "Extrapolation for Large-batch Training in Deep Learning",
      "abstract": "Deep learning networks are typically trained by Stochastic Gradient Descent (SGD) methods that iteratively improve the model parameters by estimating a gradient on a very small fraction of the training data. A major roadblock faced when increasing the batch size to a substantial fraction of the training data for improving training time is the persistent degradation in performance (generalization gap). To address this issue, recent work propose to add small perturbations to the model parameters when computing the stochastic gradients and report improved generalization performance due to smoothing effects. However, this approach is poorly understood; it requires often model-specific noise and fine-tuning. To alleviate these drawbacks, we propose to use instead computationally efficient extrapolation (extragradient) to stabilize the optimization trajectory while still benefiting from smoothing to avoid sharp minima. This principled approach is well grounded from an optimization perspective and we show that a host of variations can be covered in a unified framework that we propose. We prove the convergence of this novel scheme and rigorously evaluate its empirical performance on ResNet, LSTM, and Transformer. We demonstrate that in a variety of experiments the scheme allows scaling to much larger batch sizes than before whilst reaching or surpassing SOTA accuracy.",
      "meta_data": {
        "arxiv_id": "2006.05720v1",
        "authors": [
          "Tao Lin",
          "Lingjing Kong",
          "Sebastian U. Stich",
          "Martin Jaggi"
        ],
        "published_date": "2020-06-10T08:22:41Z",
        "pdf_url": "https://arxiv.org/pdf/2006.05720v1.pdf"
      }
    },
    {
      "title": "DRAGONN: Distributed Randomized Approximate Gradients of Neural Networks"
    },
    {
      "title": "DRAGONN: Distributed Randomized Approximate Gradients of Neural Networks"
    },
    {
      "title": "DD-PPO: Learning Near-Perfect PointGoal Navigators from 2.5 Billion Frames",
      "abstract": "We present Decentralized Distributed Proximal Policy Optimization (DD-PPO), a method for distributed reinforcement learning in resource-intensive simulated environments. DD-PPO is distributed (uses multiple machines), decentralized (lacks a centralized server), and synchronous (no computation is ever stale), making it conceptually simple and easy to implement. In our experiments on training virtual robots to navigate in Habitat-Sim, DD-PPO exhibits near-linear scaling -- achieving a speedup of 107x on 128 GPUs over a serial implementation. We leverage this scaling to train an agent for 2.5 Billion steps of experience (the equivalent of 80 years of human experience) -- over 6 months of GPU-time training in under 3 days of wall-clock time with 64 GPUs.\n  This massive-scale training not only sets the state of art on Habitat Autonomous Navigation Challenge 2019, but essentially solves the task --near-perfect autonomous navigation in an unseen environment without access to a map, directly from an RGB-D camera and a GPS+Compass sensor. Fortuitously, error vs computation exhibits a power-law-like distribution; thus, 90% of peak performance is obtained relatively early (at 100 million steps) and relatively cheaply (under 1 day with 8 GPUs). Finally, we show that the scene understanding and navigation policies learned can be transferred to other navigation tasks -- the analog of ImageNet pre-training + task-specific fine-tuning for embodied AI. Our model outperforms ImageNet pre-trained CNNs on these transfer tasks and can serve as a universal resource (all models and code are publicly available).",
      "meta_data": {
        "arxiv_id": "1911.00357v2",
        "authors": [
          "Erik Wijmans",
          "Abhishek Kadian",
          "Ari Morcos",
          "Stefan Lee",
          "Irfan Essa",
          "Devi Parikh",
          "Manolis Savva",
          "Dhruv Batra"
        ],
        "published_date": "2019-11-01T13:07:37Z",
        "pdf_url": "https://arxiv.org/pdf/1911.00357v2.pdf"
      }
    },
    {
      "title": "Towards a Better Theoretical Understanding of Independent Subnetwork Training",
      "abstract": "Modern advancements in large-scale machine learning would be impossible without the paradigm of data-parallel distributed computing. Since distributed computing with large-scale models imparts excessive pressure on communication channels, significant recent research has been directed toward co-designing communication compression strategies and training algorithms with the goal of reducing communication costs. While pure data parallelism allows better data scaling, it suffers from poor model scaling properties. Indeed, compute nodes are severely limited by memory constraints, preventing further increases in model size. For this reason, the latest achievements in training giant neural network models also rely on some form of model parallelism. In this work, we take a closer theoretical look at Independent Subnetwork Training (IST), which is a recently proposed and highly effective technique for solving the aforementioned problems. We identify fundamental differences between IST and alternative approaches, such as distributed methods with compressed communication, and provide a precise analysis of its optimization performance on a quadratic model.",
      "meta_data": {
        "arxiv_id": "2306.16484v2",
        "authors": [
          "Egor Shulgin",
          "Peter Richtárik"
        ],
        "published_date": "2023-06-28T18:14:22Z",
        "pdf_url": "https://arxiv.org/pdf/2306.16484v2.pdf"
      }
    },
    {
      "title": "Debiasing Distributed Second Order Optimization with Surrogate Sketching and Scaled Regularization",
      "abstract": "In distributed second order optimization, a standard strategy is to average many local estimates, each of which is based on a small sketch or batch of the data. However, the local estimates on each machine are typically biased, relative to the full solution on all of the data, and this can limit the effectiveness of averaging. Here, we introduce a new technique for debiasing the local estimates, which leads to both theoretical and empirical improvements in the convergence rate of distributed second order methods. Our technique has two novel components: (1) modifying standard sketching techniques to obtain what we call a surrogate sketch; and (2) carefully scaling the global regularization parameter for local computations. Our surrogate sketches are based on determinantal point processes, a family of distributions for which the bias of an estimate of the inverse Hessian can be computed exactly. Based on this computation, we show that when the objective being minimized is $l_2$-regularized with parameter $λ$ and individual machines are each given a sketch of size $m$, then to eliminate the bias, local estimates should be computed using a shrunk regularization parameter given by $λ^{\\prime}=λ\\cdot(1-\\frac{d_λ}{m})$, where $d_λ$ is the $λ$-effective dimension of the Hessian (or, for quadratic problems, the data matrix).",
      "meta_data": {
        "arxiv_id": "2007.01327v1",
        "authors": [
          "Michał Dereziński",
          "Burak Bartan",
          "Mert Pilanci",
          "Michael W. Mahoney"
        ],
        "published_date": "2020-07-02T18:08:14Z",
        "pdf_url": "https://arxiv.org/pdf/2007.01327v1.pdf"
      }
    },
    {
      "title": "AutoLRS: Automatic Learning-Rate Schedule by Bayesian Optimization on the Fly",
      "abstract": "The learning rate (LR) schedule is one of the most important hyper-parameters needing careful tuning in training DNNs. However, it is also one of the least automated parts of machine learning systems and usually costs significant manual effort and computing. Though there are pre-defined LR schedules and optimizers with adaptive LR, they introduce new hyperparameters that need to be tuned separately for different tasks/datasets. In this paper, we consider the question: Can we automatically tune the LR over the course of training without human involvement? We propose an efficient method, AutoLRS, which automatically optimizes the LR for each training stage by modeling training dynamics. AutoLRS aims to find an LR applied to every $τ$ steps that minimizes the resulted validation loss. We solve this black-box optimization on the fly by Bayesian optimization (BO). However, collecting training instances for BO requires a system to evaluate each LR queried by BO's acquisition function for $τ$ steps, which is prohibitively expensive in practice. Instead, we apply each candidate LR for only $τ'\\llτ$ steps and train an exponential model to predict the validation loss after $τ$ steps. This mutual-training process between BO and the loss-prediction model allows us to limit the training steps invested in the BO search. We demonstrate the advantages and the generality of AutoLRS through extensive experiments of training DNNs for tasks from diverse domains using different optimizers. The LR schedules auto-generated by AutoLRS lead to a speedup of $1.22\\times$, $1.43\\times$, and $1.5\\times$ when training ResNet-50, Transformer, and BERT, respectively, compared to the LR schedules in their original papers, and an average speedup of $1.31\\times$ over state-of-the-art heavily-tuned LR schedules.",
      "meta_data": {
        "arxiv_id": "2105.10762v1",
        "authors": [
          "Yuchen Jin",
          "Tianyi Zhou",
          "Liangyu Zhao",
          "Yibo Zhu",
          "Chuanxiong Guo",
          "Marco Canini",
          "Arvind Krishnamurthy"
        ],
        "published_date": "2021-05-22T16:41:10Z",
        "pdf_url": "https://arxiv.org/pdf/2105.10762v1.pdf"
      }
    },
    {
      "title": "Budgeted Training: Rethinking Deep Neural Network Training Under Resource Constraints",
      "abstract": "In most practical settings and theoretical analyses, one assumes that a model can be trained until convergence. However, the growing complexity of machine learning datasets and models may violate such assumptions. Indeed, current approaches for hyper-parameter tuning and neural architecture search tend to be limited by practical resource constraints. Therefore, we introduce a formal setting for studying training under the non-asymptotic, resource-constrained regime, i.e., budgeted training. We analyze the following problem: \"given a dataset, algorithm, and fixed resource budget, what is the best achievable performance?\" We focus on the number of optimization iterations as the representative resource. Under such a setting, we show that it is critical to adjust the learning rate schedule according to the given budget. Among budget-aware learning schedules, we find simple linear decay to be both robust and high-performing. We support our claim through extensive experiments with state-of-the-art models on ImageNet (image classification), Kinetics (video classification), MS COCO (object detection and instance segmentation), and Cityscapes (semantic segmentation). We also analyze our results and find that the key to a good schedule is budgeted convergence, a phenomenon whereby the gradient vanishes at the end of each allowed budget. We also revisit existing approaches for fast convergence and show that budget-aware learning schedules readily outperform such approaches under (the practical but under-explored) budgeted training setting.",
      "meta_data": {
        "arxiv_id": "1905.04753v4",
        "authors": [
          "Mengtian Li",
          "Ersin Yumer",
          "Deva Ramanan"
        ],
        "published_date": "2019-05-12T17:49:49Z",
        "pdf_url": "https://arxiv.org/pdf/1905.04753v4.pdf"
      }
    },
    {
      "title": "Learning Rate Schedules in the Presence of Distribution Shift",
      "abstract": "We design learning rate schedules that minimize regret for SGD-based online learning in the presence of a changing data distribution. We fully characterize the optimal learning rate schedule for online linear regression via a novel analysis with stochastic differential equations. For general convex loss functions, we propose new learning rate schedules that are robust to distribution shift and we give upper and lower bounds for the regret that only differ by constants. For non-convex loss functions, we define a notion of regret based on the gradient norm of the estimated models and propose a learning schedule that minimizes an upper bound on the total expected regret. Intuitively, one expects changing loss landscapes to require more exploration, and we confirm that optimal learning rate schedules typically increase in the presence of distribution shift. Finally, we provide experiments for high-dimensional regression models and neural networks to illustrate these learning rate schedules and their cumulative regret.",
      "meta_data": {
        "arxiv_id": "2303.15634v2",
        "authors": [
          "Matthew Fahrbach",
          "Adel Javanmard",
          "Vahab Mirrokni",
          "Pratik Worah"
        ],
        "published_date": "2023-03-27T23:29:02Z",
        "venue": "Proceedings of the 40th International Conference on Machine Learning (ICML 2023) 9523-9546",
        "pdf_url": "https://arxiv.org/pdf/2303.15634v2.pdf"
      }
    },
    {
      "title": "Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations",
      "abstract": "Scale has become a main ingredient in obtaining strong machine learning models. As a result, understanding a model's scaling properties is key to effectively designing both the right training setup as well as future generations of architectures. In this work, we argue that scale and training research has been needlessly complex due to reliance on the cosine schedule, which prevents training across different lengths for the same model size. We investigate the training behavior of a direct alternative -- constant learning rate and cooldowns -- and find that it scales predictably and reliably similar to cosine. Additionally, we show that stochastic weight averaging yields improved performance along the training trajectory, without additional training costs, across different scales. Importantly, with these findings we demonstrate that scaling experiments can be performed with significantly reduced compute and GPU hours by utilizing fewer but reusable training runs. Our code is available at \\url{https://github.com/epfml/schedules-and-scaling/}.",
      "meta_data": {
        "arxiv_id": "2405.18392v3",
        "authors": [
          "Alexander Hägele",
          "Elie Bakouch",
          "Atli Kosson",
          "Loubna Ben Allal",
          "Leandro Von Werra",
          "Martin Jaggi"
        ],
        "published_date": "2024-05-28T17:33:54Z",
        "pdf_url": "https://arxiv.org/pdf/2405.18392v3.pdf"
      }
    },
    {
      "title": "Stepping on the Edge: Curvature Aware Learning Rate Tuners",
      "abstract": "Curvature information -- particularly, the largest eigenvalue of the loss Hessian, known as the sharpness -- often forms the basis for learning rate tuners. However, recent work has shown that the curvature information undergoes complex dynamics during training, going from a phase of increasing sharpness to eventual stabilization. We analyze the closed-loop feedback effect between learning rate tuning and curvature. We find that classical learning rate tuners may yield greater one-step loss reduction, yet they ultimately underperform in the long term when compared to constant learning rates in the full batch regime. These models break the stabilization of the sharpness, which we explain using a simplified model of the joint dynamics of the learning rate and the curvature. To further investigate these effects, we introduce a new learning rate tuning method, Curvature Dynamics Aware Tuning (CDAT), which prioritizes long term curvature stabilization over instantaneous progress on the objective. In the full batch regime, CDAT shows behavior akin to prefixed warm-up schedules on deep learning objectives, outperforming tuned constant learning rates. In the mini batch regime, we observe that stochasticity introduces confounding effects that explain the previous success of some learning rate tuners at appropriate batch sizes. Our findings highlight the critical role of understanding the joint dynamics of the learning rate and curvature, beyond greedy minimization, to diagnose failures and design effective adaptive learning rate tuners.",
      "meta_data": {
        "arxiv_id": "2407.06183v1",
        "authors": [
          "Vincent Roulet",
          "Atish Agarwala",
          "Jean-Bastien Grill",
          "Grzegorz Swirszcz",
          "Mathieu Blondel",
          "Fabian Pedregosa"
        ],
        "published_date": "2024-07-08T17:56:00Z",
        "pdf_url": "https://arxiv.org/pdf/2407.06183v1.pdf"
      }
    },
    {
      "title": "Fantastic Weights and How to Find Them: Where to Prune in Dynamic Sparse Training",
      "abstract": "Dynamic Sparse Training (DST) is a rapidly evolving area of research that seeks to optimize the sparse initialization of a neural network by adapting its topology during training. It has been shown that under specific conditions, DST is able to outperform dense models. The key components of this framework are the pruning and growing criteria, which are repeatedly applied during the training process to adjust the network's sparse connectivity. While the growing criterion's impact on DST performance is relatively well studied, the influence of the pruning criterion remains overlooked. To address this issue, we design and perform an extensive empirical analysis of various pruning criteria to better understand their impact on the dynamics of DST solutions. Surprisingly, we find that most of the studied methods yield similar results. The differences become more significant in the low-density regime, where the best performance is predominantly given by the simplest technique: magnitude-based pruning. The code is provided at https://github.com/alooow/fantastic_weights_paper",
      "meta_data": {
        "arxiv_id": "2306.12230v2",
        "authors": [
          "Aleksandra I. Nowak",
          "Bram Grooten",
          "Decebal Constantin Mocanu",
          "Jacek Tabor"
        ],
        "published_date": "2023-06-21T12:43:55Z",
        "pdf_url": "https://arxiv.org/pdf/2306.12230v2.pdf"
      }
    },
    {
      "title": "Neuron-level Structured Pruning using Polarization Regularizer"
    },
    {
      "title": "The Combinatorial Brain Surgeon: Pruning Weights That Cancel One Another in Neural Networks",
      "abstract": "Neural networks tend to achieve better accuracy with training if they are larger -- even if the resulting models are overparameterized. Nevertheless, carefully removing such excess parameters before, during, or after training may also produce models with similar or even improved accuracy. In many cases, that can be curiously achieved by heuristics as simple as removing a percentage of the weights with the smallest absolute value -- even though magnitude is not a perfect proxy for weight relevance. With the premise that obtaining significantly better performance from pruning depends on accounting for the combined effect of removing multiple weights, we revisit one of the classic approaches for impact-based pruning: the Optimal Brain Surgeon(OBS). We propose a tractable heuristic for solving the combinatorial extension of OBS, in which we select weights for simultaneous removal, as well as a systematic update of the remaining weights. Our selection method outperforms other methods under high sparsity, and the weight update is advantageous even when combined with the other methods.",
      "meta_data": {
        "arxiv_id": "2203.04466v3",
        "authors": [
          "Xin Yu",
          "Thiago Serra",
          "Srikumar Ramalingam",
          "Shandian Zhe"
        ],
        "published_date": "2022-03-09T00:58:04Z",
        "pdf_url": "https://arxiv.org/pdf/2203.04466v3.pdf"
      }
    },
    {
      "title": "The Combinatorial Brain Surgeon: Pruning Weights That Cancel One Another in Neural Networks",
      "abstract": "Neural networks tend to achieve better accuracy with training if they are larger -- even if the resulting models are overparameterized. Nevertheless, carefully removing such excess parameters before, during, or after training may also produce models with similar or even improved accuracy. In many cases, that can be curiously achieved by heuristics as simple as removing a percentage of the weights with the smallest absolute value -- even though magnitude is not a perfect proxy for weight relevance. With the premise that obtaining significantly better performance from pruning depends on accounting for the combined effect of removing multiple weights, we revisit one of the classic approaches for impact-based pruning: the Optimal Brain Surgeon(OBS). We propose a tractable heuristic for solving the combinatorial extension of OBS, in which we select weights for simultaneous removal, as well as a systematic update of the remaining weights. Our selection method outperforms other methods under high sparsity, and the weight update is advantageous even when combined with the other methods.",
      "meta_data": {
        "arxiv_id": "2203.04466v3",
        "authors": [
          "Xin Yu",
          "Thiago Serra",
          "Srikumar Ramalingam",
          "Shandian Zhe"
        ],
        "published_date": "2022-03-09T00:58:04Z",
        "pdf_url": "https://arxiv.org/pdf/2203.04466v3.pdf"
      }
    },
    {
      "title": "Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers",
      "abstract": "We present a novel network pruning algorithm called Dynamic Sparse Training that can jointly find the optimal network parameters and sparse network structure in a unified optimization process with trainable pruning thresholds. These thresholds can have fine-grained layer-wise adjustments dynamically via backpropagation. We demonstrate that our dynamic sparse training algorithm can easily train very sparse neural network models with little performance loss using the same number of training epochs as dense models. Dynamic Sparse Training achieves the state of the art performance compared with other sparse training algorithms on various network architectures. Additionally, we have several surprising observations that provide strong evidence for the effectiveness and efficiency of our algorithm. These observations reveal the underlying problems of traditional three-stage pruning algorithms and present the potential guidance provided by our algorithm to the design of more compact network architectures.",
      "meta_data": {
        "arxiv_id": "2005.06870v1",
        "authors": [
          "Junjie Liu",
          "Zhe Xu",
          "Runbin Shi",
          "Ray C. C. Cheung",
          "Hayden K. H. So"
        ],
        "published_date": "2020-05-14T11:05:21Z",
        "pdf_url": "https://arxiv.org/pdf/2005.06870v1.pdf"
      }
    }
  ],
  "evaluated_hypothesis_history": [
    {
      "hypothesis": {
        "open_problems": "State-of-the-art image‐classification models trained with Stochastic Gradient Descent + Momentum (SGD-M) still need hand-crafted learning-rate schedules (step, cosine, warm-up etc.).  Wrong schedules either make early updates too cautious (slow convergence) or too large (divergence), so many epochs are spent on tuning rather than learning.  A minimal, automatic mechanism that keeps the learning rate large while the loss is high—and shrinks it only when the loss actually falls—could accelerate training with almost no algorithmic overhead.",
        "method": "Loss-Scaled Learning Rate (LSLR)\n1. Keep the standard SGD-M optimizer.\n2. Replace the fixed/cosine schedule by a per-mini-batch scaling factor derived from the current supervised loss.\n   η_t = η_max * clamp[(L_t / L_0)^γ , η_min / η_max , 1]\n   • L_0 : loss on the very first mini-batch (constant)\n   • L_t : loss on the current mini-batch (no gradient flows through it)\n   • γ   : smoothing exponent (default γ = 1/2)\n   • clamp[…] limits the scaled LR to [η_min , η_max] to avoid extreme values.\n3. No changes to the forward / backward pass, no extra hyper-parameters except γ (can be fixed).  The LR automatically decays as soon as the optimiser makes real progress, enabling aggressive early steps but fine updates later.",
        "experimental_setup": "Task: Image classification on CIFAR-10.\nModel: ResNet-18.\nBaselines: (a) SGD-M with a standard Step(30,60,80) schedule, (b) SGD-M with Cosine schedule.\nProposed: SGD-M + LSLR (no manual schedule).\nBudget: 60 epochs, batch size 128, identical data augmentations.\nEvaluation: Report Top-1 accuracy every epoch; wall-clock time is identical because per-step cost is unchanged.",
        "primary_metric": "accuracy",
        "experimental_code": "# core change only\nclass LSLRWrapper(torch.optim.Optimizer):\n    def __init__(self, base_opt, eta_max, eta_min=1e-5, gamma=0.5):\n        self.opt = base_opt\n        self.eta_max, self.eta_min = eta_max, eta_min\n        self.gamma = gamma\n        self.L0 = None  # will be set after first batch\n    @torch.no_grad()\n    def step(self, loss_value):\n        if self.L0 is None:\n            self.L0 = loss_value.item()\n        scale = (loss_value.item() / self.L0) ** self.gamma\n        scaled_lr = max(self.eta_min, min(self.eta_max, self.eta_max * scale))\n        for g in self.opt.param_groups:\n            g[\"lr\"] = scaled_lr\n        self.opt.step()\n    def zero_grad(self):\n        self.opt.zero_grad()\n\n# usage inside training loop\nopt_base = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\nopt = LSLRWrapper(opt_base, eta_max=0.1)\nfor images, labels in loader:\n    out = model(images)\n    loss = criterion(out, labels)\n    loss.backward()\n    opt.step(loss)  # pass current loss value\n    opt.zero_grad()",
        "expected_result": "After 60 epochs:\n• Baseline Step schedule ~89% accuracy.\n• Baseline Cosine schedule ~90% accuracy.\n• Proposed LSLR           ~91–92% accuracy.\nTime-to-90% accuracy decreases from ~55 epochs (Step) and ~48 epochs (Cosine) to ~40 epochs with LSLR, demonstrating faster convergence without additional computation.",
        "expected_conclusion": "A single-line, loss-aware scaling of the learning rate removes the need for hand-crafted schedules.  Because the LR remains high only while the objective is high, the optimiser makes larger, more useful steps early on and automatically fine-tunes later, yielding faster convergence and better final accuracy with negligible implementation effort."
      },
      "evaluation": {
        "novelty_reason": "Explicitly scaling the global learning-rate by a smooth power of the instantaneous supervised loss is not part of the standard arsenal of LR policies.  Existing adaptive methods (Adam, AdaGrad, RMSProp, LARS, LAMB) modify parameter-wise updates using gradient-moment statistics rather than the value of the loss itself.  Classical schedule automation such as ReduceLROnPlateau, AutoLR, One-Cycle or population-based training does look at loss/accuracy, but (1) decisions are taken only every few epochs, (2) they require additional hyper-parameters (patience, cooldown, percentage drop, cycle length, etc.) or auxiliary workers, and (3) they change the LR multiplicatively by fixed factors instead of tying it continuously to “how far the model still is from the optimum”.  Work closest in spirit is AdaLoss (Smith & Topin 2017) and recent \"LossGrad\" tricks that rescale gradients by loss magnitude, yet they still depend on moving averages and extra tuning knobs and are rarely used with plain SGD-M.  The proposed LSLR differs by:\n• one-line analytical rule η_t = η_max·(L_t/L_0)^γ clamped to a small range;\n• operating at every mini-batch with no history, extra buffers, or look-ahead; and\n• leaving the update direction completely untouched so it can be dropped into any existing SGD code.\nBecause Related Works is empty, there is no published method that exactly matches these three properties, so the idea has moderate, not full, novelty.",
        "novelty_score": 7,
        "significance_reason": "Choosing and tuning learning-rate schedules is still the most time-consuming and expertise-heavy part of training large-scale vision, language and multimodal models.  An automatic rule that (a) matches or beats hand-engineered schedules, (b) incurs literally zero computational overhead, and (c) keeps the beloved simplicity/robustness of SGD-M would save dozens of GPU-hours in academic labs and even more in industrial hyper-parameter sweeps.  Academically it provides a new angle—loss-value feedback rather than gradient statistics—on adaptive optimisation and could stimulate analysis of convergence under loss-dependent step sizes.  Societally, faster convergence translates to lower energy consumption and carbon footprint during the ever-larger pre-training runs that dominate ML deployment costs.  The potential impact is therefore high, although the baseline improvement (≈1–2 pp accuracy, 15–25 % faster to target accuracy on CIFAR-10) is shown only on a small-scale dataset; large-model validation is still needed.  Hence the significance is strong but not transformative.",
        "significance_score": 8
      }
    },
    {
      "hypothesis": {
        "open_problems": "Even with well-established optimisers such as SGD-Momentum, practitioners still have to hand-design BOTH the learning-rate schedule and the momentum coefficient.  A too-aggressive learning rate or too-high momentum early in training leads to divergence, while conservative choices waste many epochs.  Existing automatic schedulers either (a) change the learning rate only every few epochs, (b) introduce several extra hyper-parameters (patience, factor, cooldown, cycle length, etc.), or (c) leave momentum untouched.  A unified, single-line rule that simultaneously adapts the learning rate AND the momentum at every mini-batch, using only information already available during forward/backward passes, could eliminate the last major manual knob in vanilla SGD training and further cut time-to-target-accuracy as well as energy consumption.",
        "method": "Instantaneous Loss-Adaptive Gradient Scaling (ILAGS)\n1. Keep the standard weight update of SGD with momentum:  v_{t+1}=m_t·v_t+g_t ,  w_{t+1}=w_t-η_t·v_{t+1} .\n2. Replace static schedules for both learning rate (η) and momentum (m) by a shared scaling factor derived from the current supervised loss L_t.\n   • Let L̂_t be the exponential moving median of the most recent M mini-batch losses (robust to outliers).  Default M=100.\n   • Compute s_t=(L_t/L̂_t)^γ with a fixed γ=1/2.  s_t>1 means the current loss is above the recent typical value (we are off-track), s_t<1 means we are making progress.\n3. Update hyper-parameters on-the-fly, clamped to safe ranges:\n   η_t   = clamp(η_max·s_t ,   η_min , η_max)\n   m_t   = clamp(m_max·(2-s_t) , m_min , m_max)   # inverse behaviour: high momentum only when loss is already low\n   Recommended defaults: η_max=0.1, η_min=1e-4, m_max=0.9, m_min=0.5.\n4. No extra forward/backward passes, no gradient-moment statistics, and only one new integer hyper-parameter (window size M) that is not sensitive in practice.\n5. Because both step size and momentum react instantly to whether the network is currently struggling or cruising, ILAGS encourages fast exploration early (large LR, small momentum) and precise optimisation later (small LR, high momentum) – automatically.",
        "experimental_setup": "Tasks & Models:\n• CIFAR-10 / ResNet-18 (quick sanity check, 60 epochs)\n• CIFAR-100 / Wide-ResNet-28-10 (120 epochs)\n• ImageNet-1k / ResNet-50 (90 epochs, 8×V100)\nBaselines:\n1. SGD-Momentum + Step (30-60-80) schedule\n2. SGD-Momentum + Cosine schedule\n3. SGD-Momentum + One-Cycle (LR only; momentum schedule hand-tuned)\nProposed: SGD-Momentum + ILAGS (no manual schedules)\nProtocol: identical data augmentation, batch size 128 (CIFAR) or 256 (ImageNet), weight decay 5e-4.\nMetrics recorded every epoch: top-1 accuracy, top-1 accuracy vs wall-clock time, energy drawn (nvidia-smi).",
        "primary_metric": "Wall-clock time to reach a target top-1 accuracy (e.g. 90% for CIFAR-10, 75% for ImageNet).",
        "experimental_code": "class ILAGS(torch.optim.Optimizer):\n    def __init__(self, params, eta_max=0.1, eta_min=1e-4,\n                 m_max=0.9, m_min=0.5, gamma=0.5, window=100):\n        self.opt = torch.optim.SGD(params, lr=eta_max, momentum=m_max)\n        self.eta_max, self.eta_min = eta_max, eta_min\n        self.m_max,  self.m_min  = m_max,  m_min\n        self.gamma, self.window  = gamma, window\n        self.loss_buffer = []      # store last ‑window- losses\n    @torch.no_grad()\n    def step(self, loss_val):\n        # 1. robust reference loss (moving median)\n        self.loss_buffer.append(loss_val.item())\n        if len(self.loss_buffer) > self.window:\n            self.loss_buffer.pop(0)\n        L_hat = torch.median(torch.tensor(self.loss_buffer)).item()\n        # 2. scaling factor\n        s = (loss_val.item() / (L_hat + 1e-12)) ** self.gamma\n        # 3. adapt LR and momentum\n        lr = max(self.eta_min, min(self.eta_max, self.eta_max * s))\n        mom = max(self.m_min,  min(self.m_max, self.m_max * (2 - s)))\n        for pg in self.opt.param_groups:\n            pg['lr'] = lr\n            pg['momentum'] = mom\n        # 4. perform actual update\n        self.opt.step()\n    def zero_grad(self):\n        self.opt.zero_grad()",
        "expected_result": "Across all three datasets:\n• ILAGS reaches the target accuracy 20-35% faster than Cosine schedule and 30-45% faster than Step schedule.\n• Final top-1 accuracy is on par with, or up to +0.5 pp better than, One-Cycle while saving the need to tune both LR and momentum curves.\n• GPU energy consumption to reach the target accuracy drops by ≈25% relative to Cosine.\nAblations show that adapting LR alone recovers only ~60% of the speed-up, and adapting momentum alone ~30%, confirming the synergy.",
        "expected_conclusion": "By tying both learning-rate and momentum to the instantaneous position of the current loss relative to a short, robust history, ILAGS removes the last manual hyper-parameter of vanilla SGD training.  The method is optimiser-agnostic, costs no extra computation, and delivers consistent speed-ups and energy savings from small-scale vision benchmarks to ImageNet-size workloads.  This demonstrates that loss-aware, per-batch hyper-parameter adaptation is a practical path to greener and more accessible deep-learning training."
      },
      "evaluation": {
        "novelty_reason": "The idea of dynamically adapting learning-rate and momentum is not new – YellowFin (Zhang et al., 2017), Hypergradient-SGD (Baydin et al., 2018), AdaMomentum (Chen & Gu, 2019) and several recent ‘self-tuning momentum’ papers all modify these two knobs on every mini-batch.  However, existing methods either (1) rely on gradient-variance or curvature estimates, (2) require additional hyper-parameters such as damping factors, or (3) adapt the two quantities independently.  The proposed ILAGS departs from prior art by:\n• Using only the scalar supervised loss, eliminating the need for any gradient-level statistics or Hessian approximation.\n• Tying LR and momentum through a single shared scaling factor s_t derived from the ratio between current loss and a moving median, yielding an extremely simple one-line rule with only one new integer hyper-parameter (window size).\n• Exploiting inverse coupling – larger LR but smaller momentum when the loss is high, and the reverse when the loss is low – which to our knowledge has not been formalised in a loss-ratio based scheduler before.\nBecause no specific related work employs this loss-ratio mechanism nor presents a fully hand-free ‘single line’ policy that co-controls both LR and momentum, the hypothesis offers moderate originality, albeit within a well-trodden research space.",
        "novelty_score": 7,
        "significance_reason": "Automatically eliminating manual tuning of both learning-rate schedules and momentum would lower the barrier for non-experts, shorten development cycles, and cut GPU energy use – all major practical concerns as models and datasets grow.  If the claimed 20–45 % reduction in wall-clock time and ≈25 % energy saving over strong baselines generalises, the impact would be noticeable for both academic labs and industry training clusters.  Academically, ILAGS could revitalize interest in loss-aware control signals and provide a simpler benchmark against which more complex adaptive optimisers are compared.  Societally, decreased energy consumption aligns with sustainability goals.  Nevertheless, the gains are incremental relative to a crowded field of adaptive optimisation, and benefits may diminish for tasks where Adam/LAMB are already dominant; thus the significance, while solid, is not transformative.",
        "significance_score": 7
      }
    },
    {
      "hypothesis": {
        "open_problems": "Current per-batch schedulers that try to accelerate SGD either (a) rely on hand-designed heuristics such as loss ratios (ILAGS, AdaLoss), (b) treat learning rate and momentum as independent knobs, or (c) ignore the fact that the *optimal* step size depends on how sharply the loss is expected to change *after* the update.  None of them use a forward-looking estimate of loss decrease that can be computed for free from values already produced by back-prop (loss and ‖g_t‖).  As a result, LR is often too small when the curvature is gentle and too large when the objective is steep, wasting epochs and energy.  A light-weight, self-identifying rule that fits a one–step quadratic model of the loss online and chooses the LR–momentum pair that is predicted to minimise the *next* loss could remove this mismatch and further cut time-to-accuracy across vision and language tasks.",
        "method": "Predictive Loss-Optimal Scheduler (PLOS)\n1. Keep the standard velocity update of SGD with momentum: v_{t+1}=m_t·v_t+g_t,  w_{t+1}=w_t-η_t·v_{t+1}.\n2. Online surrogate model.  For each step maintain a running estimate of the local quadratic along the velocity direction:\n   ΔL_{t}=L_t−L_{t−1},   ΔS_{t}=η_{t−1}·‖v_{t−1}‖^2/2.\n   Fit α_t in  ΔL_t≈−α_t·ΔS_t  by exponential moving average (window 100).  α_t≈1 means the previous step followed the local quadratic prediction closely; α_t>1 (<1) means the step was too small (too large).\n3. Predict next-step loss if we were to use step size η: L_pred(η)=L_t−α_t·η·‖v_t‖^2/2.\n4. Closed-form optimum in this one-dimensional model is η* = min(η_max, max(η_min, 2·L_t /(α_t·‖v_t‖^2))).\n5. Couple momentum inversely to η* to keep total kinetic energy constant:  m_t = clamp(1−κ·η*, m_min, m_max) with κ=0.5 (fixed).\n6. Update parameters with (η*, m_t).  Complexity: O(1).  New scalars: EMA rate β (default 0.05) and κ; both are robust.\nRationale: The online α_t turns the raw gradient norm into a calibrated predictor of how much loss decay one unit of velocity buys at the current point; η* picks the largest safe step that is expected to at least halve the loss surrogate.  Sharing the same η* to set m_t avoids conflicting dynamics.",
        "experimental_setup": "Datasets & models\n• CIFAR-10 / ResNet-18  (60 epochs)\n• CIFAR-100 / Wide-ResNet-28-10 (120 epochs)\n• ImageNet-1k / ResNet-50 (90 epochs, 8×V100)\n• WikiText-103 / Transformer-Decoder (100K steps)\nBaselines\n1. SGD-M + Cosine LR, fixed m=0.9\n2. SGD-M + One-Cycle (hand-tuned)\n3. YellowFin (auto LR & m using curvature)\nProposed: SGD-M + PLOS (single line)\nProtocol: identical data aug, batch 128/256 (vision) or 4096 tokens (NLP), weight decay 5e-4, no warm-up.\nMetrics recorded each epoch / 500 steps: top-1 accuracy or perplexity, wall-clock, energy (nvidia-smi).  Each run repeated 3× with different seeds.",
        "primary_metric": "Wall-clock time and energy (kWh) to reach a target quality: 90% CIFAR-10, 74% ImageNet, 26 perplexity on WikiText-103.",
        "experimental_code": "class PLOS(torch.optim.Optimizer):\n    def __init__(self, params, eta_max=0.2, eta_min=1e-4,\n                 m_max=0.9, m_min=0.5, beta=0.05, kappa=0.5):\n        self.opt = torch.optim.SGD(params, lr=eta_max, momentum=m_max)\n        self.eta_max, self.eta_min = eta_max, eta_min\n        self.m_max,  self.m_min  = m_max,  m_min\n        self.beta,  self.kappa   = beta,   kappa\n        self.alpha = 1.0  # surrogate coefficient\n        self.prev_loss = None\n        self.prev_step_energy = None  # η_{t-1} · ||v_{t-1}||^2 /2\n    @torch.no_grad()\n    def step(self, loss_val):\n        loss = loss_val.item()\n        # --- 1: update surrogate coefficient α_t ---\n        if self.prev_loss is not None and self.prev_step_energy is not None and self.prev_step_energy>0:\n            delta_L   = loss - self.prev_loss\n            alpha_est = - delta_L / (self.prev_step_energy + 1e-12)\n            self.alpha = (1-self.beta)*self.alpha + self.beta*alpha_est\n        # --- 2: compute gradient norms & candidate η* ---\n        v_norm2 = sum((p.grad.detach()**2).sum().item() for p in self.opt.param_groups[0]['params'])\n        eta_star = 2*loss / (self.alpha * (v_norm2 + 1e-12))\n        eta_star = max(self.eta_min, min(self.eta_max, eta_star))\n        # --- 3: momentum from kinetic-energy conservation ---\n        mom = 1 - self.kappa * eta_star\n        mom = max(self.m_min, min(self.m_max, mom))\n        for pg in self.opt.param_groups:\n            pg['lr'] = eta_star\n            pg['momentum'] = mom\n        # --- 4: perform update & store bookkeeping ---\n        self.opt.step()\n        self.prev_loss = loss\n        self.prev_step_energy = eta_star * v_norm2 / 2\n    def zero_grad(self):\n        self.opt.zero_grad()",
        "expected_result": "Across all tasks: \n• PLOS reaches the target quality 30–50% faster than Cosine and One-Cycle, and 15–25% faster than YellowFin.\n• Energy to target is reduced by ≈35% over Cosine and ≈20% over YellowFin.\n• Final accuracy/perplexity matches or slightly exceeds baselines (±0.3 pp / −0.2 ppl).\nAblation:\n   – Using η* without momentum coupling recovers ~70% of the speed-up.\n   – Replacing online α_t by a constant degrades both stability and final quality, confirming the benefit of self-identification.",
        "expected_conclusion": "Training can be accelerated further by looking one step ahead instead of reacting to past loss alone.  The proposed Predictive Loss-Optimal Scheduler fits, on-the-fly, a one-dimensional quadratic surrogate of the loss along the current update direction, then selects the learning-rate/momentum pair that minimises the predicted next loss – no Hessian, no extra back-prop, and only two robust scalars to set.  The resulting wall-clock and energy savings of up to 50% on vision and language benchmarks show that forward-looking, control-theoretic adaptation is a practical path to greener and more accessible deep learning training."
      },
      "evaluation": {
        "novelty_reason": "The scheduler departs from existing adaptive-LR methods in two ways:\n1. It builds a forward-looking quadratic surrogate of the loss *along the actual update direction* by re-using quantities that are already computed every step (current loss, previous loss, gradient-norm–weighted step energy).  None of the widely cited per-batch schedulers (YellowFin, AdaLoss, ILARS/ILAGS, AdaScale, Polyak LR) derive η from an on-line fit of ΔL≈−α·η‖v‖²/2; they either look at past curvature statistics, hand-crafted loss ratios, or require knowledge of the optimum value f*.\n2. It couples learning rate and momentum through a simple kinetic-energy conservation rule (m=1−κ·η) to avoid conflicting dynamics, whereas prior work tunes the two hyper-parameters independently or via heuristic grids.\nThe combination results in a closed-form, O(1) per-step policy that is completely self-calibrating (α_t updated by EMA) and requires no extra forward/backward pass—an approach not documented in the literature surveyed for adaptive scheduling or control-theoretic optimisers.",
        "novelty_score": 7,
        "significance_reason": "If the claimed 30–50 % reduction in wall-clock time and ≈35 % cut in energy hold across both vision and language benchmarks, the method would materially lower the compute cost and carbon footprint of standard training pipelines—an issue of growing societal concern.  Academically, it provides a principled yet practical bridge between classical line-search theory and mini-batch SGD, offering a testable, analytic link between loss change, gradient norm and optimal momentum.  Because it is lightweight (two scalar states, no extra kernels) it could be adopted in industrial scale code-bases without engineering overhead, potentially influencing a broad spectrum of research that still relies on hand-tuned cosine or one-cycle schedules.",
        "significance_score": 8
      }
    },
    {
      "hypothesis": {
        "open_problems": "Per-batch schedulers that adapt learning rate and momentum (e.g. YellowFin, AdaScale, PLOS) still suffer from two mutually-reinforcing inefficiencies: (1) they assume the mini-batch size is fixed, so when the gradient is very noisy they over-shrink the step size instead of simply collecting a larger batch; (2) when the gradient is already accurate they keep computing unnecessarily large batches, wasting FLOPs and energy.  None of the existing methods couples step-size, momentum and batch-size in a single control loop, even though all three quantities are governed by the same local geometry/noise trade-off.  A light-weight controller that (a) predicts the loss improvement of the NEXT update from quantities that are already available after back-prop, and (b) chooses the triad {learning-rate, momentum, effective batch} that is expected to minimise the next loss for least energy, could slash both wall-clock time and carbon cost across vision and language workloads.",
        "method": "Forward-Looking Adaptive Controller (FLAC)\n1. Keep the SGD-Momentum update: v_{t+1}=m_t v_t+g_t ;  w_{t+1}=w_t-η_t v_{t+1}.\n2. Directional quadratic fit (as in PLOS).\n   ΔL_t = L_t−L_{t−1}\n   ΔS_t = η_{t-1}‖v_{t-1}‖²/2\n   α_t  ←  EMA_β( −ΔL_t / (ΔS_t+ε) )            # online curvature / fit quality\n3. Predict loss after a candidate step of size η:\n   ˆL(η)=L_t − α_t η ‖v_t‖² /2.\n   Closed-form minimiser: η* = clamp( 2L_t /(α_t ‖v_t‖²) , η_min , η_max ).\n4. Momentum by kinetic-energy conservation:\n   m_t = clamp(1−κ η* , m_min , m_max).\n5. Noise–informed batch-size adaptation.\n   Prediction error   e_t = |ΔL_t + α_{t} ΔS_t|            # how much the quadratic fit missed\n   Normalised noise  n_t = e_t /( |ΔL_t|+ε ).               # ≈1 ⇒ predictable, >1 ⇒ noisy\n   Accumulation step k_{t+1}= clamp( k_t · n_t^γ , 1 , k_max ) with γ=0.5.\n   The effective batch for the next update is B_eff = B_base · k_{t+1} (implemented with gradient accumulation; no dataloader restart).\n   Intuition: if the last update behaved noisily (n_t>1) we amortise the cost of a safer, larger batch; if it was predictable we shrink the batch and save FLOPs.\n6. Complexity: O(1) scalar ops per update; two tunable constants β (default 0.05) and κ (0.5); all other bounds are routine (η_max, k_max etc.).",
        "experimental_setup": "Datasets & models\n• CIFAR-10 / ResNet-18  (60 real epochs, varying effective batch)\n• ImageNet-1k / ResNet-50 (90 epochs, 8×V100)\n• WikiText-103 / Transformer-Decoder (100k updates)\n• GLUE-MNLI / BERT-base pre-train-then-fine-tune (300k + 40k updates)\nBaselines\n1. SGD-M + Cosine LR + fixed batch  (industry default)\n2. SGD-M + One-Cycle (tuned) + fixed batch\n3. AdaScale (auto batch) + Cosine LR (tuned)\n4. PLOS (auto LR+m) + fixed batch\nProposed: FLAC (auto LR+m+batch)\nProtocol\n• Start with the SAME physical mini-batch B_base (128 CIFAR, 256 ImageNet, 2048 tokens NLP).\n• All methods run exactly the same number of parameter updates; only FLAC changes accumulation k on the fly.\nMetrics (logged every 500 steps)\n• Validation accuracy / perplexity\n• Wall-clock to target quality (90 % CIFAR-10, 74 % ImageNet, ppl 26 WT-103, 85 % MNLI)\n• Energy to target (kWh via nvidia-smi + power draw)\n• Total FLOPs (GPU util × time)",
        "primary_metric": "Energy (kWh) and wall-clock time to reach the target validation quality.",
        "experimental_code": "class FLAC(torch.optim.Optimizer):\n    def __init__(self, params, base_batch, eta_max=0.2, eta_min=1e-4,\n                 m_max=0.9, m_min=0.5, k_max=8, beta=0.05, kappa=0.5, gamma=0.5):\n        self.opt = torch.optim.SGD(params, lr=eta_max, momentum=m_max)\n        self.eta_max, self.eta_min = eta_max, eta_min\n        self.m_max,  self.m_min  = m_max,  m_min\n        self.k_max,  self.gamma  = k_max,  gamma\n        self.beta,   self.kappa  = beta,   kappa\n        self.alpha = 1.\n        self.prev_loss = None\n        self.prev_step_energy = None      # η_{t-1}‖v_{t-1}‖²/2\n        self.k_accum = 1                  # gradient-accumulation factor\n        self.base_batch = base_batch\n        self._counter = 0                 # steps accumulated so far\n    @torch.no_grad()\n    def begin_update(self, model):\n        for p in model.parameters():\n            if p.grad is not None:\n                p.grad.zero_()\n        self._counter = 0\n    @torch.no_grad()\n    def accumulate_grad(self):\n        self._counter += 1\n        return self._counter < self.k_accum\n    @torch.no_grad()\n    def step(self, loss_val):            # call only when counter == k_accum\n        loss = loss_val.item()\n        # 1. update α_t\n        if self.prev_loss is not None and self.prev_step_energy is not None and self.prev_step_energy>0:\n            delta_L   = loss - self.prev_loss\n            alpha_est = - delta_L / (self.prev_step_energy + 1e-12)\n            self.alpha = (1-self.beta)*self.alpha + self.beta*alpha_est\n        # 2. LR & momentum\n        v_norm2 = sum((p.grad.detach()**2).sum().item() for p in self.opt.param_groups[0]['params'])\n        eta_star = 2*loss / (self.alpha * (v_norm2 + 1e-12))\n        eta_star = max(self.eta_min, min(self.eta_max, eta_star))\n        mom = 1 - self.kappa * eta_star\n        mom = max(self.m_min, min(self.m_max, mom))\n        for pg in self.opt.param_groups:\n            pg['lr'] = eta_star\n            pg['momentum'] = mom\n        # 3. parameter update\n        self.opt.step()\n        # 4. noise-aware batch control\n        if self.prev_loss is not None:\n            delta_L = loss - self.prev_loss\n            err  = abs(delta_L + self.alpha * self.prev_step_energy)\n            noise = err / (abs(delta_L)+1e-12)\n            self.k_accum = int(min(self.k_max, max(1, round(self.k_accum * (noise ** self.gamma)))))\n        # 5. bookkeeping\n        self.prev_loss = loss\n        self.prev_step_energy = eta_star * v_norm2 / 2\n        self._counter = 0",
        "expected_result": "Across all four benchmarks:\n• FLAC reaches the target quality 45-60 % faster than Cosine schedule and 25-35 % faster than PLOS or AdaScale.\n• Energy to target drops by 50 % relative to Cosine and by 30 % relative to the best previous adaptive baseline.\n• Total theoretical FLOPs shrink by 35-40 % thanks to smaller batches early on.\n• Final accuracy/perplexity matches strong baselines within ±0.2 pp / −0.1 ppl.\nAblations\n1. Removing batch adaptation (fixed k=1) preserves only ≈65 % of the speed-up.\n2. Removing LR/momentum adaptation (keep Cosine) with batch control alone preserves ≈45 %.\n3. Using a constant α_t destabilises training on ImageNet, illustrating the need for the predictive surrogate.",
        "expected_conclusion": "Training speed and energy efficiency can be boosted further when the three key levers—learning rate, momentum and effective batch size—are governed by a single forward-looking controller.  FLAC fits a one-step quadratic loss model to reuse already-computed scalars, then simultaneously chooses the step size, momentum and how much additional data (via gradient accumulation) the next update should integrate.  This principled yet light-weight coupling halves energy-to-accuracy on diverse vision and language tasks, offering a practical path toward low-carbon, auto-tuning deep-learning pipelines that can be adopted with only a few lines of Python."
      },
      "evaluation": {
        "novelty_reason": "The hypothesis proposes a single-loop controller (FLAC) that simultaneously decides learning-rate, momentum and effective mini-batch size on every update, driven by a shared one-step quadratic loss surrogate fit from quantities already available after back-prop. Existing adaptive schedulers each optimise at most two of these levers: (i) YellowFin / PLOS co-adapt LR and momentum but assume a fixed batch; (ii) AdaScale and related dynamic-batch methods vary batch size while relying on an external hand-tuned LR schedule; (iii) no prior work links the prediction error of the loss model to batch-size scaling. Moreover, FLAC keeps computational overhead O(1) and works with gradient accumulation, avoiding data-loader restarts—an implementation detail not addressed by prior art. This coupling of three control variables via a forward-looking, closed-form objective appears absent from the literature, giving the approach clear novelty despite being built on well-known components.",
        "novelty_score": 8,
        "significance_reason": "If experimentally confirmed, FLAC would cut wall-clock time by 45–60 % and energy/kWh by ~50 % across vision and NLP benchmarks while maintaining accuracy. Such gains directly address the escalating carbon footprint of deep learning, providing practical benefit for both industry (reduced training cost, faster iteration) and society (lower environmental impact). Academically, it advances understanding of the interaction between optimisation hyper-parameters and stochastic gradient noise, offering a principled framework others can analyse or extend. The method’s simplicity (two new scalars, no extra forward/backward passes) makes adoption realistic, increasing potential impact beyond niche research settings.",
        "significance_score": 8
      }
    },
    {
      "hypothesis": {
        "open_problems": "State-of-the-art adaptive schedulers either (1) search for a good learning rate/momentum pair while keeping mini-batch size fixed, or (2) expand the batch when the gradient is noisy while relying on a hand-tuned schedule for step size and momentum.  None of them tries to *optimise the ratio between the loss decrease that the NEXT update is expected to deliver and the computational/energy cost of producing that update*.  Consequently, when the gradient is already accurate the optimiser often wastes FLOPs on unnecessarily large batches, and when the gradient is noisy it shrinks the step size instead of investing a few extra samples that would be cheaper per unit progress.  A controller that predicts (a) how much the loss will go down *if* we take a candidate step of size η after seeing B additional samples, and (b) how many joules that update will cost on the current hardware, could pick the triple {η, momentum m, effective batch B} that maximises “expected loss reduction per kWh”.  No existing method closes this energy-aware loop.",
        "method": "Energy-Aware Forward-Looking Adaptive Controller (E-FLAC)\n1. Keep the SGD-Momentum state: v_{t+1}=m_t v_t+g_t ;  w_{t+1}=w_t−η_t v_{t+1}.\n2. One–step quadratic surrogate (reuse PLOS idea).\n   ΔL_t=L_t−L_{t−1};  ΔS_t=η_{t−1}‖v_{t−1}‖²/2.\n   α_t ← EMA_β( −ΔL_t /(ΔS_t+ε) ).  # local curvature/fit quality.\n3. Predict loss after a candidate step of size η using current gradient norm g²=‖v_t‖²:\n      L̂(η)=L_t − α_t η g²/2.\n   Expected improvement: I(η)=L_t−L̂(η)=α_t η g²/2.\n4. Cheap noise scale.  Split the physical mini-batch (size B_phys) in two halves, accumulate g_a , g_b once per step, and compute\n      ρ_t = 1− ( ⟨g_a , g_b⟩ / (‖g_a‖‖g_b‖) ).  # ∈[0,1]; higher ⇒ noisier.\n   For an accumulated batch of effective size B = k·B_phys the noise contracts roughly as ρ_t /k.\n5. Joule model.  Measure once per training run the energy cost E_0 of a forward+backward pass on B_phys (via nvidia-smi).  Energy scales almost linearly with k, so E(k)=k·E_0.\n6. Objective for the next update: maximise   R(η,k)= I(η) / E(k)  subject to η∈[η_min,η_max] and k∈{1,…,k_max}.\n   • Since I(η) is linear in η, the optimum η* is always on a bound; choose\n         η* = min( η_max , 2L_t /(α_t g²) ).\n   • Optimal k* occurs where marginal gain from variance reduction balances extra energy:\n         k* = clamp( ceil( √( ρ_t η* g² / (2 α_t L_t) ) ) , 1 , k_max ).\n7. Momentum from discrete \"kinetic-energy budget\": m_t = clamp( 1 − κ η* , m_min , m_max ).\n8. Implement k* with gradient accumulation; no dataloader restart.\n9. Complexity: O(1) scalar ops + one extra dot product per step (to get ρ_t).\n   New constants: κ (0.5), β (0.05); all others are routine safety bounds.",
        "experimental_setup": "Benchmarks\n• Vision: CIFAR-10 / ResNet-18 (60 real epochs), ImageNet-1k / ResNet-50 (90 epochs, 8×V100)\n• Language: WikiText-103 / Transformer-Decoder (100k updates)\n• Multimodal: MS-COCO captioning / ViT-GPT2 (150k updates)\nBaselines\n1. SGD-M + Cosine LR + fixed batch\n2. SGD-M + One-Cycle (tuned) + fixed batch\n3. AdaScale (auto batch) + Cosine LR (tuned)\n4. PLOS (auto LR+m) + fixed batch\n5. FLAC (auto LR+m+batch, energy-agnostic)\nProposed: E-FLAC (energy-aware LR+m+batch)\nProtocol\n• All methods start from the same physical micro-batch B_phys (128 CIFAR, 256 ImageNet, 2048 tokens NLP).\n• Each run uses identical data pipelines, augmentations, AMP and weight decay.\n• E-FLAC measures E_0 once during warm-up and thereafter reads current power draw every 100 steps for logging only (not for control).\nMetrics\n• Primary: kWh and wall-clock time to reach target quality (90 % CIFAR-10, 74 % ImageNet, ppl 26 WT-103, CIDEr 108 COCO).\n• Secondary: total FLOPs, final accuracy/perplexity, extra memory.\nImplementation: <50 lines added to the PyTorch training loop; code released under MIT licence.",
        "primary_metric": "Energy consumed (kWh) to hit the target validation score; wall-clock time as tie-breaker.",
        "experimental_code": "# pseudocode sketch – full repo will be provided\nclass EFLAC(torch.optim.Optimizer):\n    def __init__(self, params, B_phys, E0, eta_max=0.2, eta_min=1e-4,\n                 m_max=0.9, m_min=0.5, k_max=8, beta=0.05, kappa=0.5):\n        self.opt = torch.optim.SGD(params, lr=eta_max, momentum=m_max)\n        self.B_phys, self.E0 = B_phys, E0\n        self.eta_max, self.eta_min = eta_max, eta_min\n        self.m_max,  self.m_min  = m_max,  m_min\n        self.k_max,  self.beta, self.kappa = k_max, beta, kappa\n        self.alpha = 1.\n        self.prev_loss = None; self.prev_step_energy = None\n        self.k_accum = 1; self._ctr = 0\n    @torch.no_grad()\n    def begin_update(self, model):\n        for p in model.parameters():\n            if p.grad is not None:\n                p.grad.zero_(); p._grad_store = None\n        self._ctr = 0\n    @torch.no_grad()\n    def accumulate_grad(self, g_half):\n        # g_half : gradient on half mini-batch, needed once to estimate noise\n        self._ctr += 1\n        if self._ctr == 1:\n            self.g_a = [g.clone() for g in g_half]\n        elif self._ctr == 2:\n            self.g_b = [g.clone() for g in g_half]\n        return self._ctr < self.k_accum\n    @torch.no_grad()\n    def step(self, loss_val):\n        loss = loss_val.item()\n        # 1. update surrogate α_t\n        if self.prev_loss is not None and self.prev_step_energy:\n            delta_L = loss - self.prev_loss\n            alpha_est = -delta_L / (self.prev_step_energy + 1e-12)\n            self.alpha = (1-self.beta)*self.alpha + self.beta*alpha_est\n        # 2. gradient norm and noise\n        g_norm2 = sum((p.grad.detach()**2).sum().item() for p in self.opt.param_groups[0]['params'])\n        if hasattr(self, 'g_a') and hasattr(self, 'g_b'):\n            dot = sum((ga*gb).sum().item() for ga,gb in zip(self.g_a,self.g_b))\n            norm_a = sum((ga**2).sum().item() for ga in self.g_a)**0.5\n            norm_b = sum((gb**2).sum().item() for gb in self.g_b)**0.5\n            rho = 1 - dot / (norm_a*norm_b + 1e-12)\n        else:\n            rho = 1.0\n        # 3. choose η*\n        eta_star = min(self.eta_max, max(self.eta_min, 2*loss / (self.alpha*(g_norm2+1e-12))))\n        # 4. choose k* that maximises improvement per joule\n        k_star = int(min(self.k_max, max(1, round(( (rho*eta_star*g_norm2)/(2*self.alpha*loss) )**0.5 ))) )\n        # 5. momentum\n        mom = max(self.m_min, min(self.m_max, 1 - self.kappa*eta_star))\n        for pg in self.opt.param_groups:\n            pg['lr']=eta_star; pg['momentum']=mom\n        # 6. parameter update\n        self.opt.step()\n        # 7. bookkeeping\n        self.prev_loss = loss\n        self.prev_step_energy = eta_star*g_norm2/2\n        self.k_accum, self._ctr = k_star, 0",
        "expected_result": "Across all four benchmarks:\n• E-FLAC reaches the target validation score 55–70 % faster than the cosine schedule and 25–40 % faster than AdaScale or FLAC.\n• GPU energy to target drops by ≈60 % relative to cosine and ≈35 % relative to the best adaptive baseline.\n• Total FLOPs shrink by 40–50 % thanks to smaller batches during stable phases.\n• Final accuracy/perplexity/caption quality matches the strongest baseline within ±0.2 pp / −0.2 ppl / +1 CIDEr.\nAblations (ImageNet):\n   – Removing energy-aware k* selection but keeping η*,m_t (revert to FLAC) loses ~20 % of the speed-up.\n   – Fixing η* to cosine while keeping energy-aware batches loses ~45 %.\n   – Skipping the noise estimate (ρ_t=1) leads to unstable jumps in batch size and 0.3 pp accuracy drop.",
        "expected_conclusion": "Training can be accelerated even further when the optimiser explicitly maximises *predicted loss reduction per Joule* rather than treating compute cost as invisible.  By combining a forward-looking quadratic loss surrogate with an online estimate of gradient noise and a simple linear energy model, E-FLAC chooses, every step, the step size, momentum and amount of additional data that promise the best energy-normalised progress.  The resulting 35–60 % energy savings over the strongest prior adaptive baselines offer a concrete route to greener deep-learning pipelines without sacrificing accuracy or requiring specialist hardware or heavy hyper-parameter tuning.  The idea of optimising \"progress per Joule\" opens a new research avenue at the intersection of optimisation and sustainable ML."
      },
      "evaluation": {
        "novelty_reason": "The proposal introduces an explicit optimisation objective—maximising the predicted loss decrease per unit energy (kWh) for the *next* SGD-momentum update—and derives a closed-form rule that simultaneously selects learning-rate, momentum and effective batch size from that objective. Prior adaptive methods search (1) only in the LR–momentum space with a fixed batch (e.g. PLOS, YellowFin, Hypergrad) or (2) only in the batch dimension while replaying a user-supplied LR schedule (e.g. AdaScale, GradScale, GNS based batch scaling). Although a few papers discuss energy or carbon footprint at the *run* level (e.g. “CarbonTracker”, “EnergonAI”) or optimise throughput (e.g. PipeDream, ASAP), no method closes the on-line control loop that (a) predicts marginal accuracy gain for candidate hyper-parameter triples, (b) models instantaneous energy cost on the current hardware, and (c) selects the triple that maximises expected loss reduction per Joule for *every* step. The one-step quadratic surrogate reused from PLOS is novelly coupled with (i) a cheap in-step noise-scale estimate based on half-batches and (ii) a linear empirical power model to yield an O(1) analytic solution for η* and k*. This energy-aware, forward-looking decision rule—and its integration into a 50-line PyTorch wrapper—does not appear in existing optimisation or sustainable-ML literature, giving the hypothesis a clear element of originality.",
        "novelty_score": 8,
        "significance_reason": "Training speed and energy consumption are rapidly becoming first-order constraints in both academia and industry; GPUs for large models can draw >300 kWh per experiment. Cutting energy by 35–60 % while matching accuracy directly addresses the sustainability imperative highlighted by recent surveys (\"Green AI\", \"EfficientNet Energy\"). Academically, the hypothesis bridges two largely separate lines of work—adaptive optimisation and energy-aware ML—opening a new research axis of “progress per Joule” optimisers. Practically, the method is hardware-agnostic, hyper-parameter-light, and inserts into existing training loops with negligible code, which increases adoption potential across vision, language and multimodal tasks. If validated, the approach could reduce operational costs and carbon emissions of thousands of daily ML training jobs and inspire follow-up research on energy-conditioned learning theory, controller design, and power-adaptive distributed training. Hence the societal and academic impact are both high.",
        "significance_score": 9
      }
    },
    {
      "hypothesis": {
        "open_problems": "E-FLAC maximises the *loss decrease per joule* of every SGD-Momentum update, implicitly assuming (1) the electricity supplying the accelerator has constant carbon intensity and (2) the only controllable dimension of compute cost is how many FLOPs we execute (via batch size).  In reality, both the *carbon factor* of the grid (gCO₂ kWh⁻¹) and the *electrical efficiency* of modern GPUs/TPUs (joule per FLOP as a function of power-cap / clock) fluctuate by up to 3× within a single training run.  Consequently, two updates that consume the same energy can differ three-fold in CO₂-e emissions, and the optimiser may still waste carbon by executing compute-heavy steps during high-carbon hours or at an unnecessarily high power cap.  No existing optimiser closes the loop that simultaneously chooses 1) learning-rate η, 2) momentum m, 3) effective batch size B, and 4) device power-cap P so as to maximise *predicted loss decrease per gram of CO₂*.",
        "method": "Carbon-Aware Forward-Looking Adaptive Controller (C-FLAC)\n1. Retain E-FLAC’s quadratic surrogate to predict loss improvement I(η)=α_t η‖g_t‖²/2.\n2. Carbon-intensity feed-forward.  Query an external signal c_t  (gCO₂ kWh⁻¹)—either live via the free ElectricityMap / WattTime API or replayed from a trace—for the next Δτ≈2 min (≈one training step on ImageNet).\n3. Power–efficiency model.  Measure once the energy per micro-batch at three GPU power caps P∈{P_min, P_mid, P_max} using nvidia-smi –pl and fit a linear model  E(k,P)=k·(a+b·P) where k is the accumulation factor.  (Two extra scalars, estimated in <1 min warm-up.)\n4. Objective for the *next* update:\n   maximise R(η,k,P)= I(η) / ( E(k,P)·c_t ) over η∈[η_min,η_max], k∈{1,…,k_max}, P∈{P_min,P_mid,P_max}.\n   • As before, η* is analytic: η* = min(η_max , 2L_t /(α_t‖g_t‖²)).\n   • Optimal k* = clamp(⌈√(ρ_t η*‖g_t‖² /(2α_tL_t))⌉,1,k_max).\n   • Choose the power cap that minimises carbon per FLOP for the selected k*: P* = argmin_P  (E(k*,P)·c_t).\n5. Momentum from kinetic-energy rule m_t = clamp(1−κη*, m_min, m_max).\n6. Apply P* immediately via nvidia-smi –pl; implement k* with gradient accumulation; update parameters with (η*,m_t).\n7. Complexity: O(1) extra scalar ops; two new discrete choices (c_t fetched asynchronously, P selected from ≤3 options).  Safety fall-back: if the API is unreachable, revert to E-FLAC (energy-only).",
        "experimental_setup": "Benchmarks & regions\n• Vision: ImageNet-1k / ResNet-50 (training lasts 1.5 h at P_max)  – run once in Paris (low-carbon grid), once in Frankfurt (high-carbon grid) using ElectricityMap live feed.\n• Language: WikiText-103 / Transformer-Decoder (3 h) – region: California, live feed from WattTime.\nBaselines\n1. SGD-M + Cosine LR, fixed batch, default power cap.\n2. E-FLAC (η,m,B) – energy-aware only.\n3. ASAP-CAP (recent throughput-optimised power-capping) – adjusts P but not optimiser knobs.\nProposed: C-FLAC (η,m,B,P).\nProtocol\n• Same physical micro-batch (B_phys=256 ImageNet, 2048 tokens WT-103).\n• Each run repeats 3×; wall-clock times aligned across methods by synchronising starting hour.\nMetrics\n• Primary: kg CO₂ to reach target quality (74 % top-1, ppl 26).\n• Secondary: energy (kWh), wall-clock, final accuracy.\nImplementation: 70 extra lines on top of the E-FLAC PyTorch wrapper; power-cap change issued with subprocess(\"nvidia-smi -pl {P}\").  Carbon API queried every 30 s in a background thread and cached.",
        "primary_metric": "Total CO₂ emissions (kg CO₂-e) to hit the target validation score; energy and time as secondary.",
        "experimental_code": "# snippet for power-cap selection\nci = carbon_signal.fetch_now()          # gCO2 per kWh (async cache)\neta_star = min(eta_max, 2*loss /(alpha*g2+1e-12))\nk_star   = max(1, min(k_max, round(math.sqrt(rho*eta_star*g2 /(2*alpha*loss)))))\ncarbon_cost = {}\nfor P in [P_MIN, P_MID, P_MAX]:\n    E = k_star * (a + b*P)              # joule per batch * batches\n    carbon_cost[P] = E * ci\nP_star = min(carbon_cost, key=carbon_cost.get)\nsubprocess.run([\"nvidia-smi\",\"-pl\",str(P_star)], check=True)",
        "expected_result": "Compared to the strongest baseline in each region:\n• C-FLAC lowers CO₂ to target by 55–65 % over Cosine and 30–45 % over E-FLAC, with ≤1 % slower wall-clock.\n• On low-carbon grids the gap between C-FLAC and E-FLAC shrinks to ≈15 %, validating carbon-adaptivity.\n• Accuracy/perplexity matches baselines within statistical noise.\nAblations (ImageNet, Frankfurt):\n   – Disabling power-cap selection (+P fixed at 300 W) loses ~25 % of the carbon saving.\n   – Using a stale carbon signal (hour-old) degrades benefit by ~10 %, showing the need for live data.",
        "expected_conclusion": "Real-world carbon footprint depends jointly on how *much* we compute, *when* we compute, and at what *electrical efficiency* the hardware operates.  By extending forward-looking optimisation from energy to CO₂ and adding a controllable power-cap knob, C-FLAC achieves the first per-mini-batch policy that maximises *predicted loss reduction per gram of CO₂*.  Experiments across grids with contrasting carbon profiles demonstrate up to 65 % emission cuts without accuracy loss or additional training time, providing a concrete, immediately deployable path toward climate-aware deep learning."
      },
      "evaluation": {
        "novelty_reason": "The proposal extends the recent E-FLAC optimiser, which already maximises loss-decrease per joule, by (1) replacing the objective with loss-decrease per gram of CO₂ and (2) introducing live control of the GPU/TPU power-cap so that every step can trade electrical efficiency against throughput in response to time-varying grid carbon intensity. While prior work exists on each axis separately—e.g.\n • Energy-aware optimisers (E-FLAC, SharpSepConv)\n • Carbon-aware job schedulers that simply pause or migrate whole jobs (Google Carbon Aware Computing, GreenSlot)\n • Throughput-optimised power-capping approaches (ASAP-CAP, Choi et al. 2021)\nnone of them jointly decides the optimiser hyper-parameters (η, momentum, batch size) together with device power-cap conditioned on a real-time carbon signal, nor do they optimise a closed-form surrogate objective at every SGD step. The introduced quadratic surrogate, the analytic selection of η*, k*, and the discrete search over three power-caps within O(1) computation, constitute a new control algorithm that closes the loop between learning dynamics, hardware efficiency, and external carbon variability. This fine-grain, per-update carbon-aware control has not been reported in the literature, giving the hypothesis clear novelty.",
        "novelty_score": 8,
        "significance_reason": "Academically, the hypothesis bridges optimisation theory, systems for ML, and sustainability research by defining and maximising a new metric—expected loss reduction per gram CO₂—and showing how to solve it analytically and cheaply online. If validated, it would open a new research line on carbon-aware optimisers, inspiring follow-up work for other algorithms (Adam, L-BFGS) and hardware (ASICs, CPUs).\nSocietally, training large models already emits kiloton-scale CO₂. Achieving the claimed 30–65 % emission cut without sacrificing accuracy or wall-clock time offers an immediately deployable technique for any practitioner with access to a carbon-intensity API and nvidia-smi. Because the method requires only minor code changes and no extra hardware, its potential impact spans academia, industry, and cloud providers, directly contributing to climate-change mitigation efforts in AI.",
        "significance_score": 9
      }
    }
  ],
  "research_session": {
    "hypothesis": {
      "open_problems": "E-FLAC maximises the *loss decrease per joule* of every SGD-Momentum update, implicitly assuming (1) the electricity supplying the accelerator has constant carbon intensity and (2) the only controllable dimension of compute cost is how many FLOPs we execute (via batch size).  In reality, both the *carbon factor* of the grid (gCO₂ kWh⁻¹) and the *electrical efficiency* of modern GPUs/TPUs (joule per FLOP as a function of power-cap / clock) fluctuate by up to 3× within a single training run.  Consequently, two updates that consume the same energy can differ three-fold in CO₂-e emissions, and the optimiser may still waste carbon by executing compute-heavy steps during high-carbon hours or at an unnecessarily high power cap.  No existing optimiser closes the loop that simultaneously chooses 1) learning-rate η, 2) momentum m, 3) effective batch size B, and 4) device power-cap P so as to maximise *predicted loss decrease per gram of CO₂*.",
      "method": "Carbon-Aware Forward-Looking Adaptive Controller (C-FLAC)\n1. Retain E-FLAC’s quadratic surrogate to predict loss improvement I(η)=α_t η‖g_t‖²/2.\n2. Carbon-intensity feed-forward.  Query an external signal c_t  (gCO₂ kWh⁻¹)—either live via the free ElectricityMap / WattTime API or replayed from a trace—for the next Δτ≈2 min (≈one training step on ImageNet).\n3. Power–efficiency model.  Measure once the energy per micro-batch at three GPU power caps P∈{P_min, P_mid, P_max} using nvidia-smi –pl and fit a linear model  E(k,P)=k·(a+b·P) where k is the accumulation factor.  (Two extra scalars, estimated in <1 min warm-up.)\n4. Objective for the *next* update:\n   maximise R(η,k,P)= I(η) / ( E(k,P)·c_t ) over η∈[η_min,η_max], k∈{1,…,k_max}, P∈{P_min,P_mid,P_max}.\n   • As before, η* is analytic: η* = min(η_max , 2L_t /(α_t‖g_t‖²)).\n   • Optimal k* = clamp(⌈√(ρ_t η*‖g_t‖² /(2α_tL_t))⌉,1,k_max).\n   • Choose the power cap that minimises carbon per FLOP for the selected k*: P* = argmin_P  (E(k*,P)·c_t).\n5. Momentum from kinetic-energy rule m_t = clamp(1−κη*, m_min, m_max).\n6. Apply P* immediately via nvidia-smi –pl; implement k* with gradient accumulation; update parameters with (η*,m_t).\n7. Complexity: O(1) extra scalar ops; two new discrete choices (c_t fetched asynchronously, P selected from ≤3 options).  Safety fall-back: if the API is unreachable, revert to E-FLAC (energy-only).",
      "experimental_setup": "Benchmarks & regions\n• Vision: ImageNet-1k / ResNet-50 (training lasts 1.5 h at P_max)  – run once in Paris (low-carbon grid), once in Frankfurt (high-carbon grid) using ElectricityMap live feed.\n• Language: WikiText-103 / Transformer-Decoder (3 h) – region: California, live feed from WattTime.\nBaselines\n1. SGD-M + Cosine LR, fixed batch, default power cap.\n2. E-FLAC (η,m,B) – energy-aware only.\n3. ASAP-CAP (recent throughput-optimised power-capping) – adjusts P but not optimiser knobs.\nProposed: C-FLAC (η,m,B,P).\nProtocol\n• Same physical micro-batch (B_phys=256 ImageNet, 2048 tokens WT-103).\n• Each run repeats 3×; wall-clock times aligned across methods by synchronising starting hour.\nMetrics\n• Primary: kg CO₂ to reach target quality (74 % top-1, ppl 26).\n• Secondary: energy (kWh), wall-clock, final accuracy.\nImplementation: 70 extra lines on top of the E-FLAC PyTorch wrapper; power-cap change issued with subprocess(\"nvidia-smi -pl {P}\").  Carbon API queried every 30 s in a background thread and cached.",
      "primary_metric": "Total CO₂ emissions (kg CO₂-e) to hit the target validation score; energy and time as secondary.",
      "experimental_code": "# snippet for power-cap selection\nci = carbon_signal.fetch_now()          # gCO2 per kWh (async cache)\neta_star = min(eta_max, 2*loss /(alpha*g2+1e-12))\nk_star   = max(1, min(k_max, round(math.sqrt(rho*eta_star*g2 /(2*alpha*loss)))))\ncarbon_cost = {}\nfor P in [P_MIN, P_MID, P_MAX]:\n    E = k_star * (a + b*P)              # joule per batch * batches\n    carbon_cost[P] = E * ci\nP_star = min(carbon_cost, key=carbon_cost.get)\nsubprocess.run([\"nvidia-smi\",\"-pl\",str(P_star)], check=True)",
      "expected_result": "Compared to the strongest baseline in each region:\n• C-FLAC lowers CO₂ to target by 55–65 % over Cosine and 30–45 % over E-FLAC, with ≤1 % slower wall-clock.\n• On low-carbon grids the gap between C-FLAC and E-FLAC shrinks to ≈15 %, validating carbon-adaptivity.\n• Accuracy/perplexity matches baselines within statistical noise.\nAblations (ImageNet, Frankfurt):\n   – Disabling power-cap selection (+P fixed at 300 W) loses ~25 % of the carbon saving.\n   – Using a stale carbon signal (hour-old) degrades benefit by ~10 %, showing the need for live data.",
      "expected_conclusion": "Real-world carbon footprint depends jointly on how *much* we compute, *when* we compute, and at what *electrical efficiency* the hardware operates.  By extending forward-looking optimisation from energy to CO₂ and adding a controllable power-cap knob, C-FLAC achieves the first per-mini-batch policy that maximises *predicted loss reduction per gram of CO₂*.  Experiments across grids with contrasting carbon profiles demonstrate up to 65 % emission cuts without accuracy loss or additional training time, providing a concrete, immediately deployable path toward climate-aware deep learning."
    },
    "iterations": []
  }
}