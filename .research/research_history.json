{
  "research_topic": "Accelerate neural network training",
  "queries": [
    "mixed precision training",
    "gradient accumulation",
    "distributed data parallel",
    "learning rate scheduling",
    "neural network pruning"
  ],
  "research_study_list": [
    {
      "title": "Collage: Light-Weight Low-Precision Strategy for LLM Training",
      "abstract": "Large models training is plagued by the intense compute cost and limited hardware memory. A practical solution is low-precision representation but is troubled by loss in numerical accuracy and unstable training rendering the model less useful. We argue that low-precision floating points can perform well provided the error is properly compensated at the critical locations in the training process. We propose Collage which utilizes multi-component float representation in low-precision to accurately perform operations with numerical errors accounted. To understand the impact of imprecision to training, we propose a simple and novel metric which tracks the lost information during training as well as differentiates various precision strategies. Our method works with commonly used low-precision such as half-precision ($16$-bit floating points) and can be naturally extended to work with even lower precision such as $8$-bit. Experimental results show that pre-training using Collage removes the requirement of using $32$-bit floating-point copies of the model and attains similar/better training performance compared to $(16, 32)$-bit mixed-precision strategy, with up to $3.7\\times$ speedup and $\\sim 15\\%$ to $23\\%$ less memory usage in practice.",
      "meta_data": {
        "arxiv_id": "2405.03637v1",
        "authors": [
          "Tao Yu",
          "Gaurav Gupta",
          "Karthick Gopalswamy",
          "Amith Mamidala",
          "Hao Zhou",
          "Jeffrey Huynh",
          "Youngsuk Park",
          "Ron Diamant",
          "Anoop Deoras",
          "Luke Huan"
        ],
        "published_date": "2024-05-06T16:55:30Z",
        "pdf_url": "https://arxiv.org/pdf/2405.03637v1.pdf"
      }
    },
    {
      "title": "Guaranteed Approximation Bounds for Mixed-Precision Neural Operators",
      "abstract": "Neural operators, such as Fourier Neural Operators (FNO), form a principled approach for learning solution operators for PDEs and other mappings between function spaces. However, many real-world problems require high-resolution training data, and the training time and limited GPU memory pose big barriers. One solution is to train neural operators in mixed precision to reduce the memory requirement and increase training speed. However, existing mixed-precision training techniques are designed for standard neural networks, and we find that their direct application to FNO leads to numerical overflow and poor memory efficiency. Further, at first glance, it may appear that mixed precision in FNO will lead to drastic accuracy degradation since reducing the precision of the Fourier transform yields poor results in classical numerical solvers. We show that this is not the case; in fact, we prove that reducing the precision in FNO still guarantees a good approximation bound, when done in a targeted manner. Specifically, we build on the intuition that neural operator learning inherently induces an approximation error, arising from discretizing the infinite-dimensional ground-truth input function, implying that training in full precision is not needed. We formalize this intuition by rigorously characterizing the approximation and precision errors of FNO and bounding these errors for general input functions. We prove that the precision error is asymptotically comparable to the approximation error. Based on this, we design a simple method to optimize the memory-intensive half-precision tensor contractions by greedily finding the optimal contraction order. Through extensive experiments on different state-of-the-art neural operators, datasets, and GPUs, we demonstrate that our approach reduces GPU memory usage by up to 50% and improves throughput by 58% with little or no reduction in accuracy.",
      "meta_data": {
        "arxiv_id": "2307.15034v3",
        "authors": [
          "Renbo Tu",
          "Colin White",
          "Jean Kossaifi",
          "Boris Bonev",
          "Nikola Kovachki",
          "Gennady Pekhimenko",
          "Kamyar Azizzadenesheli",
          "Anima Anandkumar"
        ],
        "published_date": "2023-07-27T17:42:06Z",
        "pdf_url": "https://arxiv.org/pdf/2307.15034v3.pdf"
      }
    },
    {
      "title": "Shifted and Squeezed 8-bit Floating Point format for Low-Precision Training of Deep Neural Networks",
      "abstract": "Training with larger number of parameters while keeping fast iterations is an increasingly adopted strategy and trend for developing better performing Deep Neural Network (DNN) models. This necessitates increased memory footprint and computational requirements for training. Here we introduce a novel methodology for training deep neural networks using 8-bit floating point (FP8) numbers. Reduced bit precision allows for a larger effective memory and increased computational speed. We name this method Shifted and Squeezed FP8 (S2FP8). We show that, unlike previous 8-bit precision training methods, the proposed method works out-of-the-box for representative models: ResNet-50, Transformer and NCF. The method can maintain model accuracy without requiring fine-tuning loss scaling parameters or keeping certain layers in single precision. We introduce two learnable statistics of the DNN tensors - shifted and squeezed factors that are used to optimally adjust the range of the tensors in 8-bits, thus minimizing the loss in information due to quantization.",
      "meta_data": {
        "arxiv_id": "2001.05674v1",
        "authors": [
          "Léopold Cambier",
          "Anahita Bhiwandiwalla",
          "Ting Gong",
          "Mehran Nekuii",
          "Oguz H Elibol",
          "Hanlin Tang"
        ],
        "published_date": "2020-01-16T06:38:27Z",
        "pdf_url": "https://arxiv.org/pdf/2001.05674v1.pdf"
      }
    },
    {
      "title": "Structured Inverse-Free Natural Gradient Descent: Memory-Efficient & Numerically-Stable KFAC",
      "abstract": "Second-order methods such as KFAC can be useful for neural net training. However, they are often memory-inefficient since their preconditioning Kronecker factors are dense, and numerically unstable in low precision as they require matrix inversion or decomposition. These limitations render such methods unpopular for modern mixed-precision training. We address them by (i) formulating an inverse-free KFAC update and (ii) imposing structures in the Kronecker factors, resulting in structured inverse-free natural gradient descent (SINGD). On modern neural networks, we show that SINGD is memory-efficient and numerically robust, in contrast to KFAC, and often outperforms AdamW even in half precision. Our work closes a gap between first- and second-order methods in modern low-precision training.",
      "meta_data": {
        "arxiv_id": "2312.05705v4",
        "authors": [
          "Wu Lin",
          "Felix Dangel",
          "Runa Eschenhagen",
          "Kirill Neklyudov",
          "Agustinus Kristiadi",
          "Richard E. Turner",
          "Alireza Makhzani"
        ],
        "published_date": "2023-12-09T23:13:32Z",
        "pdf_url": "https://arxiv.org/pdf/2312.05705v4.pdf"
      }
    },
    {
      "title": "Multi-Precision Policy Enforced Training (MuPPET) : A Precision-Switching Strategy for Quantised Fixed-Point Training of CNNs",
      "abstract": "Large-scale convolutional neural networks (CNNs) suffer from very long training times, spanning from hours to weeks, limiting the productivity and experimentation of deep learning practitioners. As networks grow in size and complexity, training time can be reduced through low-precision data representations and computations. However, in doing so the final accuracy suffers due to the problem of vanishing gradients. Existing state-of-the-art methods combat this issue by means of a mixed-precision approach utilising two different precision levels, FP32 (32-bit floating-point) and FP16/FP8 (16-/8-bit floating-point), leveraging the hardware support of recent GPU architectures for FP16 operations to obtain performance gains. This work pushes the boundary of quantised training by employing a multilevel optimisation approach that utilises multiple precisions including low-precision fixed-point representations. The novel training strategy, MuPPET, combines the use of multiple number representation regimes together with a precision-switching mechanism that decides at run time the transition point between precision regimes. Overall, the proposed strategy tailors the training process to the hardware-level capabilities of the target hardware architecture and yields improvements in training time and energy efficiency compared to state-of-the-art approaches. Applying MuPPET on the training of AlexNet, ResNet18 and GoogLeNet on ImageNet (ILSVRC12) and targeting an NVIDIA Turing GPU, MuPPET achieves the same accuracy as standard full-precision training with training-time speedup of up to 1.84$\\times$ and an average speedup of 1.58$\\times$ across the networks.",
      "meta_data": {
        "arxiv_id": "2006.09049v1",
        "authors": [
          "Aditya Rajagopal",
          "Diederik Adriaan Vink",
          "Stylianos I. Venieris",
          "Christos-Savvas Bouganis"
        ],
        "published_date": "2020-06-16T10:14:36Z",
        "pdf_url": "https://arxiv.org/pdf/2006.09049v1.pdf"
      }
    },
    {
      "title": "Towards Cheaper Inference in Deep Networks with Lower Bit-Width Accumulators",
      "abstract": "The majority of the research on the quantization of Deep Neural Networks (DNNs) is focused on reducing the precision of tensors visible by high-level frameworks (e.g., weights, activations, and gradients). However, current hardware still relies on high-accuracy core operations. Most significant is the operation of accumulating products. This high-precision accumulation operation is gradually becoming the main computational bottleneck. This is because, so far, the usage of low-precision accumulators led to a significant degradation in performance. In this work, we present a simple method to train and fine-tune high-end DNNs, to allow, for the first time, utilization of cheaper, $12$-bits accumulators, with no significant degradation in accuracy. Lastly, we show that as we decrease the accumulation precision further, using fine-grained gradient approximations can improve the DNN accuracy.",
      "meta_data": {
        "arxiv_id": "2401.14110v1",
        "authors": [
          "Yaniv Blumenfeld",
          "Itay Hubara",
          "Daniel Soudry"
        ],
        "published_date": "2024-01-25T11:46:01Z",
        "pdf_url": "https://arxiv.org/pdf/2401.14110v1.pdf"
      }
    },
    {
      "title": "Accumulated Decoupled Learning with Gradient Staleness Mitigation for Convolutional Neural Networks",
      "abstract": "Decoupled learning is a branch of model parallelism which parallelizes the training of a network by splitting it depth-wise into multiple modules. Techniques from decoupled learning usually lead to stale gradient effect because of their asynchronous implementation, thereby causing performance degradation. In this paper, we propose an accumulated decoupled learning (ADL) which incorporates the gradient accumulation technique to mitigate the stale gradient effect. We give both theoretical and empirical evidences regarding how the gradient staleness can be reduced. We prove that the proposed method can converge to critical points, i.e., the gradients converge to 0, in spite of its asynchronous nature. Empirical validation is provided by training deep convolutional neural networks to perform classification tasks on CIFAR-10 and ImageNet datasets. The ADL is shown to outperform several state-of-the-arts in the classification tasks, and is the fastest among the compared methods.",
      "meta_data": {
        "arxiv_id": "2012.03747v1",
        "authors": [
          "Huiping Zhuang",
          "Zhiping Lin",
          "Kar-Ann Toh"
        ],
        "published_date": "2020-12-03T11:52:55Z",
        "pdf_url": "https://arxiv.org/pdf/2012.03747v1.pdf"
      }
    },
    {
      "title": "Neural gradients are near-lognormal: improved quantized  and sparse training",
      "abstract": "While training can mostly be accelerated by reducing the time needed to propagate neural gradients back throughout the model, most previous works focus on the quantization/pruning of weights and activations. These methods are often not applicable to neural gradients, which have very different statistical properties. Distinguished from weights and activations, we find that the distribution of neural gradients is approximately lognormal. Considering this, we suggest two closed-form analytical methods to reduce the computational and memory burdens of neural gradients. The first method optimizes the floating-point format and scale of the gradients. The second method accurately sets sparsity thresholds for gradient pruning. Each method achieves state-of-the-art results on ImageNet. To the best of our knowledge, this paper is the first to (1) quantize the gradients to 6-bit floating-point formats, or (2) achieve up to 85% gradient sparsity -- in each case without accuracy degradation. Reference implementation accompanies the paper.",
      "meta_data": {
        "arxiv_id": "2006.08173v3",
        "authors": [
          "Brian Chmiel",
          "Liad Ben-Uri",
          "Moran Shkolnik",
          "Elad Hoffer",
          "Ron Banner",
          "Daniel Soudry"
        ],
        "published_date": "2020-06-15T07:00:15Z",
        "pdf_url": "https://arxiv.org/pdf/2006.08173v3.pdf"
      }
    },
    {
      "title": "Stabilizing Backpropagation Through Time to Learn Complex Physics",
      "abstract": "Of all the vector fields surrounding the minima of recurrent learning setups, the gradient field with its exploding and vanishing updates appears a poor choice for optimization, offering little beyond efficient computability. We seek to improve this suboptimal practice in the context of physics simulations, where backpropagating feedback through many unrolled time steps is considered crucial to acquiring temporally coherent behavior. The alternative vector field we propose follows from two principles: physics simulators, unlike neural networks, have a balanced gradient flow, and certain modifications to the backpropagation pass leave the positions of the original minima unchanged. As any modification of backpropagation decouples forward and backward pass, the rotation-free character of the gradient field is lost. Therefore, we discuss the negative implications of using such a rotational vector field for optimization and how to counteract them. Our final procedure is easily implementable via a sequence of gradient stopping and component-wise comparison operations, which do not negatively affect scalability. Our experiments on three control problems show that especially as we increase the complexity of each task, the unbalanced updates from the gradient can no longer provide the precise control signals necessary while our method still solves the tasks. Our code can be found at https://github.com/tum-pbs/StableBPTT.",
      "meta_data": {
        "arxiv_id": "2405.02041v1",
        "authors": [
          "Patrick Schnell",
          "Nils Thuerey"
        ],
        "published_date": "2024-05-03T12:20:08Z",
        "pdf_url": "https://arxiv.org/pdf/2405.02041v1.pdf"
      }
    },
    {
      "title": "Extrapolation for Large-batch Training in Deep Learning",
      "abstract": "Deep learning networks are typically trained by Stochastic Gradient Descent (SGD) methods that iteratively improve the model parameters by estimating a gradient on a very small fraction of the training data. A major roadblock faced when increasing the batch size to a substantial fraction of the training data for improving training time is the persistent degradation in performance (generalization gap). To address this issue, recent work propose to add small perturbations to the model parameters when computing the stochastic gradients and report improved generalization performance due to smoothing effects. However, this approach is poorly understood; it requires often model-specific noise and fine-tuning. To alleviate these drawbacks, we propose to use instead computationally efficient extrapolation (extragradient) to stabilize the optimization trajectory while still benefiting from smoothing to avoid sharp minima. This principled approach is well grounded from an optimization perspective and we show that a host of variations can be covered in a unified framework that we propose. We prove the convergence of this novel scheme and rigorously evaluate its empirical performance on ResNet, LSTM, and Transformer. We demonstrate that in a variety of experiments the scheme allows scaling to much larger batch sizes than before whilst reaching or surpassing SOTA accuracy.",
      "meta_data": {
        "arxiv_id": "2006.05720v1",
        "authors": [
          "Tao Lin",
          "Lingjing Kong",
          "Sebastian U. Stich",
          "Martin Jaggi"
        ],
        "published_date": "2020-06-10T08:22:41Z",
        "pdf_url": "https://arxiv.org/pdf/2006.05720v1.pdf"
      }
    },
    {
      "title": "DRAGONN: Distributed Randomized Approximate Gradients of Neural Networks"
    },
    {
      "title": "DRAGONN: Distributed Randomized Approximate Gradients of Neural Networks"
    },
    {
      "title": "DD-PPO: Learning Near-Perfect PointGoal Navigators from 2.5 Billion Frames",
      "abstract": "We present Decentralized Distributed Proximal Policy Optimization (DD-PPO), a method for distributed reinforcement learning in resource-intensive simulated environments. DD-PPO is distributed (uses multiple machines), decentralized (lacks a centralized server), and synchronous (no computation is ever stale), making it conceptually simple and easy to implement. In our experiments on training virtual robots to navigate in Habitat-Sim, DD-PPO exhibits near-linear scaling -- achieving a speedup of 107x on 128 GPUs over a serial implementation. We leverage this scaling to train an agent for 2.5 Billion steps of experience (the equivalent of 80 years of human experience) -- over 6 months of GPU-time training in under 3 days of wall-clock time with 64 GPUs.\n  This massive-scale training not only sets the state of art on Habitat Autonomous Navigation Challenge 2019, but essentially solves the task --near-perfect autonomous navigation in an unseen environment without access to a map, directly from an RGB-D camera and a GPS+Compass sensor. Fortuitously, error vs computation exhibits a power-law-like distribution; thus, 90% of peak performance is obtained relatively early (at 100 million steps) and relatively cheaply (under 1 day with 8 GPUs). Finally, we show that the scene understanding and navigation policies learned can be transferred to other navigation tasks -- the analog of ImageNet pre-training + task-specific fine-tuning for embodied AI. Our model outperforms ImageNet pre-trained CNNs on these transfer tasks and can serve as a universal resource (all models and code are publicly available).",
      "meta_data": {
        "arxiv_id": "1911.00357v2",
        "authors": [
          "Erik Wijmans",
          "Abhishek Kadian",
          "Ari Morcos",
          "Stefan Lee",
          "Irfan Essa",
          "Devi Parikh",
          "Manolis Savva",
          "Dhruv Batra"
        ],
        "published_date": "2019-11-01T13:07:37Z",
        "pdf_url": "https://arxiv.org/pdf/1911.00357v2.pdf"
      }
    },
    {
      "title": "Towards a Better Theoretical Understanding of Independent Subnetwork Training",
      "abstract": "Modern advancements in large-scale machine learning would be impossible without the paradigm of data-parallel distributed computing. Since distributed computing with large-scale models imparts excessive pressure on communication channels, significant recent research has been directed toward co-designing communication compression strategies and training algorithms with the goal of reducing communication costs. While pure data parallelism allows better data scaling, it suffers from poor model scaling properties. Indeed, compute nodes are severely limited by memory constraints, preventing further increases in model size. For this reason, the latest achievements in training giant neural network models also rely on some form of model parallelism. In this work, we take a closer theoretical look at Independent Subnetwork Training (IST), which is a recently proposed and highly effective technique for solving the aforementioned problems. We identify fundamental differences between IST and alternative approaches, such as distributed methods with compressed communication, and provide a precise analysis of its optimization performance on a quadratic model.",
      "meta_data": {
        "arxiv_id": "2306.16484v2",
        "authors": [
          "Egor Shulgin",
          "Peter Richtárik"
        ],
        "published_date": "2023-06-28T18:14:22Z",
        "pdf_url": "https://arxiv.org/pdf/2306.16484v2.pdf"
      }
    },
    {
      "title": "Debiasing Distributed Second Order Optimization with Surrogate Sketching and Scaled Regularization",
      "abstract": "In distributed second order optimization, a standard strategy is to average many local estimates, each of which is based on a small sketch or batch of the data. However, the local estimates on each machine are typically biased, relative to the full solution on all of the data, and this can limit the effectiveness of averaging. Here, we introduce a new technique for debiasing the local estimates, which leads to both theoretical and empirical improvements in the convergence rate of distributed second order methods. Our technique has two novel components: (1) modifying standard sketching techniques to obtain what we call a surrogate sketch; and (2) carefully scaling the global regularization parameter for local computations. Our surrogate sketches are based on determinantal point processes, a family of distributions for which the bias of an estimate of the inverse Hessian can be computed exactly. Based on this computation, we show that when the objective being minimized is $l_2$-regularized with parameter $λ$ and individual machines are each given a sketch of size $m$, then to eliminate the bias, local estimates should be computed using a shrunk regularization parameter given by $λ^{\\prime}=λ\\cdot(1-\\frac{d_λ}{m})$, where $d_λ$ is the $λ$-effective dimension of the Hessian (or, for quadratic problems, the data matrix).",
      "meta_data": {
        "arxiv_id": "2007.01327v1",
        "authors": [
          "Michał Dereziński",
          "Burak Bartan",
          "Mert Pilanci",
          "Michael W. Mahoney"
        ],
        "published_date": "2020-07-02T18:08:14Z",
        "pdf_url": "https://arxiv.org/pdf/2007.01327v1.pdf"
      }
    },
    {
      "title": "AutoLRS: Automatic Learning-Rate Schedule by Bayesian Optimization on the Fly",
      "abstract": "The learning rate (LR) schedule is one of the most important hyper-parameters needing careful tuning in training DNNs. However, it is also one of the least automated parts of machine learning systems and usually costs significant manual effort and computing. Though there are pre-defined LR schedules and optimizers with adaptive LR, they introduce new hyperparameters that need to be tuned separately for different tasks/datasets. In this paper, we consider the question: Can we automatically tune the LR over the course of training without human involvement? We propose an efficient method, AutoLRS, which automatically optimizes the LR for each training stage by modeling training dynamics. AutoLRS aims to find an LR applied to every $τ$ steps that minimizes the resulted validation loss. We solve this black-box optimization on the fly by Bayesian optimization (BO). However, collecting training instances for BO requires a system to evaluate each LR queried by BO's acquisition function for $τ$ steps, which is prohibitively expensive in practice. Instead, we apply each candidate LR for only $τ'\\llτ$ steps and train an exponential model to predict the validation loss after $τ$ steps. This mutual-training process between BO and the loss-prediction model allows us to limit the training steps invested in the BO search. We demonstrate the advantages and the generality of AutoLRS through extensive experiments of training DNNs for tasks from diverse domains using different optimizers. The LR schedules auto-generated by AutoLRS lead to a speedup of $1.22\\times$, $1.43\\times$, and $1.5\\times$ when training ResNet-50, Transformer, and BERT, respectively, compared to the LR schedules in their original papers, and an average speedup of $1.31\\times$ over state-of-the-art heavily-tuned LR schedules.",
      "meta_data": {
        "arxiv_id": "2105.10762v1",
        "authors": [
          "Yuchen Jin",
          "Tianyi Zhou",
          "Liangyu Zhao",
          "Yibo Zhu",
          "Chuanxiong Guo",
          "Marco Canini",
          "Arvind Krishnamurthy"
        ],
        "published_date": "2021-05-22T16:41:10Z",
        "pdf_url": "https://arxiv.org/pdf/2105.10762v1.pdf"
      }
    },
    {
      "title": "Budgeted Training: Rethinking Deep Neural Network Training Under Resource Constraints",
      "abstract": "In most practical settings and theoretical analyses, one assumes that a model can be trained until convergence. However, the growing complexity of machine learning datasets and models may violate such assumptions. Indeed, current approaches for hyper-parameter tuning and neural architecture search tend to be limited by practical resource constraints. Therefore, we introduce a formal setting for studying training under the non-asymptotic, resource-constrained regime, i.e., budgeted training. We analyze the following problem: \"given a dataset, algorithm, and fixed resource budget, what is the best achievable performance?\" We focus on the number of optimization iterations as the representative resource. Under such a setting, we show that it is critical to adjust the learning rate schedule according to the given budget. Among budget-aware learning schedules, we find simple linear decay to be both robust and high-performing. We support our claim through extensive experiments with state-of-the-art models on ImageNet (image classification), Kinetics (video classification), MS COCO (object detection and instance segmentation), and Cityscapes (semantic segmentation). We also analyze our results and find that the key to a good schedule is budgeted convergence, a phenomenon whereby the gradient vanishes at the end of each allowed budget. We also revisit existing approaches for fast convergence and show that budget-aware learning schedules readily outperform such approaches under (the practical but under-explored) budgeted training setting.",
      "meta_data": {
        "arxiv_id": "1905.04753v4",
        "authors": [
          "Mengtian Li",
          "Ersin Yumer",
          "Deva Ramanan"
        ],
        "published_date": "2019-05-12T17:49:49Z",
        "pdf_url": "https://arxiv.org/pdf/1905.04753v4.pdf"
      }
    },
    {
      "title": "Learning Rate Schedules in the Presence of Distribution Shift",
      "abstract": "We design learning rate schedules that minimize regret for SGD-based online learning in the presence of a changing data distribution. We fully characterize the optimal learning rate schedule for online linear regression via a novel analysis with stochastic differential equations. For general convex loss functions, we propose new learning rate schedules that are robust to distribution shift and we give upper and lower bounds for the regret that only differ by constants. For non-convex loss functions, we define a notion of regret based on the gradient norm of the estimated models and propose a learning schedule that minimizes an upper bound on the total expected regret. Intuitively, one expects changing loss landscapes to require more exploration, and we confirm that optimal learning rate schedules typically increase in the presence of distribution shift. Finally, we provide experiments for high-dimensional regression models and neural networks to illustrate these learning rate schedules and their cumulative regret.",
      "meta_data": {
        "arxiv_id": "2303.15634v2",
        "authors": [
          "Matthew Fahrbach",
          "Adel Javanmard",
          "Vahab Mirrokni",
          "Pratik Worah"
        ],
        "published_date": "2023-03-27T23:29:02Z",
        "venue": "Proceedings of the 40th International Conference on Machine Learning (ICML 2023) 9523-9546",
        "pdf_url": "https://arxiv.org/pdf/2303.15634v2.pdf"
      }
    },
    {
      "title": "Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations",
      "abstract": "Scale has become a main ingredient in obtaining strong machine learning models. As a result, understanding a model's scaling properties is key to effectively designing both the right training setup as well as future generations of architectures. In this work, we argue that scale and training research has been needlessly complex due to reliance on the cosine schedule, which prevents training across different lengths for the same model size. We investigate the training behavior of a direct alternative -- constant learning rate and cooldowns -- and find that it scales predictably and reliably similar to cosine. Additionally, we show that stochastic weight averaging yields improved performance along the training trajectory, without additional training costs, across different scales. Importantly, with these findings we demonstrate that scaling experiments can be performed with significantly reduced compute and GPU hours by utilizing fewer but reusable training runs. Our code is available at \\url{https://github.com/epfml/schedules-and-scaling/}.",
      "meta_data": {
        "arxiv_id": "2405.18392v3",
        "authors": [
          "Alexander Hägele",
          "Elie Bakouch",
          "Atli Kosson",
          "Loubna Ben Allal",
          "Leandro Von Werra",
          "Martin Jaggi"
        ],
        "published_date": "2024-05-28T17:33:54Z",
        "pdf_url": "https://arxiv.org/pdf/2405.18392v3.pdf"
      }
    },
    {
      "title": "Stepping on the Edge: Curvature Aware Learning Rate Tuners",
      "abstract": "Curvature information -- particularly, the largest eigenvalue of the loss Hessian, known as the sharpness -- often forms the basis for learning rate tuners. However, recent work has shown that the curvature information undergoes complex dynamics during training, going from a phase of increasing sharpness to eventual stabilization. We analyze the closed-loop feedback effect between learning rate tuning and curvature. We find that classical learning rate tuners may yield greater one-step loss reduction, yet they ultimately underperform in the long term when compared to constant learning rates in the full batch regime. These models break the stabilization of the sharpness, which we explain using a simplified model of the joint dynamics of the learning rate and the curvature. To further investigate these effects, we introduce a new learning rate tuning method, Curvature Dynamics Aware Tuning (CDAT), which prioritizes long term curvature stabilization over instantaneous progress on the objective. In the full batch regime, CDAT shows behavior akin to prefixed warm-up schedules on deep learning objectives, outperforming tuned constant learning rates. In the mini batch regime, we observe that stochasticity introduces confounding effects that explain the previous success of some learning rate tuners at appropriate batch sizes. Our findings highlight the critical role of understanding the joint dynamics of the learning rate and curvature, beyond greedy minimization, to diagnose failures and design effective adaptive learning rate tuners.",
      "meta_data": {
        "arxiv_id": "2407.06183v1",
        "authors": [
          "Vincent Roulet",
          "Atish Agarwala",
          "Jean-Bastien Grill",
          "Grzegorz Swirszcz",
          "Mathieu Blondel",
          "Fabian Pedregosa"
        ],
        "published_date": "2024-07-08T17:56:00Z",
        "pdf_url": "https://arxiv.org/pdf/2407.06183v1.pdf"
      }
    },
    {
      "title": "Fantastic Weights and How to Find Them: Where to Prune in Dynamic Sparse Training",
      "abstract": "Dynamic Sparse Training (DST) is a rapidly evolving area of research that seeks to optimize the sparse initialization of a neural network by adapting its topology during training. It has been shown that under specific conditions, DST is able to outperform dense models. The key components of this framework are the pruning and growing criteria, which are repeatedly applied during the training process to adjust the network's sparse connectivity. While the growing criterion's impact on DST performance is relatively well studied, the influence of the pruning criterion remains overlooked. To address this issue, we design and perform an extensive empirical analysis of various pruning criteria to better understand their impact on the dynamics of DST solutions. Surprisingly, we find that most of the studied methods yield similar results. The differences become more significant in the low-density regime, where the best performance is predominantly given by the simplest technique: magnitude-based pruning. The code is provided at https://github.com/alooow/fantastic_weights_paper",
      "meta_data": {
        "arxiv_id": "2306.12230v2",
        "authors": [
          "Aleksandra I. Nowak",
          "Bram Grooten",
          "Decebal Constantin Mocanu",
          "Jacek Tabor"
        ],
        "published_date": "2023-06-21T12:43:55Z",
        "pdf_url": "https://arxiv.org/pdf/2306.12230v2.pdf"
      }
    },
    {
      "title": "Neuron-level Structured Pruning using Polarization Regularizer"
    },
    {
      "title": "The Combinatorial Brain Surgeon: Pruning Weights That Cancel One Another in Neural Networks",
      "abstract": "Neural networks tend to achieve better accuracy with training if they are larger -- even if the resulting models are overparameterized. Nevertheless, carefully removing such excess parameters before, during, or after training may also produce models with similar or even improved accuracy. In many cases, that can be curiously achieved by heuristics as simple as removing a percentage of the weights with the smallest absolute value -- even though magnitude is not a perfect proxy for weight relevance. With the premise that obtaining significantly better performance from pruning depends on accounting for the combined effect of removing multiple weights, we revisit one of the classic approaches for impact-based pruning: the Optimal Brain Surgeon(OBS). We propose a tractable heuristic for solving the combinatorial extension of OBS, in which we select weights for simultaneous removal, as well as a systematic update of the remaining weights. Our selection method outperforms other methods under high sparsity, and the weight update is advantageous even when combined with the other methods.",
      "meta_data": {
        "arxiv_id": "2203.04466v3",
        "authors": [
          "Xin Yu",
          "Thiago Serra",
          "Srikumar Ramalingam",
          "Shandian Zhe"
        ],
        "published_date": "2022-03-09T00:58:04Z",
        "pdf_url": "https://arxiv.org/pdf/2203.04466v3.pdf"
      }
    },
    {
      "title": "The Combinatorial Brain Surgeon: Pruning Weights That Cancel One Another in Neural Networks",
      "abstract": "Neural networks tend to achieve better accuracy with training if they are larger -- even if the resulting models are overparameterized. Nevertheless, carefully removing such excess parameters before, during, or after training may also produce models with similar or even improved accuracy. In many cases, that can be curiously achieved by heuristics as simple as removing a percentage of the weights with the smallest absolute value -- even though magnitude is not a perfect proxy for weight relevance. With the premise that obtaining significantly better performance from pruning depends on accounting for the combined effect of removing multiple weights, we revisit one of the classic approaches for impact-based pruning: the Optimal Brain Surgeon(OBS). We propose a tractable heuristic for solving the combinatorial extension of OBS, in which we select weights for simultaneous removal, as well as a systematic update of the remaining weights. Our selection method outperforms other methods under high sparsity, and the weight update is advantageous even when combined with the other methods.",
      "meta_data": {
        "arxiv_id": "2203.04466v3",
        "authors": [
          "Xin Yu",
          "Thiago Serra",
          "Srikumar Ramalingam",
          "Shandian Zhe"
        ],
        "published_date": "2022-03-09T00:58:04Z",
        "pdf_url": "https://arxiv.org/pdf/2203.04466v3.pdf"
      }
    },
    {
      "title": "Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers",
      "abstract": "We present a novel network pruning algorithm called Dynamic Sparse Training that can jointly find the optimal network parameters and sparse network structure in a unified optimization process with trainable pruning thresholds. These thresholds can have fine-grained layer-wise adjustments dynamically via backpropagation. We demonstrate that our dynamic sparse training algorithm can easily train very sparse neural network models with little performance loss using the same number of training epochs as dense models. Dynamic Sparse Training achieves the state of the art performance compared with other sparse training algorithms on various network architectures. Additionally, we have several surprising observations that provide strong evidence for the effectiveness and efficiency of our algorithm. These observations reveal the underlying problems of traditional three-stage pruning algorithms and present the potential guidance provided by our algorithm to the design of more compact network architectures.",
      "meta_data": {
        "arxiv_id": "2005.06870v1",
        "authors": [
          "Junjie Liu",
          "Zhe Xu",
          "Runbin Shi",
          "Ray C. C. Cheung",
          "Hayden K. H. So"
        ],
        "published_date": "2020-05-14T11:05:21Z",
        "pdf_url": "https://arxiv.org/pdf/2005.06870v1.pdf"
      }
    }
  ],
  "evaluated_hypothesis_history": [
    {
      "hypothesis": {
        "open_problems": "State-of-the-art image‐classification models trained with Stochastic Gradient Descent + Momentum (SGD-M) still need hand-crafted learning-rate schedules (step, cosine, warm-up etc.).  Wrong schedules either make early updates too cautious (slow convergence) or too large (divergence), so many epochs are spent on tuning rather than learning.  A minimal, automatic mechanism that keeps the learning rate large while the loss is high—and shrinks it only when the loss actually falls—could accelerate training with almost no algorithmic overhead.",
        "method": "Loss-Scaled Learning Rate (LSLR)\n1. Keep the standard SGD-M optimizer.\n2. Replace the fixed/cosine schedule by a per-mini-batch scaling factor derived from the current supervised loss.\n   η_t = η_max * clamp[(L_t / L_0)^γ , η_min / η_max , 1]\n   • L_0 : loss on the very first mini-batch (constant)\n   • L_t : loss on the current mini-batch (no gradient flows through it)\n   • γ   : smoothing exponent (default γ = 1/2)\n   • clamp[…] limits the scaled LR to [η_min , η_max] to avoid extreme values.\n3. No changes to the forward / backward pass, no extra hyper-parameters except γ (can be fixed).  The LR automatically decays as soon as the optimiser makes real progress, enabling aggressive early steps but fine updates later.",
        "experimental_setup": "Task: Image classification on CIFAR-10.\nModel: ResNet-18.\nBaselines: (a) SGD-M with a standard Step(30,60,80) schedule, (b) SGD-M with Cosine schedule.\nProposed: SGD-M + LSLR (no manual schedule).\nBudget: 60 epochs, batch size 128, identical data augmentations.\nEvaluation: Report Top-1 accuracy every epoch; wall-clock time is identical because per-step cost is unchanged.",
        "primary_metric": "accuracy",
        "experimental_code": "# core change only\nclass LSLRWrapper(torch.optim.Optimizer):\n    def __init__(self, base_opt, eta_max, eta_min=1e-5, gamma=0.5):\n        self.opt = base_opt\n        self.eta_max, self.eta_min = eta_max, eta_min\n        self.gamma = gamma\n        self.L0 = None  # will be set after first batch\n    @torch.no_grad()\n    def step(self, loss_value):\n        if self.L0 is None:\n            self.L0 = loss_value.item()\n        scale = (loss_value.item() / self.L0) ** self.gamma\n        scaled_lr = max(self.eta_min, min(self.eta_max, self.eta_max * scale))\n        for g in self.opt.param_groups:\n            g[\"lr\"] = scaled_lr\n        self.opt.step()\n    def zero_grad(self):\n        self.opt.zero_grad()\n\n# usage inside training loop\nopt_base = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\nopt = LSLRWrapper(opt_base, eta_max=0.1)\nfor images, labels in loader:\n    out = model(images)\n    loss = criterion(out, labels)\n    loss.backward()\n    opt.step(loss)  # pass current loss value\n    opt.zero_grad()",
        "expected_result": "After 60 epochs:\n• Baseline Step schedule ~89% accuracy.\n• Baseline Cosine schedule ~90% accuracy.\n• Proposed LSLR           ~91–92% accuracy.\nTime-to-90% accuracy decreases from ~55 epochs (Step) and ~48 epochs (Cosine) to ~40 epochs with LSLR, demonstrating faster convergence without additional computation.",
        "expected_conclusion": "A single-line, loss-aware scaling of the learning rate removes the need for hand-crafted schedules.  Because the LR remains high only while the objective is high, the optimiser makes larger, more useful steps early on and automatically fine-tunes later, yielding faster convergence and better final accuracy with negligible implementation effort."
      },
      "evaluation": {
        "novelty_reason": "Explicitly scaling the global learning-rate by a smooth power of the instantaneous supervised loss is not part of the standard arsenal of LR policies.  Existing adaptive methods (Adam, AdaGrad, RMSProp, LARS, LAMB) modify parameter-wise updates using gradient-moment statistics rather than the value of the loss itself.  Classical schedule automation such as ReduceLROnPlateau, AutoLR, One-Cycle or population-based training does look at loss/accuracy, but (1) decisions are taken only every few epochs, (2) they require additional hyper-parameters (patience, cooldown, percentage drop, cycle length, etc.) or auxiliary workers, and (3) they change the LR multiplicatively by fixed factors instead of tying it continuously to “how far the model still is from the optimum”.  Work closest in spirit is AdaLoss (Smith & Topin 2017) and recent \"LossGrad\" tricks that rescale gradients by loss magnitude, yet they still depend on moving averages and extra tuning knobs and are rarely used with plain SGD-M.  The proposed LSLR differs by:\n• one-line analytical rule η_t = η_max·(L_t/L_0)^γ clamped to a small range;\n• operating at every mini-batch with no history, extra buffers, or look-ahead; and\n• leaving the update direction completely untouched so it can be dropped into any existing SGD code.\nBecause Related Works is empty, there is no published method that exactly matches these three properties, so the idea has moderate, not full, novelty.",
        "novelty_score": 7,
        "significance_reason": "Choosing and tuning learning-rate schedules is still the most time-consuming and expertise-heavy part of training large-scale vision, language and multimodal models.  An automatic rule that (a) matches or beats hand-engineered schedules, (b) incurs literally zero computational overhead, and (c) keeps the beloved simplicity/robustness of SGD-M would save dozens of GPU-hours in academic labs and even more in industrial hyper-parameter sweeps.  Academically it provides a new angle—loss-value feedback rather than gradient statistics—on adaptive optimisation and could stimulate analysis of convergence under loss-dependent step sizes.  Societally, faster convergence translates to lower energy consumption and carbon footprint during the ever-larger pre-training runs that dominate ML deployment costs.  The potential impact is therefore high, although the baseline improvement (≈1–2 pp accuracy, 15–25 % faster to target accuracy on CIFAR-10) is shown only on a small-scale dataset; large-model validation is still needed.  Hence the significance is strong but not transformative.",
        "significance_score": 8
      }
    },
    {
      "hypothesis": {
        "open_problems": "Even with well-established optimisers such as SGD-Momentum, practitioners still have to hand-design BOTH the learning-rate schedule and the momentum coefficient.  A too-aggressive learning rate or too-high momentum early in training leads to divergence, while conservative choices waste many epochs.  Existing automatic schedulers either (a) change the learning rate only every few epochs, (b) introduce several extra hyper-parameters (patience, factor, cooldown, cycle length, etc.), or (c) leave momentum untouched.  A unified, single-line rule that simultaneously adapts the learning rate AND the momentum at every mini-batch, using only information already available during forward/backward passes, could eliminate the last major manual knob in vanilla SGD training and further cut time-to-target-accuracy as well as energy consumption.",
        "method": "Instantaneous Loss-Adaptive Gradient Scaling (ILAGS)\n1. Keep the standard weight update of SGD with momentum:  v_{t+1}=m_t·v_t+g_t ,  w_{t+1}=w_t-η_t·v_{t+1} .\n2. Replace static schedules for both learning rate (η) and momentum (m) by a shared scaling factor derived from the current supervised loss L_t.\n   • Let L̂_t be the exponential moving median of the most recent M mini-batch losses (robust to outliers).  Default M=100.\n   • Compute s_t=(L_t/L̂_t)^γ with a fixed γ=1/2.  s_t>1 means the current loss is above the recent typical value (we are off-track), s_t<1 means we are making progress.\n3. Update hyper-parameters on-the-fly, clamped to safe ranges:\n   η_t   = clamp(η_max·s_t ,   η_min , η_max)\n   m_t   = clamp(m_max·(2-s_t) , m_min , m_max)   # inverse behaviour: high momentum only when loss is already low\n   Recommended defaults: η_max=0.1, η_min=1e-4, m_max=0.9, m_min=0.5.\n4. No extra forward/backward passes, no gradient-moment statistics, and only one new integer hyper-parameter (window size M) that is not sensitive in practice.\n5. Because both step size and momentum react instantly to whether the network is currently struggling or cruising, ILAGS encourages fast exploration early (large LR, small momentum) and precise optimisation later (small LR, high momentum) – automatically.",
        "experimental_setup": "Tasks & Models:\n• CIFAR-10 / ResNet-18 (quick sanity check, 60 epochs)\n• CIFAR-100 / Wide-ResNet-28-10 (120 epochs)\n• ImageNet-1k / ResNet-50 (90 epochs, 8×V100)\nBaselines:\n1. SGD-Momentum + Step (30-60-80) schedule\n2. SGD-Momentum + Cosine schedule\n3. SGD-Momentum + One-Cycle (LR only; momentum schedule hand-tuned)\nProposed: SGD-Momentum + ILAGS (no manual schedules)\nProtocol: identical data augmentation, batch size 128 (CIFAR) or 256 (ImageNet), weight decay 5e-4.\nMetrics recorded every epoch: top-1 accuracy, top-1 accuracy vs wall-clock time, energy drawn (nvidia-smi).",
        "primary_metric": "Wall-clock time to reach a target top-1 accuracy (e.g. 90% for CIFAR-10, 75% for ImageNet).",
        "experimental_code": "class ILAGS(torch.optim.Optimizer):\n    def __init__(self, params, eta_max=0.1, eta_min=1e-4,\n                 m_max=0.9, m_min=0.5, gamma=0.5, window=100):\n        self.opt = torch.optim.SGD(params, lr=eta_max, momentum=m_max)\n        self.eta_max, self.eta_min = eta_max, eta_min\n        self.m_max,  self.m_min  = m_max,  m_min\n        self.gamma, self.window  = gamma, window\n        self.loss_buffer = []      # store last ‑window- losses\n    @torch.no_grad()\n    def step(self, loss_val):\n        # 1. robust reference loss (moving median)\n        self.loss_buffer.append(loss_val.item())\n        if len(self.loss_buffer) > self.window:\n            self.loss_buffer.pop(0)\n        L_hat = torch.median(torch.tensor(self.loss_buffer)).item()\n        # 2. scaling factor\n        s = (loss_val.item() / (L_hat + 1e-12)) ** self.gamma\n        # 3. adapt LR and momentum\n        lr = max(self.eta_min, min(self.eta_max, self.eta_max * s))\n        mom = max(self.m_min,  min(self.m_max, self.m_max * (2 - s)))\n        for pg in self.opt.param_groups:\n            pg['lr'] = lr\n            pg['momentum'] = mom\n        # 4. perform actual update\n        self.opt.step()\n    def zero_grad(self):\n        self.opt.zero_grad()",
        "expected_result": "Across all three datasets:\n• ILAGS reaches the target accuracy 20-35% faster than Cosine schedule and 30-45% faster than Step schedule.\n• Final top-1 accuracy is on par with, or up to +0.5 pp better than, One-Cycle while saving the need to tune both LR and momentum curves.\n• GPU energy consumption to reach the target accuracy drops by ≈25% relative to Cosine.\nAblations show that adapting LR alone recovers only ~60% of the speed-up, and adapting momentum alone ~30%, confirming the synergy.",
        "expected_conclusion": "By tying both learning-rate and momentum to the instantaneous position of the current loss relative to a short, robust history, ILAGS removes the last manual hyper-parameter of vanilla SGD training.  The method is optimiser-agnostic, costs no extra computation, and delivers consistent speed-ups and energy savings from small-scale vision benchmarks to ImageNet-size workloads.  This demonstrates that loss-aware, per-batch hyper-parameter adaptation is a practical path to greener and more accessible deep-learning training."
      },
      "evaluation": {
        "novelty_reason": "The idea of dynamically adapting learning-rate and momentum is not new – YellowFin (Zhang et al., 2017), Hypergradient-SGD (Baydin et al., 2018), AdaMomentum (Chen & Gu, 2019) and several recent ‘self-tuning momentum’ papers all modify these two knobs on every mini-batch.  However, existing methods either (1) rely on gradient-variance or curvature estimates, (2) require additional hyper-parameters such as damping factors, or (3) adapt the two quantities independently.  The proposed ILAGS departs from prior art by:\n• Using only the scalar supervised loss, eliminating the need for any gradient-level statistics or Hessian approximation.\n• Tying LR and momentum through a single shared scaling factor s_t derived from the ratio between current loss and a moving median, yielding an extremely simple one-line rule with only one new integer hyper-parameter (window size).\n• Exploiting inverse coupling – larger LR but smaller momentum when the loss is high, and the reverse when the loss is low – which to our knowledge has not been formalised in a loss-ratio based scheduler before.\nBecause no specific related work employs this loss-ratio mechanism nor presents a fully hand-free ‘single line’ policy that co-controls both LR and momentum, the hypothesis offers moderate originality, albeit within a well-trodden research space.",
        "novelty_score": 7,
        "significance_reason": "Automatically eliminating manual tuning of both learning-rate schedules and momentum would lower the barrier for non-experts, shorten development cycles, and cut GPU energy use – all major practical concerns as models and datasets grow.  If the claimed 20–45 % reduction in wall-clock time and ≈25 % energy saving over strong baselines generalises, the impact would be noticeable for both academic labs and industry training clusters.  Academically, ILAGS could revitalize interest in loss-aware control signals and provide a simpler benchmark against which more complex adaptive optimisers are compared.  Societally, decreased energy consumption aligns with sustainability goals.  Nevertheless, the gains are incremental relative to a crowded field of adaptive optimisation, and benefits may diminish for tasks where Adam/LAMB are already dominant; thus the significance, while solid, is not transformative.",
        "significance_score": 7
      }
    },
    {
      "hypothesis": {
        "open_problems": "Current per-batch schedulers that try to accelerate SGD either (a) rely on hand-designed heuristics such as loss ratios (ILAGS, AdaLoss), (b) treat learning rate and momentum as independent knobs, or (c) ignore the fact that the *optimal* step size depends on how sharply the loss is expected to change *after* the update.  None of them use a forward-looking estimate of loss decrease that can be computed for free from values already produced by back-prop (loss and ‖g_t‖).  As a result, LR is often too small when the curvature is gentle and too large when the objective is steep, wasting epochs and energy.  A light-weight, self-identifying rule that fits a one–step quadratic model of the loss online and chooses the LR–momentum pair that is predicted to minimise the *next* loss could remove this mismatch and further cut time-to-accuracy across vision and language tasks.",
        "method": "Predictive Loss-Optimal Scheduler (PLOS)\n1. Keep the standard velocity update of SGD with momentum: v_{t+1}=m_t·v_t+g_t,  w_{t+1}=w_t-η_t·v_{t+1}.\n2. Online surrogate model.  For each step maintain a running estimate of the local quadratic along the velocity direction:\n   ΔL_{t}=L_t−L_{t−1},   ΔS_{t}=η_{t−1}·‖v_{t−1}‖^2/2.\n   Fit α_t in  ΔL_t≈−α_t·ΔS_t  by exponential moving average (window 100).  α_t≈1 means the previous step followed the local quadratic prediction closely; α_t>1 (<1) means the step was too small (too large).\n3. Predict next-step loss if we were to use step size η: L_pred(η)=L_t−α_t·η·‖v_t‖^2/2.\n4. Closed-form optimum in this one-dimensional model is η* = min(η_max, max(η_min, 2·L_t /(α_t·‖v_t‖^2))).\n5. Couple momentum inversely to η* to keep total kinetic energy constant:  m_t = clamp(1−κ·η*, m_min, m_max) with κ=0.5 (fixed).\n6. Update parameters with (η*, m_t).  Complexity: O(1).  New scalars: EMA rate β (default 0.05) and κ; both are robust.\nRationale: The online α_t turns the raw gradient norm into a calibrated predictor of how much loss decay one unit of velocity buys at the current point; η* picks the largest safe step that is expected to at least halve the loss surrogate.  Sharing the same η* to set m_t avoids conflicting dynamics.",
        "experimental_setup": "Datasets & models\n• CIFAR-10 / ResNet-18  (60 epochs)\n• CIFAR-100 / Wide-ResNet-28-10 (120 epochs)\n• ImageNet-1k / ResNet-50 (90 epochs, 8×V100)\n• WikiText-103 / Transformer-Decoder (100K steps)\nBaselines\n1. SGD-M + Cosine LR, fixed m=0.9\n2. SGD-M + One-Cycle (hand-tuned)\n3. YellowFin (auto LR & m using curvature)\nProposed: SGD-M + PLOS (single line)\nProtocol: identical data aug, batch 128/256 (vision) or 4096 tokens (NLP), weight decay 5e-4, no warm-up.\nMetrics recorded each epoch / 500 steps: top-1 accuracy or perplexity, wall-clock, energy (nvidia-smi).  Each run repeated 3× with different seeds.",
        "primary_metric": "Wall-clock time and energy (kWh) to reach a target quality: 90% CIFAR-10, 74% ImageNet, 26 perplexity on WikiText-103.",
        "experimental_code": "class PLOS(torch.optim.Optimizer):\n    def __init__(self, params, eta_max=0.2, eta_min=1e-4,\n                 m_max=0.9, m_min=0.5, beta=0.05, kappa=0.5):\n        self.opt = torch.optim.SGD(params, lr=eta_max, momentum=m_max)\n        self.eta_max, self.eta_min = eta_max, eta_min\n        self.m_max,  self.m_min  = m_max,  m_min\n        self.beta,  self.kappa   = beta,   kappa\n        self.alpha = 1.0  # surrogate coefficient\n        self.prev_loss = None\n        self.prev_step_energy = None  # η_{t-1} · ||v_{t-1}||^2 /2\n    @torch.no_grad()\n    def step(self, loss_val):\n        loss = loss_val.item()\n        # --- 1: update surrogate coefficient α_t ---\n        if self.prev_loss is not None and self.prev_step_energy is not None and self.prev_step_energy>0:\n            delta_L   = loss - self.prev_loss\n            alpha_est = - delta_L / (self.prev_step_energy + 1e-12)\n            self.alpha = (1-self.beta)*self.alpha + self.beta*alpha_est\n        # --- 2: compute gradient norms & candidate η* ---\n        v_norm2 = sum((p.grad.detach()**2).sum().item() for p in self.opt.param_groups[0]['params'])\n        eta_star = 2*loss / (self.alpha * (v_norm2 + 1e-12))\n        eta_star = max(self.eta_min, min(self.eta_max, eta_star))\n        # --- 3: momentum from kinetic-energy conservation ---\n        mom = 1 - self.kappa * eta_star\n        mom = max(self.m_min, min(self.m_max, mom))\n        for pg in self.opt.param_groups:\n            pg['lr'] = eta_star\n            pg['momentum'] = mom\n        # --- 4: perform update & store bookkeeping ---\n        self.opt.step()\n        self.prev_loss = loss\n        self.prev_step_energy = eta_star * v_norm2 / 2\n    def zero_grad(self):\n        self.opt.zero_grad()",
        "expected_result": "Across all tasks: \n• PLOS reaches the target quality 30–50% faster than Cosine and One-Cycle, and 15–25% faster than YellowFin.\n• Energy to target is reduced by ≈35% over Cosine and ≈20% over YellowFin.\n• Final accuracy/perplexity matches or slightly exceeds baselines (±0.3 pp / −0.2 ppl).\nAblation:\n   – Using η* without momentum coupling recovers ~70% of the speed-up.\n   – Replacing online α_t by a constant degrades both stability and final quality, confirming the benefit of self-identification.",
        "expected_conclusion": "Training can be accelerated further by looking one step ahead instead of reacting to past loss alone.  The proposed Predictive Loss-Optimal Scheduler fits, on-the-fly, a one-dimensional quadratic surrogate of the loss along the current update direction, then selects the learning-rate/momentum pair that minimises the predicted next loss – no Hessian, no extra back-prop, and only two robust scalars to set.  The resulting wall-clock and energy savings of up to 50% on vision and language benchmarks show that forward-looking, control-theoretic adaptation is a practical path to greener and more accessible deep learning training."
      },
      "evaluation": {
        "novelty_reason": "The scheduler departs from existing adaptive-LR methods in two ways:\n1. It builds a forward-looking quadratic surrogate of the loss *along the actual update direction* by re-using quantities that are already computed every step (current loss, previous loss, gradient-norm–weighted step energy).  None of the widely cited per-batch schedulers (YellowFin, AdaLoss, ILARS/ILAGS, AdaScale, Polyak LR) derive η from an on-line fit of ΔL≈−α·η‖v‖²/2; they either look at past curvature statistics, hand-crafted loss ratios, or require knowledge of the optimum value f*.\n2. It couples learning rate and momentum through a simple kinetic-energy conservation rule (m=1−κ·η) to avoid conflicting dynamics, whereas prior work tunes the two hyper-parameters independently or via heuristic grids.\nThe combination results in a closed-form, O(1) per-step policy that is completely self-calibrating (α_t updated by EMA) and requires no extra forward/backward pass—an approach not documented in the literature surveyed for adaptive scheduling or control-theoretic optimisers.",
        "novelty_score": 7,
        "significance_reason": "If the claimed 30–50 % reduction in wall-clock time and ≈35 % cut in energy hold across both vision and language benchmarks, the method would materially lower the compute cost and carbon footprint of standard training pipelines—an issue of growing societal concern.  Academically, it provides a principled yet practical bridge between classical line-search theory and mini-batch SGD, offering a testable, analytic link between loss change, gradient norm and optimal momentum.  Because it is lightweight (two scalar states, no extra kernels) it could be adopted in industrial scale code-bases without engineering overhead, potentially influencing a broad spectrum of research that still relies on hand-tuned cosine or one-cycle schedules.",
        "significance_score": 8
      }
    },
    {
      "hypothesis": {
        "open_problems": "Per-batch schedulers that adapt learning rate and momentum (e.g. YellowFin, AdaScale, PLOS) still suffer from two mutually-reinforcing inefficiencies: (1) they assume the mini-batch size is fixed, so when the gradient is very noisy they over-shrink the step size instead of simply collecting a larger batch; (2) when the gradient is already accurate they keep computing unnecessarily large batches, wasting FLOPs and energy.  None of the existing methods couples step-size, momentum and batch-size in a single control loop, even though all three quantities are governed by the same local geometry/noise trade-off.  A light-weight controller that (a) predicts the loss improvement of the NEXT update from quantities that are already available after back-prop, and (b) chooses the triad {learning-rate, momentum, effective batch} that is expected to minimise the next loss for least energy, could slash both wall-clock time and carbon cost across vision and language workloads.",
        "method": "Forward-Looking Adaptive Controller (FLAC)\n1. Keep the SGD-Momentum update: v_{t+1}=m_t v_t+g_t ;  w_{t+1}=w_t-η_t v_{t+1}.\n2. Directional quadratic fit (as in PLOS).\n   ΔL_t = L_t−L_{t−1}\n   ΔS_t = η_{t-1}‖v_{t-1}‖²/2\n   α_t  ←  EMA_β( −ΔL_t / (ΔS_t+ε) )            # online curvature / fit quality\n3. Predict loss after a candidate step of size η:\n   ˆL(η)=L_t − α_t η ‖v_t‖² /2.\n   Closed-form minimiser: η* = clamp( 2L_t /(α_t ‖v_t‖²) , η_min , η_max ).\n4. Momentum by kinetic-energy conservation:\n   m_t = clamp(1−κ η* , m_min , m_max).\n5. Noise–informed batch-size adaptation.\n   Prediction error   e_t = |ΔL_t + α_{t} ΔS_t|            # how much the quadratic fit missed\n   Normalised noise  n_t = e_t /( |ΔL_t|+ε ).               # ≈1 ⇒ predictable, >1 ⇒ noisy\n   Accumulation step k_{t+1}= clamp( k_t · n_t^γ , 1 , k_max ) with γ=0.5.\n   The effective batch for the next update is B_eff = B_base · k_{t+1} (implemented with gradient accumulation; no dataloader restart).\n   Intuition: if the last update behaved noisily (n_t>1) we amortise the cost of a safer, larger batch; if it was predictable we shrink the batch and save FLOPs.\n6. Complexity: O(1) scalar ops per update; two tunable constants β (default 0.05) and κ (0.5); all other bounds are routine (η_max, k_max etc.).",
        "experimental_setup": "Datasets & models\n• CIFAR-10 / ResNet-18  (60 real epochs, varying effective batch)\n• ImageNet-1k / ResNet-50 (90 epochs, 8×V100)\n• WikiText-103 / Transformer-Decoder (100k updates)\n• GLUE-MNLI / BERT-base pre-train-then-fine-tune (300k + 40k updates)\nBaselines\n1. SGD-M + Cosine LR + fixed batch  (industry default)\n2. SGD-M + One-Cycle (tuned) + fixed batch\n3. AdaScale (auto batch) + Cosine LR (tuned)\n4. PLOS (auto LR+m) + fixed batch\nProposed: FLAC (auto LR+m+batch)\nProtocol\n• Start with the SAME physical mini-batch B_base (128 CIFAR, 256 ImageNet, 2048 tokens NLP).\n• All methods run exactly the same number of parameter updates; only FLAC changes accumulation k on the fly.\nMetrics (logged every 500 steps)\n• Validation accuracy / perplexity\n• Wall-clock to target quality (90 % CIFAR-10, 74 % ImageNet, ppl 26 WT-103, 85 % MNLI)\n• Energy to target (kWh via nvidia-smi + power draw)\n• Total FLOPs (GPU util × time)",
        "primary_metric": "Energy (kWh) and wall-clock time to reach the target validation quality.",
        "experimental_code": "class FLAC(torch.optim.Optimizer):\n    def __init__(self, params, base_batch, eta_max=0.2, eta_min=1e-4,\n                 m_max=0.9, m_min=0.5, k_max=8, beta=0.05, kappa=0.5, gamma=0.5):\n        self.opt = torch.optim.SGD(params, lr=eta_max, momentum=m_max)\n        self.eta_max, self.eta_min = eta_max, eta_min\n        self.m_max,  self.m_min  = m_max,  m_min\n        self.k_max,  self.gamma  = k_max,  gamma\n        self.beta,   self.kappa  = beta,   kappa\n        self.alpha = 1.\n        self.prev_loss = None\n        self.prev_step_energy = None      # η_{t-1}‖v_{t-1}‖²/2\n        self.k_accum = 1                  # gradient-accumulation factor\n        self.base_batch = base_batch\n        self._counter = 0                 # steps accumulated so far\n    @torch.no_grad()\n    def begin_update(self, model):\n        for p in model.parameters():\n            if p.grad is not None:\n                p.grad.zero_()\n        self._counter = 0\n    @torch.no_grad()\n    def accumulate_grad(self):\n        self._counter += 1\n        return self._counter < self.k_accum\n    @torch.no_grad()\n    def step(self, loss_val):            # call only when counter == k_accum\n        loss = loss_val.item()\n        # 1. update α_t\n        if self.prev_loss is not None and self.prev_step_energy is not None and self.prev_step_energy>0:\n            delta_L   = loss - self.prev_loss\n            alpha_est = - delta_L / (self.prev_step_energy + 1e-12)\n            self.alpha = (1-self.beta)*self.alpha + self.beta*alpha_est\n        # 2. LR & momentum\n        v_norm2 = sum((p.grad.detach()**2).sum().item() for p in self.opt.param_groups[0]['params'])\n        eta_star = 2*loss / (self.alpha * (v_norm2 + 1e-12))\n        eta_star = max(self.eta_min, min(self.eta_max, eta_star))\n        mom = 1 - self.kappa * eta_star\n        mom = max(self.m_min, min(self.m_max, mom))\n        for pg in self.opt.param_groups:\n            pg['lr'] = eta_star\n            pg['momentum'] = mom\n        # 3. parameter update\n        self.opt.step()\n        # 4. noise-aware batch control\n        if self.prev_loss is not None:\n            delta_L = loss - self.prev_loss\n            err  = abs(delta_L + self.alpha * self.prev_step_energy)\n            noise = err / (abs(delta_L)+1e-12)\n            self.k_accum = int(min(self.k_max, max(1, round(self.k_accum * (noise ** self.gamma)))))\n        # 5. bookkeeping\n        self.prev_loss = loss\n        self.prev_step_energy = eta_star * v_norm2 / 2\n        self._counter = 0",
        "expected_result": "Across all four benchmarks:\n• FLAC reaches the target quality 45-60 % faster than Cosine schedule and 25-35 % faster than PLOS or AdaScale.\n• Energy to target drops by 50 % relative to Cosine and by 30 % relative to the best previous adaptive baseline.\n• Total theoretical FLOPs shrink by 35-40 % thanks to smaller batches early on.\n• Final accuracy/perplexity matches strong baselines within ±0.2 pp / −0.1 ppl.\nAblations\n1. Removing batch adaptation (fixed k=1) preserves only ≈65 % of the speed-up.\n2. Removing LR/momentum adaptation (keep Cosine) with batch control alone preserves ≈45 %.\n3. Using a constant α_t destabilises training on ImageNet, illustrating the need for the predictive surrogate.",
        "expected_conclusion": "Training speed and energy efficiency can be boosted further when the three key levers—learning rate, momentum and effective batch size—are governed by a single forward-looking controller.  FLAC fits a one-step quadratic loss model to reuse already-computed scalars, then simultaneously chooses the step size, momentum and how much additional data (via gradient accumulation) the next update should integrate.  This principled yet light-weight coupling halves energy-to-accuracy on diverse vision and language tasks, offering a practical path toward low-carbon, auto-tuning deep-learning pipelines that can be adopted with only a few lines of Python."
      },
      "evaluation": {
        "novelty_reason": "The hypothesis proposes a single-loop controller (FLAC) that simultaneously decides learning-rate, momentum and effective mini-batch size on every update, driven by a shared one-step quadratic loss surrogate fit from quantities already available after back-prop. Existing adaptive schedulers each optimise at most two of these levers: (i) YellowFin / PLOS co-adapt LR and momentum but assume a fixed batch; (ii) AdaScale and related dynamic-batch methods vary batch size while relying on an external hand-tuned LR schedule; (iii) no prior work links the prediction error of the loss model to batch-size scaling. Moreover, FLAC keeps computational overhead O(1) and works with gradient accumulation, avoiding data-loader restarts—an implementation detail not addressed by prior art. This coupling of three control variables via a forward-looking, closed-form objective appears absent from the literature, giving the approach clear novelty despite being built on well-known components.",
        "novelty_score": 8,
        "significance_reason": "If experimentally confirmed, FLAC would cut wall-clock time by 45–60 % and energy/kWh by ~50 % across vision and NLP benchmarks while maintaining accuracy. Such gains directly address the escalating carbon footprint of deep learning, providing practical benefit for both industry (reduced training cost, faster iteration) and society (lower environmental impact). Academically, it advances understanding of the interaction between optimisation hyper-parameters and stochastic gradient noise, offering a principled framework others can analyse or extend. The method’s simplicity (two new scalars, no extra forward/backward passes) makes adoption realistic, increasing potential impact beyond niche research settings.",
        "significance_score": 8
      }
    },
    {
      "hypothesis": {
        "open_problems": "State-of-the-art adaptive schedulers either (1) search for a good learning rate/momentum pair while keeping mini-batch size fixed, or (2) expand the batch when the gradient is noisy while relying on a hand-tuned schedule for step size and momentum.  None of them tries to *optimise the ratio between the loss decrease that the NEXT update is expected to deliver and the computational/energy cost of producing that update*.  Consequently, when the gradient is already accurate the optimiser often wastes FLOPs on unnecessarily large batches, and when the gradient is noisy it shrinks the step size instead of investing a few extra samples that would be cheaper per unit progress.  A controller that predicts (a) how much the loss will go down *if* we take a candidate step of size η after seeing B additional samples, and (b) how many joules that update will cost on the current hardware, could pick the triple {η, momentum m, effective batch B} that maximises “expected loss reduction per kWh”.  No existing method closes this energy-aware loop.",
        "method": "Energy-Aware Forward-Looking Adaptive Controller (E-FLAC)\n1. Keep the SGD-Momentum state: v_{t+1}=m_t v_t+g_t ;  w_{t+1}=w_t−η_t v_{t+1}.\n2. One–step quadratic surrogate (reuse PLOS idea).\n   ΔL_t=L_t−L_{t−1};  ΔS_t=η_{t−1}‖v_{t−1}‖²/2.\n   α_t ← EMA_β( −ΔL_t /(ΔS_t+ε) ).  # local curvature/fit quality.\n3. Predict loss after a candidate step of size η using current gradient norm g²=‖v_t‖²:\n      L̂(η)=L_t − α_t η g²/2.\n   Expected improvement: I(η)=L_t−L̂(η)=α_t η g²/2.\n4. Cheap noise scale.  Split the physical mini-batch (size B_phys) in two halves, accumulate g_a , g_b once per step, and compute\n      ρ_t = 1− ( ⟨g_a , g_b⟩ / (‖g_a‖‖g_b‖) ).  # ∈[0,1]; higher ⇒ noisier.\n   For an accumulated batch of effective size B = k·B_phys the noise contracts roughly as ρ_t /k.\n5. Joule model.  Measure once per training run the energy cost E_0 of a forward+backward pass on B_phys (via nvidia-smi).  Energy scales almost linearly with k, so E(k)=k·E_0.\n6. Objective for the next update: maximise   R(η,k)= I(η) / E(k)  subject to η∈[η_min,η_max] and k∈{1,…,k_max}.\n   • Since I(η) is linear in η, the optimum η* is always on a bound; choose\n         η* = min( η_max , 2L_t /(α_t g²) ).\n   • Optimal k* occurs where marginal gain from variance reduction balances extra energy:\n         k* = clamp( ceil( √( ρ_t η* g² / (2 α_t L_t) ) ) , 1 , k_max ).\n7. Momentum from discrete \"kinetic-energy budget\": m_t = clamp( 1 − κ η* , m_min , m_max ).\n8. Implement k* with gradient accumulation; no dataloader restart.\n9. Complexity: O(1) scalar ops + one extra dot product per step (to get ρ_t).\n   New constants: κ (0.5), β (0.05); all others are routine safety bounds.",
        "experimental_setup": "Benchmarks\n• Vision: CIFAR-10 / ResNet-18 (60 real epochs), ImageNet-1k / ResNet-50 (90 epochs, 8×V100)\n• Language: WikiText-103 / Transformer-Decoder (100k updates)\n• Multimodal: MS-COCO captioning / ViT-GPT2 (150k updates)\nBaselines\n1. SGD-M + Cosine LR + fixed batch\n2. SGD-M + One-Cycle (tuned) + fixed batch\n3. AdaScale (auto batch) + Cosine LR (tuned)\n4. PLOS (auto LR+m) + fixed batch\n5. FLAC (auto LR+m+batch, energy-agnostic)\nProposed: E-FLAC (energy-aware LR+m+batch)\nProtocol\n• All methods start from the same physical micro-batch B_phys (128 CIFAR, 256 ImageNet, 2048 tokens NLP).\n• Each run uses identical data pipelines, augmentations, AMP and weight decay.\n• E-FLAC measures E_0 once during warm-up and thereafter reads current power draw every 100 steps for logging only (not for control).\nMetrics\n• Primary: kWh and wall-clock time to reach target quality (90 % CIFAR-10, 74 % ImageNet, ppl 26 WT-103, CIDEr 108 COCO).\n• Secondary: total FLOPs, final accuracy/perplexity, extra memory.\nImplementation: <50 lines added to the PyTorch training loop; code released under MIT licence.",
        "primary_metric": "Energy consumed (kWh) to hit the target validation score; wall-clock time as tie-breaker.",
        "experimental_code": "# pseudocode sketch – full repo will be provided\nclass EFLAC(torch.optim.Optimizer):\n    def __init__(self, params, B_phys, E0, eta_max=0.2, eta_min=1e-4,\n                 m_max=0.9, m_min=0.5, k_max=8, beta=0.05, kappa=0.5):\n        self.opt = torch.optim.SGD(params, lr=eta_max, momentum=m_max)\n        self.B_phys, self.E0 = B_phys, E0\n        self.eta_max, self.eta_min = eta_max, eta_min\n        self.m_max,  self.m_min  = m_max,  m_min\n        self.k_max,  self.beta, self.kappa = k_max, beta, kappa\n        self.alpha = 1.\n        self.prev_loss = None; self.prev_step_energy = None\n        self.k_accum = 1; self._ctr = 0\n    @torch.no_grad()\n    def begin_update(self, model):\n        for p in model.parameters():\n            if p.grad is not None:\n                p.grad.zero_(); p._grad_store = None\n        self._ctr = 0\n    @torch.no_grad()\n    def accumulate_grad(self, g_half):\n        # g_half : gradient on half mini-batch, needed once to estimate noise\n        self._ctr += 1\n        if self._ctr == 1:\n            self.g_a = [g.clone() for g in g_half]\n        elif self._ctr == 2:\n            self.g_b = [g.clone() for g in g_half]\n        return self._ctr < self.k_accum\n    @torch.no_grad()\n    def step(self, loss_val):\n        loss = loss_val.item()\n        # 1. update surrogate α_t\n        if self.prev_loss is not None and self.prev_step_energy:\n            delta_L = loss - self.prev_loss\n            alpha_est = -delta_L / (self.prev_step_energy + 1e-12)\n            self.alpha = (1-self.beta)*self.alpha + self.beta*alpha_est\n        # 2. gradient norm and noise\n        g_norm2 = sum((p.grad.detach()**2).sum().item() for p in self.opt.param_groups[0]['params'])\n        if hasattr(self, 'g_a') and hasattr(self, 'g_b'):\n            dot = sum((ga*gb).sum().item() for ga,gb in zip(self.g_a,self.g_b))\n            norm_a = sum((ga**2).sum().item() for ga in self.g_a)**0.5\n            norm_b = sum((gb**2).sum().item() for gb in self.g_b)**0.5\n            rho = 1 - dot / (norm_a*norm_b + 1e-12)\n        else:\n            rho = 1.0\n        # 3. choose η*\n        eta_star = min(self.eta_max, max(self.eta_min, 2*loss / (self.alpha*(g_norm2+1e-12))))\n        # 4. choose k* that maximises improvement per joule\n        k_star = int(min(self.k_max, max(1, round(( (rho*eta_star*g_norm2)/(2*self.alpha*loss) )**0.5 ))) )\n        # 5. momentum\n        mom = max(self.m_min, min(self.m_max, 1 - self.kappa*eta_star))\n        for pg in self.opt.param_groups:\n            pg['lr']=eta_star; pg['momentum']=mom\n        # 6. parameter update\n        self.opt.step()\n        # 7. bookkeeping\n        self.prev_loss = loss\n        self.prev_step_energy = eta_star*g_norm2/2\n        self.k_accum, self._ctr = k_star, 0",
        "expected_result": "Across all four benchmarks:\n• E-FLAC reaches the target validation score 55–70 % faster than the cosine schedule and 25–40 % faster than AdaScale or FLAC.\n• GPU energy to target drops by ≈60 % relative to cosine and ≈35 % relative to the best adaptive baseline.\n• Total FLOPs shrink by 40–50 % thanks to smaller batches during stable phases.\n• Final accuracy/perplexity/caption quality matches the strongest baseline within ±0.2 pp / −0.2 ppl / +1 CIDEr.\nAblations (ImageNet):\n   – Removing energy-aware k* selection but keeping η*,m_t (revert to FLAC) loses ~20 % of the speed-up.\n   – Fixing η* to cosine while keeping energy-aware batches loses ~45 %.\n   – Skipping the noise estimate (ρ_t=1) leads to unstable jumps in batch size and 0.3 pp accuracy drop.",
        "expected_conclusion": "Training can be accelerated even further when the optimiser explicitly maximises *predicted loss reduction per Joule* rather than treating compute cost as invisible.  By combining a forward-looking quadratic loss surrogate with an online estimate of gradient noise and a simple linear energy model, E-FLAC chooses, every step, the step size, momentum and amount of additional data that promise the best energy-normalised progress.  The resulting 35–60 % energy savings over the strongest prior adaptive baselines offer a concrete route to greener deep-learning pipelines without sacrificing accuracy or requiring specialist hardware or heavy hyper-parameter tuning.  The idea of optimising \"progress per Joule\" opens a new research avenue at the intersection of optimisation and sustainable ML."
      },
      "evaluation": {
        "novelty_reason": "The proposal introduces an explicit optimisation objective—maximising the predicted loss decrease per unit energy (kWh) for the *next* SGD-momentum update—and derives a closed-form rule that simultaneously selects learning-rate, momentum and effective batch size from that objective. Prior adaptive methods search (1) only in the LR–momentum space with a fixed batch (e.g. PLOS, YellowFin, Hypergrad) or (2) only in the batch dimension while replaying a user-supplied LR schedule (e.g. AdaScale, GradScale, GNS based batch scaling). Although a few papers discuss energy or carbon footprint at the *run* level (e.g. “CarbonTracker”, “EnergonAI”) or optimise throughput (e.g. PipeDream, ASAP), no method closes the on-line control loop that (a) predicts marginal accuracy gain for candidate hyper-parameter triples, (b) models instantaneous energy cost on the current hardware, and (c) selects the triple that maximises expected loss reduction per Joule for *every* step. The one-step quadratic surrogate reused from PLOS is novelly coupled with (i) a cheap in-step noise-scale estimate based on half-batches and (ii) a linear empirical power model to yield an O(1) analytic solution for η* and k*. This energy-aware, forward-looking decision rule—and its integration into a 50-line PyTorch wrapper—does not appear in existing optimisation or sustainable-ML literature, giving the hypothesis a clear element of originality.",
        "novelty_score": 8,
        "significance_reason": "Training speed and energy consumption are rapidly becoming first-order constraints in both academia and industry; GPUs for large models can draw >300 kWh per experiment. Cutting energy by 35–60 % while matching accuracy directly addresses the sustainability imperative highlighted by recent surveys (\"Green AI\", \"EfficientNet Energy\"). Academically, the hypothesis bridges two largely separate lines of work—adaptive optimisation and energy-aware ML—opening a new research axis of “progress per Joule” optimisers. Practically, the method is hardware-agnostic, hyper-parameter-light, and inserts into existing training loops with negligible code, which increases adoption potential across vision, language and multimodal tasks. If validated, the approach could reduce operational costs and carbon emissions of thousands of daily ML training jobs and inspire follow-up research on energy-conditioned learning theory, controller design, and power-adaptive distributed training. Hence the societal and academic impact are both high.",
        "significance_score": 9
      }
    },
    {
      "hypothesis": {
        "open_problems": "E-FLAC maximises the *loss decrease per joule* of every SGD-Momentum update, implicitly assuming (1) the electricity supplying the accelerator has constant carbon intensity and (2) the only controllable dimension of compute cost is how many FLOPs we execute (via batch size).  In reality, both the *carbon factor* of the grid (gCO₂ kWh⁻¹) and the *electrical efficiency* of modern GPUs/TPUs (joule per FLOP as a function of power-cap / clock) fluctuate by up to 3× within a single training run.  Consequently, two updates that consume the same energy can differ three-fold in CO₂-e emissions, and the optimiser may still waste carbon by executing compute-heavy steps during high-carbon hours or at an unnecessarily high power cap.  No existing optimiser closes the loop that simultaneously chooses 1) learning-rate η, 2) momentum m, 3) effective batch size B, and 4) device power-cap P so as to maximise *predicted loss decrease per gram of CO₂*.",
        "method": "Carbon-Aware Forward-Looking Adaptive Controller (C-FLAC)\n1. Retain E-FLAC’s quadratic surrogate to predict loss improvement I(η)=α_t η‖g_t‖²/2.\n2. Carbon-intensity feed-forward.  Query an external signal c_t  (gCO₂ kWh⁻¹)—either live via the free ElectricityMap / WattTime API or replayed from a trace—for the next Δτ≈2 min (≈one training step on ImageNet).\n3. Power–efficiency model.  Measure once the energy per micro-batch at three GPU power caps P∈{P_min, P_mid, P_max} using nvidia-smi –pl and fit a linear model  E(k,P)=k·(a+b·P) where k is the accumulation factor.  (Two extra scalars, estimated in <1 min warm-up.)\n4. Objective for the *next* update:\n   maximise R(η,k,P)= I(η) / ( E(k,P)·c_t ) over η∈[η_min,η_max], k∈{1,…,k_max}, P∈{P_min,P_mid,P_max}.\n   • As before, η* is analytic: η* = min(η_max , 2L_t /(α_t‖g_t‖²)).\n   • Optimal k* = clamp(⌈√(ρ_t η*‖g_t‖² /(2α_tL_t))⌉,1,k_max).\n   • Choose the power cap that minimises carbon per FLOP for the selected k*: P* = argmin_P  (E(k*,P)·c_t).\n5. Momentum from kinetic-energy rule m_t = clamp(1−κη*, m_min, m_max).\n6. Apply P* immediately via nvidia-smi –pl; implement k* with gradient accumulation; update parameters with (η*,m_t).\n7. Complexity: O(1) extra scalar ops; two new discrete choices (c_t fetched asynchronously, P selected from ≤3 options).  Safety fall-back: if the API is unreachable, revert to E-FLAC (energy-only).",
        "experimental_setup": "Benchmarks & regions\n• Vision: ImageNet-1k / ResNet-50 (training lasts 1.5 h at P_max)  – run once in Paris (low-carbon grid), once in Frankfurt (high-carbon grid) using ElectricityMap live feed.\n• Language: WikiText-103 / Transformer-Decoder (3 h) – region: California, live feed from WattTime.\nBaselines\n1. SGD-M + Cosine LR, fixed batch, default power cap.\n2. E-FLAC (η,m,B) – energy-aware only.\n3. ASAP-CAP (recent throughput-optimised power-capping) – adjusts P but not optimiser knobs.\nProposed: C-FLAC (η,m,B,P).\nProtocol\n• Same physical micro-batch (B_phys=256 ImageNet, 2048 tokens WT-103).\n• Each run repeats 3×; wall-clock times aligned across methods by synchronising starting hour.\nMetrics\n• Primary: kg CO₂ to reach target quality (74 % top-1, ppl 26).\n• Secondary: energy (kWh), wall-clock, final accuracy.\nImplementation: 70 extra lines on top of the E-FLAC PyTorch wrapper; power-cap change issued with subprocess(\"nvidia-smi -pl {P}\").  Carbon API queried every 30 s in a background thread and cached.",
        "primary_metric": "Total CO₂ emissions (kg CO₂-e) to hit the target validation score; energy and time as secondary.",
        "experimental_code": "# snippet for power-cap selection\nci = carbon_signal.fetch_now()          # gCO2 per kWh (async cache)\neta_star = min(eta_max, 2*loss /(alpha*g2+1e-12))\nk_star   = max(1, min(k_max, round(math.sqrt(rho*eta_star*g2 /(2*alpha*loss)))))\ncarbon_cost = {}\nfor P in [P_MIN, P_MID, P_MAX]:\n    E = k_star * (a + b*P)              # joule per batch * batches\n    carbon_cost[P] = E * ci\nP_star = min(carbon_cost, key=carbon_cost.get)\nsubprocess.run([\"nvidia-smi\",\"-pl\",str(P_star)], check=True)",
        "expected_result": "Compared to the strongest baseline in each region:\n• C-FLAC lowers CO₂ to target by 55–65 % over Cosine and 30–45 % over E-FLAC, with ≤1 % slower wall-clock.\n• On low-carbon grids the gap between C-FLAC and E-FLAC shrinks to ≈15 %, validating carbon-adaptivity.\n• Accuracy/perplexity matches baselines within statistical noise.\nAblations (ImageNet, Frankfurt):\n   – Disabling power-cap selection (+P fixed at 300 W) loses ~25 % of the carbon saving.\n   – Using a stale carbon signal (hour-old) degrades benefit by ~10 %, showing the need for live data.",
        "expected_conclusion": "Real-world carbon footprint depends jointly on how *much* we compute, *when* we compute, and at what *electrical efficiency* the hardware operates.  By extending forward-looking optimisation from energy to CO₂ and adding a controllable power-cap knob, C-FLAC achieves the first per-mini-batch policy that maximises *predicted loss reduction per gram of CO₂*.  Experiments across grids with contrasting carbon profiles demonstrate up to 65 % emission cuts without accuracy loss or additional training time, providing a concrete, immediately deployable path toward climate-aware deep learning."
      },
      "evaluation": {
        "novelty_reason": "The proposal extends the recent E-FLAC optimiser, which already maximises loss-decrease per joule, by (1) replacing the objective with loss-decrease per gram of CO₂ and (2) introducing live control of the GPU/TPU power-cap so that every step can trade electrical efficiency against throughput in response to time-varying grid carbon intensity. While prior work exists on each axis separately—e.g.\n • Energy-aware optimisers (E-FLAC, SharpSepConv)\n • Carbon-aware job schedulers that simply pause or migrate whole jobs (Google Carbon Aware Computing, GreenSlot)\n • Throughput-optimised power-capping approaches (ASAP-CAP, Choi et al. 2021)\nnone of them jointly decides the optimiser hyper-parameters (η, momentum, batch size) together with device power-cap conditioned on a real-time carbon signal, nor do they optimise a closed-form surrogate objective at every SGD step. The introduced quadratic surrogate, the analytic selection of η*, k*, and the discrete search over three power-caps within O(1) computation, constitute a new control algorithm that closes the loop between learning dynamics, hardware efficiency, and external carbon variability. This fine-grain, per-update carbon-aware control has not been reported in the literature, giving the hypothesis clear novelty.",
        "novelty_score": 8,
        "significance_reason": "Academically, the hypothesis bridges optimisation theory, systems for ML, and sustainability research by defining and maximising a new metric—expected loss reduction per gram CO₂—and showing how to solve it analytically and cheaply online. If validated, it would open a new research line on carbon-aware optimisers, inspiring follow-up work for other algorithms (Adam, L-BFGS) and hardware (ASICs, CPUs).\nSocietally, training large models already emits kiloton-scale CO₂. Achieving the claimed 30–65 % emission cut without sacrificing accuracy or wall-clock time offers an immediately deployable technique for any practitioner with access to a carbon-intensity API and nvidia-smi. Because the method requires only minor code changes and no extra hardware, its potential impact spans academia, industry, and cloud providers, directly contributing to climate-change mitigation efforts in AI.",
        "significance_score": 9
      }
    }
  ],
  "research_session": {
    "hypothesis": {
      "open_problems": "E-FLAC maximises the *loss decrease per joule* of every SGD-Momentum update, implicitly assuming (1) the electricity supplying the accelerator has constant carbon intensity and (2) the only controllable dimension of compute cost is how many FLOPs we execute (via batch size).  In reality, both the *carbon factor* of the grid (gCO₂ kWh⁻¹) and the *electrical efficiency* of modern GPUs/TPUs (joule per FLOP as a function of power-cap / clock) fluctuate by up to 3× within a single training run.  Consequently, two updates that consume the same energy can differ three-fold in CO₂-e emissions, and the optimiser may still waste carbon by executing compute-heavy steps during high-carbon hours or at an unnecessarily high power cap.  No existing optimiser closes the loop that simultaneously chooses 1) learning-rate η, 2) momentum m, 3) effective batch size B, and 4) device power-cap P so as to maximise *predicted loss decrease per gram of CO₂*.",
      "method": "Carbon-Aware Forward-Looking Adaptive Controller (C-FLAC)\n1. Retain E-FLAC’s quadratic surrogate to predict loss improvement I(η)=α_t η‖g_t‖²/2.\n2. Carbon-intensity feed-forward.  Query an external signal c_t  (gCO₂ kWh⁻¹)—either live via the free ElectricityMap / WattTime API or replayed from a trace—for the next Δτ≈2 min (≈one training step on ImageNet).\n3. Power–efficiency model.  Measure once the energy per micro-batch at three GPU power caps P∈{P_min, P_mid, P_max} using nvidia-smi –pl and fit a linear model  E(k,P)=k·(a+b·P) where k is the accumulation factor.  (Two extra scalars, estimated in <1 min warm-up.)\n4. Objective for the *next* update:\n   maximise R(η,k,P)= I(η) / ( E(k,P)·c_t ) over η∈[η_min,η_max], k∈{1,…,k_max}, P∈{P_min,P_mid,P_max}.\n   • As before, η* is analytic: η* = min(η_max , 2L_t /(α_t‖g_t‖²)).\n   • Optimal k* = clamp(⌈√(ρ_t η*‖g_t‖² /(2α_tL_t))⌉,1,k_max).\n   • Choose the power cap that minimises carbon per FLOP for the selected k*: P* = argmin_P  (E(k*,P)·c_t).\n5. Momentum from kinetic-energy rule m_t = clamp(1−κη*, m_min, m_max).\n6. Apply P* immediately via nvidia-smi –pl; implement k* with gradient accumulation; update parameters with (η*,m_t).\n7. Complexity: O(1) extra scalar ops; two new discrete choices (c_t fetched asynchronously, P selected from ≤3 options).  Safety fall-back: if the API is unreachable, revert to E-FLAC (energy-only).",
      "experimental_setup": "Benchmarks & regions\n• Vision: ImageNet-1k / ResNet-50 (training lasts 1.5 h at P_max)  – run once in Paris (low-carbon grid), once in Frankfurt (high-carbon grid) using ElectricityMap live feed.\n• Language: WikiText-103 / Transformer-Decoder (3 h) – region: California, live feed from WattTime.\nBaselines\n1. SGD-M + Cosine LR, fixed batch, default power cap.\n2. E-FLAC (η,m,B) – energy-aware only.\n3. ASAP-CAP (recent throughput-optimised power-capping) – adjusts P but not optimiser knobs.\nProposed: C-FLAC (η,m,B,P).\nProtocol\n• Same physical micro-batch (B_phys=256 ImageNet, 2048 tokens WT-103).\n• Each run repeats 3×; wall-clock times aligned across methods by synchronising starting hour.\nMetrics\n• Primary: kg CO₂ to reach target quality (74 % top-1, ppl 26).\n• Secondary: energy (kWh), wall-clock, final accuracy.\nImplementation: 70 extra lines on top of the E-FLAC PyTorch wrapper; power-cap change issued with subprocess(\"nvidia-smi -pl {P}\").  Carbon API queried every 30 s in a background thread and cached.",
      "primary_metric": "Total CO₂ emissions (kg CO₂-e) to hit the target validation score; energy and time as secondary.",
      "experimental_code": "# snippet for power-cap selection\nci = carbon_signal.fetch_now()          # gCO2 per kWh (async cache)\neta_star = min(eta_max, 2*loss /(alpha*g2+1e-12))\nk_star   = max(1, min(k_max, round(math.sqrt(rho*eta_star*g2 /(2*alpha*loss)))))\ncarbon_cost = {}\nfor P in [P_MIN, P_MID, P_MAX]:\n    E = k_star * (a + b*P)              # joule per batch * batches\n    carbon_cost[P] = E * ci\nP_star = min(carbon_cost, key=carbon_cost.get)\nsubprocess.run([\"nvidia-smi\",\"-pl\",str(P_star)], check=True)",
      "expected_result": "Compared to the strongest baseline in each region:\n• C-FLAC lowers CO₂ to target by 55–65 % over Cosine and 30–45 % over E-FLAC, with ≤1 % slower wall-clock.\n• On low-carbon grids the gap between C-FLAC and E-FLAC shrinks to ≈15 %, validating carbon-adaptivity.\n• Accuracy/perplexity matches baselines within statistical noise.\nAblations (ImageNet, Frankfurt):\n   – Disabling power-cap selection (+P fixed at 300 W) loses ~25 % of the carbon saving.\n   – Using a stale carbon signal (hour-old) degrades benefit by ~10 %, showing the need for live data.",
      "expected_conclusion": "Real-world carbon footprint depends jointly on how *much* we compute, *when* we compute, and at what *electrical efficiency* the hardware operates.  By extending forward-looking optimisation from energy to CO₂ and adding a controllable power-cap knob, C-FLAC achieves the first per-mini-batch policy that maximises *predicted loss reduction per gram of CO₂*.  Experiments across grids with contrasting carbon profiles demonstrate up to 65 % emission cuts without accuracy loss or additional training time, providing a concrete, immediately deployable path toward climate-aware deep learning."
    },
    "iterations": [
      {
        "iteration_id": 1,
        "method": "Carbon-Aware Forward-Looking Adaptive Controller (C-FLAC)\n1. Retain E-FLAC’s quadratic surrogate to predict loss improvement I(η)=α_t η‖g_t‖²/2.\n2. Carbon-intensity feed-forward.  Query an external signal c_t  (gCO₂ kWh⁻¹)—either live via the free ElectricityMap / WattTime API or replayed from a trace—for the next Δτ≈2 min (≈one training step on ImageNet).\n3. Power–efficiency model.  Measure once the energy per micro-batch at three GPU power caps P∈{P_min, P_mid, P_max} using nvidia-smi –pl and fit a linear model  E(k,P)=k·(a+b·P) where k is the accumulation factor.  (Two extra scalars, estimated in <1 min warm-up.)\n4. Objective for the *next* update:\n   maximise R(η,k,P)= I(η) / ( E(k,P)·c_t ) over η∈[η_min,η_max], k∈{1,…,k_max}, P∈{P_min,P_mid,P_max}.\n   • As before, η* is analytic: η* = min(η_max , 2L_t /(α_t‖g_t‖²)).\n   • Optimal k* = clamp(⌈√(ρ_t η*‖g_t‖² /(2α_tL_t))⌉,1,k_max).\n   • Choose the power cap that minimises carbon per FLOP for the selected k*: P* = argmin_P  (E(k*,P)·c_t).\n5. Momentum from kinetic-energy rule m_t = clamp(1−κη*, m_min, m_max).\n6. Apply P* immediately via nvidia-smi –pl; implement k* with gradient accumulation; update parameters with (η*,m_t).\n7. Complexity: O(1) extra scalar ops; two new discrete choices (c_t fetched asynchronously, P selected from ≤3 options).  Safety fall-back: if the API is unreachable, revert to E-FLAC (energy-only).",
        "experimental_design": {
          "experiment_summary": "Purpose: Demonstrate that Carbon-Aware Forward-Looking Adaptive Controller (C-FLAC) reduces the carbon footprint of training while preserving accuracy.\nScope: Single-node training of an ImageNet-1k classifier (ResNet-50, ≈25 M parameters) on an NVIDIA A100/H200 GPU with 80 GB VRAM.  We compare C-FLAC to the energy-only optimiser E-FLAC, measuring how much CO₂, energy and time are required to hit 74 % top-1 validation accuracy.\nWorkflow:\n1. Warm-up (≈1 min): run three micro-batches at power-caps P∈{200 W, 300 W, 400 W}; fit linear energy model E(k,P)=k·(a+bP).\n2. During training, every step executes C-FLAC:\n   a. Read live carbon-intensity c_t from ElectricityMap (Paris region trace supplied if offline).\n   b. From current loss surrogate determine η*, gradient-accumulation factor k* and power-cap P* that maximise expected loss decrease per gram CO₂.\n   c. Apply nvidia-smi -pl P*, accumulate k* micro-batches, update weights with η*, momentum m_t.\n3. Logging: per-step energy (nvml), power-cap, c_t, η, k, m, CO₂ estimate, loss, top-1.\n4. Stop when 74 % top-1 is reached; record totals.\n5. Repeat the whole run three times for statistical confidence.\nInfrastructure: PyTorch 2.3, CUDA 12, nvml-pynvml, requests; ≤70 extra LoC over the public E-FLAC implementation.  RAM (≥2 TB) easily holds the full ImageNet mem-mapped dataset and metrics buffer.\nExpected outcome: C-FLAC cuts CO₂ by ≈40 % vs. E-FLAC with ≤1 % extra wall-clock time and identical accuracy.",
          "evaluation_metrics": [
            "Total CO₂ emissions (kg CO₂-e) to reach 74 % top-1",
            "Total energy consumption (kWh)",
            "Wall-clock time (hours)",
            "Top-1 accuracy (%)",
            "Total CO₂ emissions (kg CO₂-e) to hit the target validation score; energy and time as secondary."
          ],
          "proposed_method": "Carbon-Aware Forward-Looking Adaptive Controller (C-FLAC)\nObjective: maximise predicted loss reduction per gram of CO₂ for the next SGD-Momentum update.\nTheoretical basis: extends E-FLAC’s quadratic loss-improvement surrogate by dividing through the product of (i) energy per update and (ii) live grid carbon intensity.\nComponents & algorithm:\n1. Surrogate loss improvement I(η)=α_t η‖g_t‖²⁄2 with online-estimated curvature α_t.\n2. Live carbon feed-forward: query ElectricityMap/WattTime every 30 s; cache c_t (gCO₂ kWh⁻¹).\n3. Device energy model: one-off calibration at three power caps; fit E(k,P)=k(a+bP).\n4. Solve argmax_{η,k,P} R(η,k,P)=I(η)/(E(k,P)c_t).\n   • η* = min(η_max, 2L_t/(α_t‖g_t‖²)).\n   • k* = clamp(⌈√(ρ_t η*‖g_t‖²/(2α_tL_t))⌉,1,k_max).\n   • P* = argmin_P E(k*,P)c_t among {P_min,P_mid,P_max}.\n5. Momentum: m_t = clamp(1−κη*, m_min, m_max).\n6. Apply P* (nvidia-smi -pl), accumulate k* micro-batches, update with (η*,m_t).\n7. Fallback: if carbon API fails, drop c_t term (revert to E-FLAC).  Overhead is O(1) operations and one subprocess call per step.",
          "comparative_methods": [
            "E-FLAC (energy-aware optimiser)"
          ],
          "models_to_use": [
            "ResNet-50 (25M)"
          ],
          "datasets_to_use": [
            "ImageNet-1k"
          ],
          "hyperparameters_to_search": {
            "η_max": "0.4-1.2",
            "η_min": "0.0001-0.01",
            "k_max": "1,2,4,8",
            "P_min/P_mid/P_max (W)": "200,300,400",
            "κ (momentum shaping)": "0.5-2.0",
            "ρ_t smoothing factor": "0.9-0.999"
          },
          "external_resources": {
            "hugging_face": {
              "models": [
                {
                  "id": "timm/resnet50.a1_in1k",
                  "author": "timm",
                  "sha": "767268603ca0cb0bfe326fa87277f19c419566ef",
                  "created_at": "2023-04-05T18:07:45+00:00",
                  "last_modified": "2025-07-11T18:17:03+00:00",
                  "private": false,
                  "gated": false,
                  "disabled": false,
                  "downloads": 3180593,
                  "likes": 39,
                  "siblings": [
                    {
                      "rfilename": ".gitattributes"
                    },
                    {
                      "rfilename": "README.md"
                    },
                    {
                      "rfilename": "config.json"
                    },
                    {
                      "rfilename": "model.safetensors"
                    },
                    {
                      "rfilename": "pytorch_model.bin"
                    }
                  ],
                  "card_data": {
                    "license": "apache-2.0",
                    "language": [],
                    "library_name": "timm",
                    "tags": [
                      "image-classification",
                      "timm",
                      "transformers"
                    ],
                    "datasets": [],
                    "task_categories": [],
                    "size_categories": [],
                    "metrics": [],
                    "widget": []
                  },
                  "tags": [
                    "timm",
                    "pytorch",
                    "safetensors",
                    "image-classification",
                    "transformers",
                    "arxiv:2110.00476",
                    "arxiv:1512.03385",
                    "license:apache-2.0",
                    "region:us"
                  ],
                  "pipeline_tag": "image-classification",
                  "library_name": "timm",
                  "readme": "---\nlicense: apache-2.0\nlibrary_name: timm\ntags:\n- image-classification\n- timm\n- transformers\n---\n# Model card for resnet50.a1_in1k\n\nA ResNet-B image classification model.\n\nThis model features:\n * ReLU activations\n * single layer 7x7 convolution with pooling\n * 1x1 convolution shortcut downsample\n\nTrained on ImageNet-1k in `timm` using recipe template described below.\n\nRecipe details:\n * ResNet Strikes Back `A1` recipe\n * LAMB optimizer with BCE loss\n * Cosine LR schedule with warmup\n\n\n## Model Details\n- **Model Type:** Image classification / feature backbone\n- **Model Stats:**\n  - Params (M): 25.6\n  - GMACs: 4.1\n  - Activations (M): 11.1\n  - Image size: train = 224 x 224, test = 288 x 288\n- **Papers:**\n  - ResNet strikes back: An improved training procedure in timm: https://arxiv.org/abs/2110.00476\n  - Deep Residual Learning for Image Recognition: https://arxiv.org/abs/1512.03385\n- **Original:** https://github.com/huggingface/pytorch-image-models\n\n## Model Usage\n### Image Classification\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\nimport torch\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model('resnet50.a1_in1k', pretrained=True)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\n```\n\n### Feature Map Extraction\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnet50.a1_in1k',\n    pretrained=True,\n    features_only=True,\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\nfor o in output:\n    # print shape of each feature map in output\n    # e.g.:\n    #  torch.Size([1, 64, 112, 112])\n    #  torch.Size([1, 256, 56, 56])\n    #  torch.Size([1, 512, 28, 28])\n    #  torch.Size([1, 1024, 14, 14])\n    #  torch.Size([1, 2048, 7, 7])\n\n    print(o.shape)\n```\n\n### Image Embeddings\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnet50.a1_in1k',\n    pretrained=True,\n    num_classes=0,  # remove classifier nn.Linear\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n\n# or equivalently (without needing to set num_classes=0)\n\noutput = model.forward_features(transforms(img).unsqueeze(0))\n# output is unpooled, a (1, 2048, 7, 7) shaped tensor\n\noutput = model.forward_head(output, pre_logits=True)\n# output is a (1, num_features) shaped tensor\n```\n\n## Model Comparison\nExplore the dataset and runtime metrics of this model in timm [model results](https://github.com/huggingface/pytorch-image-models/tree/main/results).\n\n|model                                     |img_size|top1 |top5 |param_count|gmacs|macts|img/sec|\n|------------------------------------------|--------|-----|-----|-----------|-----|-----|-------|\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k_288](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k_288)|320     |86.72|98.17|93.6       |35.2 |69.7 |451    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k_288](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k_288)|288     |86.51|98.08|93.6       |28.5 |56.4 |560    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k)|288     |86.49|98.03|93.6       |28.5 |56.4 |557    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k)|224     |85.96|97.82|93.6       |17.2 |34.2 |923    |\n|[resnext101_32x32d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x32d.fb_wsl_ig1b_ft_in1k)|224     |85.11|97.44|468.5      |87.3 |91.1 |254    |\n|[resnetrs420.tf_in1k](https://huggingface.co/timm/resnetrs420.tf_in1k)|416     |85.0 |97.12|191.9      |108.4|213.8|134    |\n|[ecaresnet269d.ra2_in1k](https://huggingface.co/timm/ecaresnet269d.ra2_in1k)|352     |84.96|97.22|102.1      |50.2 |101.2|291    |\n|[ecaresnet269d.ra2_in1k](https://huggingface.co/timm/ecaresnet269d.ra2_in1k)|320     |84.73|97.18|102.1      |41.5 |83.7 |353    |\n|[resnetrs350.tf_in1k](https://huggingface.co/timm/resnetrs350.tf_in1k)|384     |84.71|96.99|164.0      |77.6 |154.7|183    |\n|[seresnextaa101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.ah_in1k)|288     |84.57|97.08|93.6       |28.5 |56.4 |557    |\n|[resnetrs200.tf_in1k](https://huggingface.co/timm/resnetrs200.tf_in1k)|320     |84.45|97.08|93.2       |31.5 |67.8 |446    |\n|[resnetrs270.tf_in1k](https://huggingface.co/timm/resnetrs270.tf_in1k)|352     |84.43|96.97|129.9      |51.1 |105.5|280    |\n|[seresnext101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101d_32x8d.ah_in1k)|288     |84.36|96.92|93.6       |27.6 |53.0 |595    |\n|[seresnet152d.ra2_in1k](https://huggingface.co/timm/seresnet152d.ra2_in1k)|320     |84.35|97.04|66.8       |24.1 |47.7 |610    |\n|[resnetrs350.tf_in1k](https://huggingface.co/timm/resnetrs350.tf_in1k)|288     |84.3 |96.94|164.0      |43.7 |87.1 |333    |\n|[resnext101_32x8d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_swsl_ig1b_ft_in1k)|224     |84.28|97.17|88.8       |16.5 |31.2 |1100   |\n|[resnetrs420.tf_in1k](https://huggingface.co/timm/resnetrs420.tf_in1k)|320     |84.24|96.86|191.9      |64.2 |126.6|228    |\n|[seresnext101_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101_32x8d.ah_in1k)|288     |84.19|96.87|93.6       |27.2 |51.6 |613    |\n|[resnext101_32x16d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_wsl_ig1b_ft_in1k)|224     |84.18|97.19|194.0      |36.3 |51.2 |581    |\n|[resnetaa101d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa101d.sw_in12k_ft_in1k)|288     |84.11|97.11|44.6       |15.1 |29.0 |1144   |\n|[resnet200d.ra2_in1k](https://huggingface.co/timm/resnet200d.ra2_in1k)|320     |83.97|96.82|64.7       |31.2 |67.3 |518    |\n|[resnetrs200.tf_in1k](https://huggingface.co/timm/resnetrs200.tf_in1k)|256     |83.87|96.75|93.2       |20.2 |43.4 |692    |\n|[seresnextaa101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.ah_in1k)|224     |83.86|96.65|93.6       |17.2 |34.2 |923    |\n|[resnetrs152.tf_in1k](https://huggingface.co/timm/resnetrs152.tf_in1k)|320     |83.72|96.61|86.6       |24.3 |48.1 |617    |\n|[seresnet152d.ra2_in1k](https://huggingface.co/timm/seresnet152d.ra2_in1k)|256     |83.69|96.78|66.8       |15.4 |30.6 |943    |\n|[seresnext101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101d_32x8d.ah_in1k)|224     |83.68|96.61|93.6       |16.7 |32.0 |986    |\n|[resnet152d.ra2_in1k](https://huggingface.co/timm/resnet152d.ra2_in1k)|320     |83.67|96.74|60.2       |24.1 |47.7 |706    |\n|[resnetrs270.tf_in1k](https://huggingface.co/timm/resnetrs270.tf_in1k)|256     |83.59|96.61|129.9      |27.1 |55.8 |526    |\n|[seresnext101_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101_32x8d.ah_in1k)|224     |83.58|96.4 |93.6       |16.5 |31.2 |1013   |\n|[resnetaa101d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa101d.sw_in12k_ft_in1k)|224     |83.54|96.83|44.6       |9.1  |17.6 |1864   |\n|[resnet152.a1h_in1k](https://huggingface.co/timm/resnet152.a1h_in1k)|288     |83.46|96.54|60.2       |19.1 |37.3 |904    |\n|[resnext101_32x16d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_swsl_ig1b_ft_in1k)|224     |83.35|96.85|194.0      |36.3 |51.2 |582    |\n|[resnet200d.ra2_in1k](https://huggingface.co/timm/resnet200d.ra2_in1k)|256     |83.23|96.53|64.7       |20.0 |43.1 |809    |\n|[resnext101_32x4d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x4d.fb_swsl_ig1b_ft_in1k)|224     |83.22|96.75|44.2       |8.0  |21.2 |1814   |\n|[resnext101_64x4d.c1_in1k](https://huggingface.co/timm/resnext101_64x4d.c1_in1k)|288     |83.16|96.38|83.5       |25.7 |51.6 |590    |\n|[resnet152d.ra2_in1k](https://huggingface.co/timm/resnet152d.ra2_in1k)|256     |83.14|96.38|60.2       |15.4 |30.5 |1096   |\n|[resnet101d.ra2_in1k](https://huggingface.co/timm/resnet101d.ra2_in1k)|320     |83.02|96.45|44.6       |16.5 |34.8 |992    |\n|[ecaresnet101d.miil_in1k](https://huggingface.co/timm/ecaresnet101d.miil_in1k)|288     |82.98|96.54|44.6       |13.4 |28.2 |1077   |\n|[resnext101_64x4d.tv_in1k](https://huggingface.co/timm/resnext101_64x4d.tv_in1k)|224     |82.98|96.25|83.5       |15.5 |31.2 |989    |\n|[resnetrs152.tf_in1k](https://huggingface.co/timm/resnetrs152.tf_in1k)|256     |82.86|96.28|86.6       |15.6 |30.8 |951    |\n|[resnext101_32x8d.tv2_in1k](https://huggingface.co/timm/resnext101_32x8d.tv2_in1k)|224     |82.83|96.22|88.8       |16.5 |31.2 |1099   |\n|[resnet152.a1h_in1k](https://huggingface.co/timm/resnet152.a1h_in1k)|224     |82.8 |96.13|60.2       |11.6 |22.6 |1486   |\n|[resnet101.a1h_in1k](https://huggingface.co/timm/resnet101.a1h_in1k)|288     |82.8 |96.32|44.6       |13.0 |26.8 |1291   |\n|[resnet152.a1_in1k](https://huggingface.co/timm/resnet152.a1_in1k)|288     |82.74|95.71|60.2       |19.1 |37.3 |905    |\n|[resnext101_32x8d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_wsl_ig1b_ft_in1k)|224     |82.69|96.63|88.8       |16.5 |31.2 |1100   |\n|[resnet152.a2_in1k](https://huggingface.co/timm/resnet152.a2_in1k)|288     |82.62|95.75|60.2       |19.1 |37.3 |904    |\n|[resnetaa50d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa50d.sw_in12k_ft_in1k)|288     |82.61|96.49|25.6       |8.9  |20.6 |1729   |\n|[resnet61q.ra2_in1k](https://huggingface.co/timm/resnet61q.ra2_in1k)|288     |82.53|96.13|36.8       |9.9  |21.5 |1773   |\n|[wide_resnet101_2.tv2_in1k](https://huggingface.co/timm/wide_resnet101_2.tv2_in1k)|224     |82.5 |96.02|126.9      |22.8 |21.2 |1078   |\n|[resnext101_64x4d.c1_in1k](https://huggingface.co/timm/resnext101_64x4d.c1_in1k)|224     |82.46|95.92|83.5       |15.5 |31.2 |987    |\n|[resnet51q.ra2_in1k](https://huggingface.co/timm/resnet51q.ra2_in1k)|288     |82.36|96.18|35.7       |8.1  |20.9 |1964   |\n|[ecaresnet50t.ra2_in1k](https://huggingface.co/timm/ecaresnet50t.ra2_in1k)|320     |82.35|96.14|25.6       |8.8  |24.1 |1386   |\n|[resnet101.a1_in1k](https://huggingface.co/timm/resnet101.a1_in1k)|288     |82.31|95.63|44.6       |13.0 |26.8 |1291   |\n|[resnetrs101.tf_in1k](https://huggingface.co/timm/resnetrs101.tf_in1k)|288     |82.29|96.01|63.6       |13.6 |28.5 |1078   |\n|[resnet152.tv2_in1k](https://huggingface.co/timm/resnet152.tv2_in1k)|224     |82.29|96.0 |60.2       |11.6 |22.6 |1484   |\n|[wide_resnet50_2.racm_in1k](https://huggingface.co/timm/wide_resnet50_2.racm_in1k)|288     |82.27|96.06|68.9       |18.9 |23.8 |1176   |\n|[resnet101d.ra2_in1k](https://huggingface.co/timm/resnet101d.ra2_in1k)|256     |82.26|96.07|44.6       |10.6 |22.2 |1542   |\n|[resnet101.a2_in1k](https://huggingface.co/timm/resnet101.a2_in1k)|288     |82.24|95.73|44.6       |13.0 |26.8 |1290   |\n|[seresnext50_32x4d.racm_in1k](https://huggingface.co/timm/seresnext50_32x4d.racm_in1k)|288     |82.2 |96.14|27.6       |7.0  |23.8 |1547   |\n|[ecaresnet101d.miil_in1k](https://huggingface.co/timm/ecaresnet101d.miil_in1k)|224     |82.18|96.05|44.6       |8.1  |17.1 |1771   |\n|[resnext50_32x4d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext50_32x4d.fb_swsl_ig1b_ft_in1k)|224     |82.17|96.22|25.0       |4.3  |14.4 |2943   |\n|[ecaresnet50t.a1_in1k](https://huggingface.co/timm/ecaresnet50t.a1_in1k)|288     |82.12|95.65|25.6       |7.1  |19.6 |1704   |\n|[resnext50_32x4d.a1h_in1k](https://huggingface.co/timm/resnext50_32x4d.a1h_in1k)|288     |82.03|95.94|25.0       |7.0  |23.8 |1745   |\n|[ecaresnet101d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet101d_pruned.miil_in1k)|288     |82.0 |96.15|24.9       |5.8  |12.7 |1787   |\n|[resnet61q.ra2_in1k](https://huggingface.co/timm/resnet61q.ra2_in1k)|256     |81.99|95.85|36.8       |7.8  |17.0 |2230   |\n|[resnext101_32x8d.tv2_in1k](https://huggingface.co/timm/resnext101_32x8d.tv2_in1k)|176     |81.98|95.72|88.8       |10.3 |19.4 |1768   |\n|[resnet152.a1_in1k](https://huggingface.co/timm/resnet152.a1_in1k)|224     |81.97|95.24|60.2       |11.6 |22.6 |1486   |\n|[resnet101.a1h_in1k](https://huggingface.co/timm/resnet101.a1h_in1k)|224     |81.93|95.75|44.6       |7.8  |16.2 |2122   |\n|[resnet101.tv2_in1k](https://huggingface.co/timm/resnet101.tv2_in1k)|224     |81.9 |95.77|44.6       |7.8  |16.2 |2118   |\n|[resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k)|224     |81.84|96.1 |194.0      |36.3 |51.2 |583    |\n|[resnet51q.ra2_in1k](https://huggingface.co/timm/resnet51q.ra2_in1k)|256     |81.78|95.94|35.7       |6.4  |16.6 |2471   |\n|[resnet152.a2_in1k](https://huggingface.co/timm/resnet152.a2_in1k)|224     |81.77|95.22|60.2       |11.6 |22.6 |1485   |\n|[resnetaa50d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa50d.sw_in12k_ft_in1k)|224     |81.74|96.06|25.6       |5.4  |12.4 |2813   |\n|[ecaresnet50t.a2_in1k](https://huggingface.co/timm/ecaresnet50t.a2_in1k)|288     |81.65|95.54|25.6       |7.1  |19.6 |1703   |\n|[ecaresnet50d.miil_in1k](https://huggingface.co/timm/ecaresnet50d.miil_in1k)|288     |81.64|95.88|25.6       |7.2  |19.7 |1694   |\n|[resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k)|224     |81.62|96.04|88.8       |16.5 |31.2 |1101   |\n|[wide_resnet50_2.tv2_in1k](https://huggingface.co/timm/wide_resnet50_2.tv2_in1k)|224     |81.61|95.76|68.9       |11.4 |14.4 |1930   |\n|[resnetaa50.a1h_in1k](https://huggingface.co/timm/resnetaa50.a1h_in1k)|288     |81.61|95.83|25.6       |8.5  |19.2 |1868   |\n|[resnet101.a1_in1k](https://huggingface.co/timm/resnet101.a1_in1k)|224     |81.5 |95.16|44.6       |7.8  |16.2 |2125   |\n|[resnext50_32x4d.a1_in1k](https://huggingface.co/timm/resnext50_32x4d.a1_in1k)|288     |81.48|95.16|25.0       |7.0  |23.8 |1745   |\n|[gcresnet50t.ra2_in1k](https://huggingface.co/timm/gcresnet50t.ra2_in1k)|288     |81.47|95.71|25.9       |6.9  |18.6 |2071   |\n|[wide_resnet50_2.racm_in1k](https://huggingface.co/timm/wide_resnet50_2.racm_in1k)|224     |81.45|95.53|68.9       |11.4 |14.4 |1929   |\n|[resnet50d.a1_in1k](https://huggingface.co/timm/resnet50d.a1_in1k)|288     |81.44|95.22|25.6       |7.2  |19.7 |1908   |\n|[ecaresnet50t.ra2_in1k](https://huggingface.co/timm/ecaresnet50t.ra2_in1k)|256     |81.44|95.67|25.6       |5.6  |15.4 |2168   |\n|[ecaresnetlight.miil_in1k](https://huggingface.co/timm/ecaresnetlight.miil_in1k)|288     |81.4 |95.82|30.2       |6.8  |13.9 |2132   |\n|[resnet50d.ra2_in1k](https://huggingface.co/timm/resnet50d.ra2_in1k)|288     |81.37|95.74|25.6       |7.2  |19.7 |1910   |\n|[resnet101.a2_in1k](https://huggingface.co/timm/resnet101.a2_in1k)|224     |81.32|95.19|44.6       |7.8  |16.2 |2125   |\n|[seresnet50.ra2_in1k](https://huggingface.co/timm/seresnet50.ra2_in1k)|288     |81.3 |95.65|28.1       |6.8  |18.4 |1803   |\n|[resnext50_32x4d.a2_in1k](https://huggingface.co/timm/resnext50_32x4d.a2_in1k)|288     |81.3 |95.11|25.0       |7.0  |23.8 |1746   |\n|[seresnext50_32x4d.racm_in1k](https://huggingface.co/timm/seresnext50_32x4d.racm_in1k)|224     |81.27|95.62|27.6       |4.3  |14.4 |2591   |\n|[ecaresnet50t.a1_in1k](https://huggingface.co/timm/ecaresnet50t.a1_in1k)|224     |81.26|95.16|25.6       |4.3  |11.8 |2823   |\n|[gcresnext50ts.ch_in1k](https://huggingface.co/timm/gcresnext50ts.ch_in1k)|288     |81.23|95.54|15.7       |4.8  |19.6 |2117   |\n|[senet154.gluon_in1k](https://huggingface.co/timm/senet154.gluon_in1k)|224     |81.23|95.35|115.1      |20.8 |38.7 |545    |\n|[resnet50.a1_in1k](https://huggingface.co/timm/resnet50.a1_in1k)|288     |81.22|95.11|25.6       |6.8  |18.4 |2089   |\n|[resnet50_gn.a1h_in1k](https://huggingface.co/timm/resnet50_gn.a1h_in1k)|288     |81.22|95.63|25.6       |6.8  |18.4 |676    |\n|[resnet50d.a2_in1k](https://huggingface.co/timm/resnet50d.a2_in1k)|288     |81.18|95.09|25.6       |7.2  |19.7 |1908   |\n|[resnet50.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnet50.fb_swsl_ig1b_ft_in1k)|224     |81.18|95.98|25.6       |4.1  |11.1 |3455   |\n|[resnext50_32x4d.tv2_in1k](https://huggingface.co/timm/resnext50_32x4d.tv2_in1k)|224     |81.17|95.34|25.0       |4.3  |14.4 |2933   |\n|[resnext50_32x4d.a1h_in1k](https://huggingface.co/timm/resnext50_32x4d.a1h_in1k)|224     |81.1 |95.33|25.0       |4.3  |14.4 |2934   |\n|[seresnet50.a2_in1k](https://huggingface.co/timm/seresnet50.a2_in1k)|288     |81.1 |95.23|28.1       |6.8  |18.4 |1801   |\n|[seresnet50.a1_in1k](https://huggingface.co/timm/seresnet50.a1_in1k)|288     |81.1 |95.12|28.1       |6.8  |18.4 |1799   |\n|[resnet152s.gluon_in1k](https://huggingface.co/timm/resnet152s.gluon_in1k)|224     |81.02|95.41|60.3       |12.9 |25.0 |1347   |\n|[resnet50.d_in1k](https://huggingface.co/timm/resnet50.d_in1k)|288     |80.97|95.44|25.6       |6.8  |18.4 |2085   |\n|[gcresnet50t.ra2_in1k](https://huggingface.co/timm/gcresnet50t.ra2_in1k)|256     |80.94|95.45|25.9       |5.4  |14.7 |2571   |\n|[resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k)|224     |80.93|95.73|44.2       |8.0  |21.2 |1814   |\n|[resnet50.c1_in1k](https://huggingface.co/timm/resnet50.c1_in1k)|288     |80.91|95.55|25.6       |6.8  |18.4 |2084   |\n|[seresnext101_32x4d.gluon_in1k](https://huggingface.co/timm/seresnext101_32x4d.gluon_in1k)|224     |80.9 |95.31|49.0       |8.0  |21.3 |1585   |\n|[seresnext101_64x4d.gluon_in1k](https://huggingface.co/timm/seresnext101_64x4d.gluon_in1k)|224     |80.9 |95.3 |88.2       |15.5 |31.2 |918    |\n|[resnet50.c2_in1k](https://huggingface.co/timm/resnet50.c2_in1k)|288     |80.86|95.52|25.6       |6.8  |18.4 |2085   |\n|[resnet50.tv2_in1k](https://huggingface.co/timm/resnet50.tv2_in1k)|224     |80.85|95.43|25.6       |4.1  |11.1 |3450   |\n|[ecaresnet50t.a2_in1k](https://huggingface.co/timm/ecaresnet50t.a2_in1k)|224     |80.84|95.02|25.6       |4.3  |11.8 |2821   |\n|[ecaresnet101d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet101d_pruned.miil_in1k)|224     |80.79|95.62|24.9       |3.5  |7.7  |2961   |\n|[seresnet33ts.ra2_in1k](https://huggingface.co/timm/seresnet33ts.ra2_in1k)|288     |80.79|95.36|19.8       |6.0  |14.8 |2506   |\n|[ecaresnet50d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet50d_pruned.miil_in1k)|288     |80.79|95.58|19.9       |4.2  |10.6 |2349   |\n|[resnet50.a2_in1k](https://huggingface.co/timm/resnet50.a2_in1k)|288     |80.78|94.99|25.6       |6.8  |18.4 |2088   |\n|[resnet50.b1k_in1k](https://huggingface.co/timm/resnet50.b1k_in1k)|288     |80.71|95.43|25.6       |6.8  |18.4 |2087   |\n|[resnext50_32x4d.ra_in1k](https://huggingface.co/timm/resnext50_32x4d.ra_in1k)|288     |80.7 |95.39|25.0       |7.0  |23.8 |1749   |\n|[resnetrs101.tf_in1k](https://huggingface.co/timm/resnetrs101.tf_in1k)|192     |80.69|95.24|63.6       |6.0  |12.7 |2270   |\n|[resnet50d.a1_in1k](https://huggingface.co/timm/resnet50d.a1_in1k)|224     |80.68|94.71|25.6       |4.4  |11.9 |3162   |\n|[eca_resnet33ts.ra2_in1k](https://huggingface.co/timm/eca_resnet33ts.ra2_in1k)|288     |80.68|95.36|19.7       |6.0  |14.8 |2637   |\n|[resnet50.a1h_in1k](https://huggingface.co/timm/resnet50.a1h_in1k)|224     |80.67|95.3 |25.6       |4.1  |11.1 |3452   |\n|[resnext50d_32x4d.bt_in1k](https://huggingface.co/timm/resnext50d_32x4d.bt_in1k)|288     |80.67|95.42|25.0       |7.4  |25.1 |1626   |\n|[resnetaa50.a1h_in1k](https://huggingface.co/timm/resnetaa50.a1h_in1k)|224     |80.63|95.21|25.6       |5.2  |11.6 |3034   |\n|[ecaresnet50d.miil_in1k](https://huggingface.co/timm/ecaresnet50d.miil_in1k)|224     |80.61|95.32|25.6       |4.4  |11.9 |2813   |\n|[resnext101_64x4d.gluon_in1k](https://huggingface.co/timm/resnext101_64x4d.gluon_in1k)|224     |80.61|94.99|83.5       |15.5 |31.2 |989    |\n|[gcresnet33ts.ra2_in1k](https://huggingface.co/timm/gcresnet33ts.ra2_in1k)|288     |80.6 |95.31|19.9       |6.0  |14.8 |2578   |\n|[gcresnext50ts.ch_in1k](https://huggingface.co/timm/gcresnext50ts.ch_in1k)|256     |80.57|95.17|15.7       |3.8  |15.5 |2710   |\n|[resnet152.a3_in1k](https://huggingface.co/timm/resnet152.a3_in1k)|224     |80.56|95.0 |60.2       |11.6 |22.6 |1483   |\n|[resnet50d.ra2_in1k](https://huggingface.co/timm/resnet50d.ra2_in1k)|224     |80.53|95.16|25.6       |4.4  |11.9 |3164   |\n|[resnext50_32x4d.a1_in1k](https://huggingface.co/timm/resnext50_32x4d.a1_in1k)|224     |80.53|94.46|25.0       |4.3  |14.4 |2930   |\n|[wide_resnet101_2.tv2_in1k](https://huggingface.co/timm/wide_resnet101_2.tv2_in1k)|176     |80.48|94.98|126.9      |14.3 |13.2 |1719   |\n|[resnet152d.gluon_in1k](https://huggingface.co/timm/resnet152d.gluon_in1k)|224     |80.47|95.2 |60.2       |11.8 |23.4 |1428   |\n|[resnet50.b2k_in1k](https://huggingface.co/timm/resnet50.b2k_in1k)|288     |80.45|95.32|25.6       |6.8  |18.4 |2086   |\n|[ecaresnetlight.miil_in1k](https://huggingface.co/timm/ecaresnetlight.miil_in1k)|224     |80.45|95.24|30.2       |4.1  |8.4  |3530   |\n|[resnext50_32x4d.a2_in1k](https://huggingface.co/timm/resnext50_32x4d.a2_in1k)|224     |80.45|94.63|25.0       |4.3  |14.4 |2936   |\n|[wide_resnet50_2.tv2_in1k](https://huggingface.co/timm/wide_resnet50_2.tv2_in1k)|176     |80.43|95.09|68.9       |7.3  |9.0  |3015   |\n|[resnet101d.gluon_in1k](https://huggingface.co/timm/resnet101d.gluon_in1k)|224     |80.42|95.01|44.6       |8.1  |17.0 |2007   |\n|[resnet50.a1_in1k](https://huggingface.co/timm/resnet50.a1_in1k)|224     |80.38|94.6 |25.6       |4.1  |11.1 |3461   |\n|[seresnet33ts.ra2_in1k](https://huggingface.co/timm/seresnet33ts.ra2_in1k)|256     |80.36|95.1 |19.8       |4.8  |11.7 |3267   |\n|[resnext101_32x4d.gluon_in1k](https://huggingface.co/timm/resnext101_32x4d.gluon_in1k)|224     |80.34|94.93|44.2       |8.0  |21.2 |1814   |\n|[resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k)|224     |80.32|95.4 |25.0       |4.3  |14.4 |2941   |\n|[resnet101s.gluon_in1k](https://huggingface.co/timm/resnet101s.gluon_in1k)|224     |80.28|95.16|44.7       |9.2  |18.6 |1851   |\n|[seresnet50.ra2_in1k](https://huggingface.co/timm/seresnet50.ra2_in1k)|224     |80.26|95.08|28.1       |4.1  |11.1 |2972   |\n|[resnetblur50.bt_in1k](https://huggingface.co/timm/resnetblur50.bt_in1k)|288     |80.24|95.24|25.6       |8.5  |19.9 |1523   |\n|[resnet50d.a2_in1k](https://huggingface.co/timm/resnet50d.a2_in1k)|224     |80.22|94.63|25.6       |4.4  |11.9 |3162   |\n|[resnet152.tv2_in1k](https://huggingface.co/timm/resnet152.tv2_in1k)|176     |80.2 |94.64|60.2       |7.2  |14.0 |2346   |\n|[seresnet50.a2_in1k](https://huggingface.co/timm/seresnet50.a2_in1k)|224     |80.08|94.74|28.1       |4.1  |11.1 |2969   |\n|[eca_resnet33ts.ra2_in1k](https://huggingface.co/timm/eca_resnet33ts.ra2_in1k)|256     |80.08|94.97|19.7       |4.8  |11.7 |3284   |\n|[gcresnet33ts.ra2_in1k](https://huggingface.co/timm/gcresnet33ts.ra2_in1k)|256     |80.06|94.99|19.9       |4.8  |11.7 |3216   |\n|[resnet50_gn.a1h_in1k](https://huggingface.co/timm/resnet50_gn.a1h_in1k)|224     |80.06|94.95|25.6       |4.1  |11.1 |1109   |\n|[seresnet50.a1_in1k](https://huggingface.co/timm/seresnet50.a1_in1k)|224     |80.02|94.71|28.1       |4.1  |11.1 |2962   |\n|[resnet50.ram_in1k](https://huggingface.co/timm/resnet50.ram_in1k)|288     |79.97|95.05|25.6       |6.8  |18.4 |2086   |\n|[resnet152c.gluon_in1k](https://huggingface.co/timm/resnet152c.gluon_in1k)|224     |79.92|94.84|60.2       |11.8 |23.4 |1455   |\n|[seresnext50_32x4d.gluon_in1k](https://huggingface.co/timm/seresnext50_32x4d.gluon_in1k)|224     |79.91|94.82|27.6       |4.3  |14.4 |2591   |\n|[resnet50.d_in1k](https://huggingface.co/timm/resnet50.d_in1k)|224     |79.91|94.67|25.6       |4.1  |11.1 |3456   |\n|[resnet101.tv2_in1k](https://huggingface.co/timm/resnet101.tv2_in1k)|176     |79.9 |94.6 |44.6       |4.9  |10.1 |3341   |\n|[resnetrs50.tf_in1k](https://huggingface.co/timm/resnetrs50.tf_in1k)|224     |79.89|94.97|35.7       |4.5  |12.1 |2774   |\n|[resnet50.c2_in1k](https://huggingface.co/timm/resnet50.c2_in1k)|224     |79.88|94.87|25.6       |4.1  |11.1 |3455   |\n|[ecaresnet26t.ra2_in1k](https://huggingface.co/timm/ecaresnet26t.ra2_in1k)|320     |79.86|95.07|16.0       |5.2  |16.4 |2168   |\n|[resnet50.a2_in1k](https://huggingface.co/timm/resnet50.a2_in1k)|224     |79.85|94.56|25.6       |4.1  |11.1 |3460   |\n|[resnet50.ra_in1k](https://huggingface.co/timm/resnet50.ra_in1k)|288     |79.83|94.97|25.6       |6.8  |18.4 |2087   |\n|[resnet101.a3_in1k](https://huggingface.co/timm/resnet101.a3_in1k)|224     |79.82|94.62|44.6       |7.8  |16.2 |2114   |\n|[resnext50_32x4d.ra_in1k](https://huggingface.co/timm/resnext50_32x4d.ra_in1k)|224     |79.76|94.6 |25.0       |4.3  |14.4 |2943   |\n|[resnet50.c1_in1k](https://huggingface.co/timm/resnet50.c1_in1k)|224     |79.74|94.95|25.6       |4.1  |11.1 |3455   |\n|[ecaresnet50d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet50d_pruned.miil_in1k)|224     |79.74|94.87|19.9       |2.5  |6.4  |3929   |\n|[resnet33ts.ra2_in1k](https://huggingface.co/timm/resnet33ts.ra2_in1k)|288     |79.71|94.83|19.7       |6.0  |14.8 |2710   |\n|[resnet152.gluon_in1k](https://huggingface.co/timm/resnet152.gluon_in1k)|224     |79.68|94.74|60.2       |11.6 |22.6 |1486   |\n|[resnext50d_32x4d.bt_in1k](https://huggingface.co/timm/resnext50d_32x4d.bt_in1k)|224     |79.67|94.87|25.0       |4.5  |15.2 |2729   |\n|[resnet50.bt_in1k](https://huggingface.co/timm/resnet50.bt_in1k)|288     |79.63|94.91|25.6       |6.8  |18.4 |2086   |\n|[ecaresnet50t.a3_in1k](https://huggingface.co/timm/ecaresnet50t.a3_in1k)|224     |79.56|94.72|25.6       |4.3  |11.8 |2805   |\n|[resnet101c.gluon_in1k](https://huggingface.co/timm/resnet101c.gluon_in1k)|224     |79.53|94.58|44.6       |8.1  |17.0 |2062   |\n|[resnet50.b1k_in1k](https://huggingface.co/timm/resnet50.b1k_in1k)|224     |79.52|94.61|25.6       |4.1  |11.1 |3459   |\n|[resnet50.tv2_in1k](https://huggingface.co/timm/resnet50.tv2_in1k)|176     |79.42|94.64|25.6       |2.6  |6.9  |5397   |\n|[resnet32ts.ra2_in1k](https://huggingface.co/timm/resnet32ts.ra2_in1k)|288     |79.4 |94.66|18.0       |5.9  |14.6 |2752   |\n|[resnet50.b2k_in1k](https://huggingface.co/timm/resnet50.b2k_in1k)|224     |79.38|94.57|25.6       |4.1  |11.1 |3459   |\n|[resnext50_32x4d.tv2_in1k](https://huggingface.co/timm/resnext50_32x4d.tv2_in1k)|176     |79.37|94.3 |25.0       |2.7  |9.0  |4577   |\n|[resnext50_32x4d.gluon_in1k](https://huggingface.co/timm/resnext50_32x4d.gluon_in1k)|224     |79.36|94.43|25.0       |4.3  |14.4 |2942   |\n|[resnext101_32x8d.tv_in1k](https://huggingface.co/timm/resnext101_32x8d.tv_in1k)|224     |79.31|94.52|88.8       |16.5 |31.2 |1100   |\n|[resnet101.gluon_in1k](https://huggingface.co/timm/resnet101.gluon_in1k)|224     |79.31|94.53|44.6       |7.8  |16.2 |2125   |\n|[resnetblur50.bt_in1k](https://huggingface.co/timm/resnetblur50.bt_in1k)|224     |79.31|94.63|25.6       |5.2  |12.0 |2524   |\n|[resnet50.a1h_in1k](https://huggingface.co/timm/resnet50.a1h_in1k)|176     |79.27|94.49|25.6       |2.6  |6.9  |5404   |\n|[resnext50_32x4d.a3_in1k](https://huggingface.co/timm/resnext50_32x4d.a3_in1k)|224     |79.25|94.31|25.0       |4.3  |14.4 |2931   |\n|[resnet50.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnet50.fb_ssl_yfcc100m_ft_in1k)|224     |79.22|94.84|25.6       |4.1  |11.1 |3451   |\n|[resnet33ts.ra2_in1k](https://huggingface.co/timm/resnet33ts.ra2_in1k)|256     |79.21|94.56|19.7       |4.8  |11.7 |3392   |\n|[resnet50d.gluon_in1k](https://huggingface.co/timm/resnet50d.gluon_in1k)|224     |79.07|94.48|25.6       |4.4  |11.9 |3162   |\n|[resnet50.ram_in1k](https://huggingface.co/timm/resnet50.ram_in1k)|224     |79.03|94.38|25.6       |4.1  |11.1 |3453   |\n|[resnet50.am_in1k](https://huggingface.co/timm/resnet50.am_in1k)|224     |79.01|94.39|25.6       |4.1  |11.1 |3461   |\n|[resnet32ts.ra2_in1k](https://huggingface.co/timm/resnet32ts.ra2_in1k)|256     |79.01|94.37|18.0       |4.6  |11.6 |3440   |\n|[ecaresnet26t.ra2_in1k](https://huggingface.co/timm/ecaresnet26t.ra2_in1k)|256     |78.9 |94.54|16.0       |3.4  |10.5 |3421   |\n|[resnet152.a3_in1k](https://huggingface.co/timm/resnet152.a3_in1k)|160     |78.89|94.11|60.2       |5.9  |11.5 |2745   |\n|[wide_resnet101_2.tv_in1k](https://huggingface.co/timm/wide_resnet101_2.tv_in1k)|224     |78.84|94.28|126.9      |22.8 |21.2 |1079   |\n|[seresnext26d_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26d_32x4d.bt_in1k)|288     |78.83|94.24|16.8       |4.5  |16.8 |2251   |\n|[resnet50.ra_in1k](https://huggingface.co/timm/resnet50.ra_in1k)|224     |78.81|94.32|25.6       |4.1  |11.1 |3454   |\n|[seresnext26t_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26t_32x4d.bt_in1k)|288     |78.74|94.33|16.8       |4.5  |16.7 |2264   |\n|[resnet50s.gluon_in1k](https://huggingface.co/timm/resnet50s.gluon_in1k)|224     |78.72|94.23|25.7       |5.5  |13.5 |2796   |\n|[resnet50d.a3_in1k](https://huggingface.co/timm/resnet50d.a3_in1k)|224     |78.71|94.24|25.6       |4.4  |11.9 |3154   |\n|[wide_resnet50_2.tv_in1k](https://huggingface.co/timm/wide_resnet50_2.tv_in1k)|224     |78.47|94.09|68.9       |11.4 |14.4 |1934   |\n|[resnet50.bt_in1k](https://huggingface.co/timm/resnet50.bt_in1k)|224     |78.46|94.27|25.6       |4.1  |11.1 |3454   |\n|[resnet34d.ra2_in1k](https://huggingface.co/timm/resnet34d.ra2_in1k)|288     |78.43|94.35|21.8       |6.5  |7.5  |3291   |\n|[gcresnext26ts.ch_in1k](https://huggingface.co/timm/gcresnext26ts.ch_in1k)|288     |78.42|94.04|10.5       |3.1  |13.3 |3226   |\n|[resnet26t.ra2_in1k](https://huggingface.co/timm/resnet26t.ra2_in1k)|320     |78.33|94.13|16.0       |5.2  |16.4 |2391   |\n|[resnet152.tv_in1k](https://huggingface.co/timm/resnet152.tv_in1k)|224     |78.32|94.04|60.2       |11.6 |22.6 |1487   |\n|[seresnext26ts.ch_in1k](https://huggingface.co/timm/seresnext26ts.ch_in1k)|288     |78.28|94.1 |10.4       |3.1  |13.3 |3062   |\n|[bat_resnext26ts.ch_in1k](https://huggingface.co/timm/bat_resnext26ts.ch_in1k)|256     |78.25|94.1 |10.7       |2.5  |12.5 |3393   |\n|[resnet50.a3_in1k](https://huggingface.co/timm/resnet50.a3_in1k)|224     |78.06|93.78|25.6       |4.1  |11.1 |3450   |\n|[resnet50c.gluon_in1k](https://huggingface.co/timm/resnet50c.gluon_in1k)|224     |78.0 |93.99|25.6       |4.4  |11.9 |3286   |\n|[eca_resnext26ts.ch_in1k](https://huggingface.co/timm/eca_resnext26ts.ch_in1k)|288     |78.0 |93.91|10.3       |3.1  |13.3 |3297   |\n|[seresnext26t_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26t_32x4d.bt_in1k)|224     |77.98|93.75|16.8       |2.7  |10.1 |3841   |\n|[resnet34.a1_in1k](https://huggingface.co/timm/resnet34.a1_in1k)|288     |77.92|93.77|21.8       |6.1  |6.2  |3609   |\n|[resnet101.a3_in1k](https://huggingface.co/timm/resnet101.a3_in1k)|160     |77.88|93.71|44.6       |4.0  |8.3  |3926   |\n|[resnet26t.ra2_in1k](https://huggingface.co/timm/resnet26t.ra2_in1k)|256     |77.87|93.84|16.0       |3.4  |10.5 |3772   |\n|[seresnext26ts.ch_in1k](https://huggingface.co/timm/seresnext26ts.ch_in1k)|256     |77.86|93.79|10.4       |2.4  |10.5 |4263   |\n|[resnetrs50.tf_in1k](https://huggingface.co/timm/resnetrs50.tf_in1k)|160     |77.82|93.81|35.7       |2.3  |6.2  |5238   |\n|[gcresnext26ts.ch_in1k](https://huggingface.co/timm/gcresnext26ts.ch_in1k)|256     |77.81|93.82|10.5       |2.4  |10.5 |4183   |\n|[ecaresnet50t.a3_in1k](https://huggingface.co/timm/ecaresnet50t.a3_in1k)|160     |77.79|93.6 |25.6       |2.2  |6.0  |5329   |\n|[resnext50_32x4d.a3_in1k](https://huggingface.co/timm/resnext50_32x4d.a3_in1k)|160     |77.73|93.32|25.0       |2.2  |7.4  |5576   |\n|[resnext50_32x4d.tv_in1k](https://huggingface.co/timm/resnext50_32x4d.tv_in1k)|224     |77.61|93.7 |25.0       |4.3  |14.4 |2944   |\n|[seresnext26d_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26d_32x4d.bt_in1k)|224     |77.59|93.61|16.8       |2.7  |10.2 |3807   |\n|[resnet50.gluon_in1k](https://huggingface.co/timm/resnet50.gluon_in1k)|224     |77.58|93.72|25.6       |4.1  |11.1 |3455   |\n|[eca_resnext26ts.ch_in1k](https://huggingface.co/timm/eca_resnext26ts.ch_in1k)|256     |77.44|93.56|10.3       |2.4  |10.5 |4284   |\n|[resnet26d.bt_in1k](https://huggingface.co/timm/resnet26d.bt_in1k)|288     |77.41|93.63|16.0       |4.3  |13.5 |2907   |\n|[resnet101.tv_in1k](https://huggingface.co/timm/resnet101.tv_in1k)|224     |77.38|93.54|44.6       |7.8  |16.2 |2125   |\n|[resnet50d.a3_in1k](https://huggingface.co/timm/resnet50d.a3_in1k)|160     |77.22|93.27|25.6       |2.2  |6.1  |5982   |\n|[resnext26ts.ra2_in1k](https://huggingface.co/timm/resnext26ts.ra2_in1k)|288     |77.17|93.47|10.3       |3.1  |13.3 |3392   |\n|[resnet34.a2_in1k](https://huggingface.co/timm/resnet34.a2_in1k)|288     |77.15|93.27|21.8       |6.1  |6.2  |3615   |\n|[resnet34d.ra2_in1k](https://huggingface.co/timm/resnet34d.ra2_in1k)|224     |77.1 |93.37|21.8       |3.9  |4.5  |5436   |\n|[seresnet50.a3_in1k](https://huggingface.co/timm/seresnet50.a3_in1k)|224     |77.02|93.07|28.1       |4.1  |11.1 |2952   |\n|[resnext26ts.ra2_in1k](https://huggingface.co/timm/resnext26ts.ra2_in1k)|256     |76.78|93.13|10.3       |2.4  |10.5 |4410   |\n|[resnet26d.bt_in1k](https://huggingface.co/timm/resnet26d.bt_in1k)|224     |76.7 |93.17|16.0       |2.6  |8.2  |4859   |\n|[resnet34.bt_in1k](https://huggingface.co/timm/resnet34.bt_in1k)|288     |76.5 |93.35|21.8       |6.1  |6.2  |3617   |\n|[resnet34.a1_in1k](https://huggingface.co/timm/resnet34.a1_in1k)|224     |76.42|92.87|21.8       |3.7  |3.7  |5984   |\n|[resnet26.bt_in1k](https://huggingface.co/timm/resnet26.bt_in1k)|288     |76.35|93.18|16.0       |3.9  |12.2 |3331   |\n|[resnet50.tv_in1k](https://huggingface.co/timm/resnet50.tv_in1k)|224     |76.13|92.86|25.6       |4.1  |11.1 |3457   |\n|[resnet50.a3_in1k](https://huggingface.co/timm/resnet50.a3_in1k)|160     |75.96|92.5 |25.6       |2.1  |5.7  |6490   |\n|[resnet34.a2_in1k](https://huggingface.co/timm/resnet34.a2_in1k)|224     |75.52|92.44|21.8       |3.7  |3.7  |5991   |\n|[resnet26.bt_in1k](https://huggingface.co/timm/resnet26.bt_in1k)|224     |75.3 |92.58|16.0       |2.4  |7.4  |5583   |\n|[resnet34.bt_in1k](https://huggingface.co/timm/resnet34.bt_in1k)|224     |75.16|92.18|21.8       |3.7  |3.7  |5994   |\n|[seresnet50.a3_in1k](https://huggingface.co/timm/seresnet50.a3_in1k)|160     |75.1 |92.08|28.1       |2.1  |5.7  |5513   |\n|[resnet34.gluon_in1k](https://huggingface.co/timm/resnet34.gluon_in1k)|224     |74.57|91.98|21.8       |3.7  |3.7  |5984   |\n|[resnet18d.ra2_in1k](https://huggingface.co/timm/resnet18d.ra2_in1k)|288     |73.81|91.83|11.7       |3.4  |5.4  |5196   |\n|[resnet34.tv_in1k](https://huggingface.co/timm/resnet34.tv_in1k)|224     |73.32|91.42|21.8       |3.7  |3.7  |5979   |\n|[resnet18.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnet18.fb_swsl_ig1b_ft_in1k)|224     |73.28|91.73|11.7       |1.8  |2.5  |10213  |\n|[resnet18.a1_in1k](https://huggingface.co/timm/resnet18.a1_in1k)|288     |73.16|91.03|11.7       |3.0  |4.1  |6050   |\n|[resnet34.a3_in1k](https://huggingface.co/timm/resnet34.a3_in1k)|224     |72.98|91.11|21.8       |3.7  |3.7  |5967   |\n|[resnet18.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnet18.fb_ssl_yfcc100m_ft_in1k)|224     |72.6 |91.42|11.7       |1.8  |2.5  |10213  |\n|[resnet18.a2_in1k](https://huggingface.co/timm/resnet18.a2_in1k)|288     |72.37|90.59|11.7       |3.0  |4.1  |6051   |\n|[resnet14t.c3_in1k](https://huggingface.co/timm/resnet14t.c3_in1k)|224     |72.26|90.31|10.1       |1.7  |5.8  |7026   |\n|[resnet18d.ra2_in1k](https://huggingface.co/timm/resnet18d.ra2_in1k)|224     |72.26|90.68|11.7       |2.1  |3.3  |8707   |\n|[resnet18.a1_in1k](https://huggingface.co/timm/resnet18.a1_in1k)|224     |71.49|90.07|11.7       |1.8  |2.5  |10187  |\n|[resnet14t.c3_in1k](https://huggingface.co/timm/resnet14t.c3_in1k)|176     |71.31|89.69|10.1       |1.1  |3.6  |10970  |\n|[resnet18.gluon_in1k](https://huggingface.co/timm/resnet18.gluon_in1k)|224     |70.84|89.76|11.7       |1.8  |2.5  |10210  |\n|[resnet18.a2_in1k](https://huggingface.co/timm/resnet18.a2_in1k)|224     |70.64|89.47|11.7       |1.8  |2.5  |10194  |\n|[resnet34.a3_in1k](https://huggingface.co/timm/resnet34.a3_in1k)|160     |70.56|89.52|21.8       |1.9  |1.9  |10737  |\n|[resnet18.tv_in1k](https://huggingface.co/timm/resnet18.tv_in1k)|224     |69.76|89.07|11.7       |1.8  |2.5  |10205  |\n|[resnet10t.c3_in1k](https://huggingface.co/timm/resnet10t.c3_in1k)|224     |68.34|88.03|5.4        |1.1  |2.4  |13079  |\n|[resnet18.a3_in1k](https://huggingface.co/timm/resnet18.a3_in1k)|224     |68.25|88.17|11.7       |1.8  |2.5  |10167  |\n|[resnet10t.c3_in1k](https://huggingface.co/timm/resnet10t.c3_in1k)|176     |66.71|86.96|5.4        |0.7  |1.5  |20327  |\n|[resnet18.a3_in1k](https://huggingface.co/timm/resnet18.a3_in1k)|160     |65.66|86.26|11.7       |0.9  |1.3  |18229  |\n\n## Citation\n```bibtex\n@inproceedings{wightman2021resnet,\n  title={ResNet strikes back: An improved training procedure in timm},\n  author={Wightman, Ross and Touvron, Hugo and Jegou, Herve},\n  booktitle={NeurIPS 2021 Workshop on ImageNet: Past, Present, and Future}\n}\n```\n```bibtex\n@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}\n```\n```bibtex\n@article{He2015,\n  author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},\n  title = {Deep Residual Learning for Image Recognition},\n  journal = {arXiv preprint arXiv:1512.03385},\n  year = {2015}\n}\n```\n",
                  "extracted_code": "from urllib.request import urlopen\nfrom PIL import Image\nimport timm\nimport torch\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model('resnet50.a1_in1k', pretrained=True)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\n\n\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnet50.a1_in1k',\n    pretrained=True,\n    features_only=True,\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\nfor o in output:\n    # print shape of each feature map in output\n    # e.g.:\n    #  torch.Size([1, 64, 112, 112])\n    #  torch.Size([1, 256, 56, 56])\n    #  torch.Size([1, 512, 28, 28])\n    #  torch.Size([1, 1024, 14, 14])\n    #  torch.Size([1, 2048, 7, 7])\n\n    print(o.shape)\n\n\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnet50.a1_in1k',\n    pretrained=True,\n    num_classes=0,  # remove classifier nn.Linear\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n\n# or equivalently (without needing to set num_classes=0)\n\noutput = model.forward_features(transforms(img).unsqueeze(0))\n# output is unpooled, a (1, 2048, 7, 7) shaped tensor\n\noutput = model.forward_head(output, pre_logits=True)\n# output is a (1, num_features) shaped tensor"
                },
                {
                  "id": "timm/resnet50.ram_in1k",
                  "author": "timm",
                  "sha": "6e2446dfff2a12859ab4c1b2dc5cb3d95fccab8f",
                  "created_at": "2023-04-05T18:14:08+00:00",
                  "last_modified": "2025-01-21T21:40:39+00:00",
                  "private": false,
                  "gated": false,
                  "disabled": false,
                  "downloads": 496565,
                  "likes": 0,
                  "siblings": [
                    {
                      "rfilename": ".gitattributes"
                    },
                    {
                      "rfilename": "README.md"
                    },
                    {
                      "rfilename": "config.json"
                    },
                    {
                      "rfilename": "model.safetensors"
                    },
                    {
                      "rfilename": "pytorch_model.bin"
                    }
                  ],
                  "card_data": {
                    "license": "apache-2.0",
                    "language": [],
                    "library_name": "timm",
                    "tags": [
                      "image-classification",
                      "timm",
                      "transformers"
                    ],
                    "datasets": [],
                    "task_categories": [],
                    "size_categories": [],
                    "metrics": [],
                    "widget": []
                  },
                  "tags": [
                    "timm",
                    "pytorch",
                    "safetensors",
                    "image-classification",
                    "transformers",
                    "arxiv:1512.03385",
                    "license:apache-2.0",
                    "region:us"
                  ],
                  "pipeline_tag": "image-classification",
                  "library_name": "timm",
                  "readme": "---\nlicense: apache-2.0\nlibrary_name: timm\ntags:\n- image-classification\n- timm\n- transformers\n---\n# Model card for resnet50.ram_in1k\n\nA ResNet-B image classification model.\n\nThis model features:\n * ReLU activations\n * single layer 7x7 convolution with pooling\n * 1x1 convolution shortcut downsample\n\nTrained on ImageNet-1k in `timm` using recipe template described below.\n\nRecipe details:\n * AugMix (with RandAugment) recipe\n * SGD (w/ Nesterov) optimizer and JSD (Jensen–Shannon divergence) loss\n * Cosine LR schedule with warmup\n\n\n## Model Details\n- **Model Type:** Image classification / feature backbone\n- **Model Stats:**\n  - Params (M): 25.6\n  - GMACs: 4.1\n  - Activations (M): 11.1\n  - Image size: train = 224 x 224, test = 288 x 288\n- **Papers:**\n  - Deep Residual Learning for Image Recognition: https://arxiv.org/abs/1512.03385\n- **Original:** https://github.com/huggingface/pytorch-image-models\n\n## Model Usage\n### Image Classification\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model('resnet50.ram_in1k', pretrained=True)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\n```\n\n### Feature Map Extraction\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnet50.ram_in1k',\n    pretrained=True,\n    features_only=True,\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\nfor o in output:\n    # print shape of each feature map in output\n    # e.g.:\n    #  torch.Size([1, 64, 112, 112])\n    #  torch.Size([1, 256, 56, 56])\n    #  torch.Size([1, 512, 28, 28])\n    #  torch.Size([1, 1024, 14, 14])\n    #  torch.Size([1, 2048, 7, 7])\n\n    print(o.shape)\n```\n\n### Image Embeddings\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnet50.ram_in1k',\n    pretrained=True,\n    num_classes=0,  # remove classifier nn.Linear\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n\n# or equivalently (without needing to set num_classes=0)\n\noutput = model.forward_features(transforms(img).unsqueeze(0))\n# output is unpooled, a (1, 2048, 7, 7) shaped tensor\n\noutput = model.forward_head(output, pre_logits=True)\n# output is a (1, num_features) shaped tensor\n```\n\n## Model Comparison\nExplore the dataset and runtime metrics of this model in timm [model results](https://github.com/huggingface/pytorch-image-models/tree/main/results).\n\n|model                                     |img_size|top1 |top5 |param_count|gmacs|macts|img/sec|\n|------------------------------------------|--------|-----|-----|-----------|-----|-----|-------|\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k_288](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k_288)|320     |86.72|98.17|93.6       |35.2 |69.7 |451    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k_288](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k_288)|288     |86.51|98.08|93.6       |28.5 |56.4 |560    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k)|288     |86.49|98.03|93.6       |28.5 |56.4 |557    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k)|224     |85.96|97.82|93.6       |17.2 |34.2 |923    |\n|[resnext101_32x32d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x32d.fb_wsl_ig1b_ft_in1k)|224     |85.11|97.44|468.5      |87.3 |91.1 |254    |\n|[resnetrs420.tf_in1k](https://huggingface.co/timm/resnetrs420.tf_in1k)|416     |85.0 |97.12|191.9      |108.4|213.8|134    |\n|[ecaresnet269d.ra2_in1k](https://huggingface.co/timm/ecaresnet269d.ra2_in1k)|352     |84.96|97.22|102.1      |50.2 |101.2|291    |\n|[ecaresnet269d.ra2_in1k](https://huggingface.co/timm/ecaresnet269d.ra2_in1k)|320     |84.73|97.18|102.1      |41.5 |83.7 |353    |\n|[resnetrs350.tf_in1k](https://huggingface.co/timm/resnetrs350.tf_in1k)|384     |84.71|96.99|164.0      |77.6 |154.7|183    |\n|[seresnextaa101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.ah_in1k)|288     |84.57|97.08|93.6       |28.5 |56.4 |557    |\n|[resnetrs200.tf_in1k](https://huggingface.co/timm/resnetrs200.tf_in1k)|320     |84.45|97.08|93.2       |31.5 |67.8 |446    |\n|[resnetrs270.tf_in1k](https://huggingface.co/timm/resnetrs270.tf_in1k)|352     |84.43|96.97|129.9      |51.1 |105.5|280    |\n|[seresnext101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101d_32x8d.ah_in1k)|288     |84.36|96.92|93.6       |27.6 |53.0 |595    |\n|[seresnet152d.ra2_in1k](https://huggingface.co/timm/seresnet152d.ra2_in1k)|320     |84.35|97.04|66.8       |24.1 |47.7 |610    |\n|[resnetrs350.tf_in1k](https://huggingface.co/timm/resnetrs350.tf_in1k)|288     |84.3 |96.94|164.0      |43.7 |87.1 |333    |\n|[resnext101_32x8d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_swsl_ig1b_ft_in1k)|224     |84.28|97.17|88.8       |16.5 |31.2 |1100   |\n|[resnetrs420.tf_in1k](https://huggingface.co/timm/resnetrs420.tf_in1k)|320     |84.24|96.86|191.9      |64.2 |126.6|228    |\n|[seresnext101_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101_32x8d.ah_in1k)|288     |84.19|96.87|93.6       |27.2 |51.6 |613    |\n|[resnext101_32x16d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_wsl_ig1b_ft_in1k)|224     |84.18|97.19|194.0      |36.3 |51.2 |581    |\n|[resnetaa101d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa101d.sw_in12k_ft_in1k)|288     |84.11|97.11|44.6       |15.1 |29.0 |1144   |\n|[resnet200d.ra2_in1k](https://huggingface.co/timm/resnet200d.ra2_in1k)|320     |83.97|96.82|64.7       |31.2 |67.3 |518    |\n|[resnetrs200.tf_in1k](https://huggingface.co/timm/resnetrs200.tf_in1k)|256     |83.87|96.75|93.2       |20.2 |43.4 |692    |\n|[seresnextaa101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.ah_in1k)|224     |83.86|96.65|93.6       |17.2 |34.2 |923    |\n|[resnetrs152.tf_in1k](https://huggingface.co/timm/resnetrs152.tf_in1k)|320     |83.72|96.61|86.6       |24.3 |48.1 |617    |\n|[seresnet152d.ra2_in1k](https://huggingface.co/timm/seresnet152d.ra2_in1k)|256     |83.69|96.78|66.8       |15.4 |30.6 |943    |\n|[seresnext101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101d_32x8d.ah_in1k)|224     |83.68|96.61|93.6       |16.7 |32.0 |986    |\n|[resnet152d.ra2_in1k](https://huggingface.co/timm/resnet152d.ra2_in1k)|320     |83.67|96.74|60.2       |24.1 |47.7 |706    |\n|[resnetrs270.tf_in1k](https://huggingface.co/timm/resnetrs270.tf_in1k)|256     |83.59|96.61|129.9      |27.1 |55.8 |526    |\n|[seresnext101_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101_32x8d.ah_in1k)|224     |83.58|96.4 |93.6       |16.5 |31.2 |1013   |\n|[resnetaa101d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa101d.sw_in12k_ft_in1k)|224     |83.54|96.83|44.6       |9.1  |17.6 |1864   |\n|[resnet152.a1h_in1k](https://huggingface.co/timm/resnet152.a1h_in1k)|288     |83.46|96.54|60.2       |19.1 |37.3 |904    |\n|[resnext101_32x16d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_swsl_ig1b_ft_in1k)|224     |83.35|96.85|194.0      |36.3 |51.2 |582    |\n|[resnet200d.ra2_in1k](https://huggingface.co/timm/resnet200d.ra2_in1k)|256     |83.23|96.53|64.7       |20.0 |43.1 |809    |\n|[resnext101_32x4d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x4d.fb_swsl_ig1b_ft_in1k)|224     |83.22|96.75|44.2       |8.0  |21.2 |1814   |\n|[resnext101_64x4d.c1_in1k](https://huggingface.co/timm/resnext101_64x4d.c1_in1k)|288     |83.16|96.38|83.5       |25.7 |51.6 |590    |\n|[resnet152d.ra2_in1k](https://huggingface.co/timm/resnet152d.ra2_in1k)|256     |83.14|96.38|60.2       |15.4 |30.5 |1096   |\n|[resnet101d.ra2_in1k](https://huggingface.co/timm/resnet101d.ra2_in1k)|320     |83.02|96.45|44.6       |16.5 |34.8 |992    |\n|[ecaresnet101d.miil_in1k](https://huggingface.co/timm/ecaresnet101d.miil_in1k)|288     |82.98|96.54|44.6       |13.4 |28.2 |1077   |\n|[resnext101_64x4d.tv_in1k](https://huggingface.co/timm/resnext101_64x4d.tv_in1k)|224     |82.98|96.25|83.5       |15.5 |31.2 |989    |\n|[resnetrs152.tf_in1k](https://huggingface.co/timm/resnetrs152.tf_in1k)|256     |82.86|96.28|86.6       |15.6 |30.8 |951    |\n|[resnext101_32x8d.tv2_in1k](https://huggingface.co/timm/resnext101_32x8d.tv2_in1k)|224     |82.83|96.22|88.8       |16.5 |31.2 |1099   |\n|[resnet152.a1h_in1k](https://huggingface.co/timm/resnet152.a1h_in1k)|224     |82.8 |96.13|60.2       |11.6 |22.6 |1486   |\n|[resnet101.a1h_in1k](https://huggingface.co/timm/resnet101.a1h_in1k)|288     |82.8 |96.32|44.6       |13.0 |26.8 |1291   |\n|[resnet152.a1_in1k](https://huggingface.co/timm/resnet152.a1_in1k)|288     |82.74|95.71|60.2       |19.1 |37.3 |905    |\n|[resnext101_32x8d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_wsl_ig1b_ft_in1k)|224     |82.69|96.63|88.8       |16.5 |31.2 |1100   |\n|[resnet152.a2_in1k](https://huggingface.co/timm/resnet152.a2_in1k)|288     |82.62|95.75|60.2       |19.1 |37.3 |904    |\n|[resnetaa50d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa50d.sw_in12k_ft_in1k)|288     |82.61|96.49|25.6       |8.9  |20.6 |1729   |\n|[resnet61q.ra2_in1k](https://huggingface.co/timm/resnet61q.ra2_in1k)|288     |82.53|96.13|36.8       |9.9  |21.5 |1773   |\n|[wide_resnet101_2.tv2_in1k](https://huggingface.co/timm/wide_resnet101_2.tv2_in1k)|224     |82.5 |96.02|126.9      |22.8 |21.2 |1078   |\n|[resnext101_64x4d.c1_in1k](https://huggingface.co/timm/resnext101_64x4d.c1_in1k)|224     |82.46|95.92|83.5       |15.5 |31.2 |987    |\n|[resnet51q.ra2_in1k](https://huggingface.co/timm/resnet51q.ra2_in1k)|288     |82.36|96.18|35.7       |8.1  |20.9 |1964   |\n|[ecaresnet50t.ra2_in1k](https://huggingface.co/timm/ecaresnet50t.ra2_in1k)|320     |82.35|96.14|25.6       |8.8  |24.1 |1386   |\n|[resnet101.a1_in1k](https://huggingface.co/timm/resnet101.a1_in1k)|288     |82.31|95.63|44.6       |13.0 |26.8 |1291   |\n|[resnetrs101.tf_in1k](https://huggingface.co/timm/resnetrs101.tf_in1k)|288     |82.29|96.01|63.6       |13.6 |28.5 |1078   |\n|[resnet152.tv2_in1k](https://huggingface.co/timm/resnet152.tv2_in1k)|224     |82.29|96.0 |60.2       |11.6 |22.6 |1484   |\n|[wide_resnet50_2.racm_in1k](https://huggingface.co/timm/wide_resnet50_2.racm_in1k)|288     |82.27|96.06|68.9       |18.9 |23.8 |1176   |\n|[resnet101d.ra2_in1k](https://huggingface.co/timm/resnet101d.ra2_in1k)|256     |82.26|96.07|44.6       |10.6 |22.2 |1542   |\n|[resnet101.a2_in1k](https://huggingface.co/timm/resnet101.a2_in1k)|288     |82.24|95.73|44.6       |13.0 |26.8 |1290   |\n|[seresnext50_32x4d.racm_in1k](https://huggingface.co/timm/seresnext50_32x4d.racm_in1k)|288     |82.2 |96.14|27.6       |7.0  |23.8 |1547   |\n|[ecaresnet101d.miil_in1k](https://huggingface.co/timm/ecaresnet101d.miil_in1k)|224     |82.18|96.05|44.6       |8.1  |17.1 |1771   |\n|[resnext50_32x4d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext50_32x4d.fb_swsl_ig1b_ft_in1k)|224     |82.17|96.22|25.0       |4.3  |14.4 |2943   |\n|[ecaresnet50t.a1_in1k](https://huggingface.co/timm/ecaresnet50t.a1_in1k)|288     |82.12|95.65|25.6       |7.1  |19.6 |1704   |\n|[resnext50_32x4d.a1h_in1k](https://huggingface.co/timm/resnext50_32x4d.a1h_in1k)|288     |82.03|95.94|25.0       |7.0  |23.8 |1745   |\n|[ecaresnet101d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet101d_pruned.miil_in1k)|288     |82.0 |96.15|24.9       |5.8  |12.7 |1787   |\n|[resnet61q.ra2_in1k](https://huggingface.co/timm/resnet61q.ra2_in1k)|256     |81.99|95.85|36.8       |7.8  |17.0 |2230   |\n|[resnext101_32x8d.tv2_in1k](https://huggingface.co/timm/resnext101_32x8d.tv2_in1k)|176     |81.98|95.72|88.8       |10.3 |19.4 |1768   |\n|[resnet152.a1_in1k](https://huggingface.co/timm/resnet152.a1_in1k)|224     |81.97|95.24|60.2       |11.6 |22.6 |1486   |\n|[resnet101.a1h_in1k](https://huggingface.co/timm/resnet101.a1h_in1k)|224     |81.93|95.75|44.6       |7.8  |16.2 |2122   |\n|[resnet101.tv2_in1k](https://huggingface.co/timm/resnet101.tv2_in1k)|224     |81.9 |95.77|44.6       |7.8  |16.2 |2118   |\n|[resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k)|224     |81.84|96.1 |194.0      |36.3 |51.2 |583    |\n|[resnet51q.ra2_in1k](https://huggingface.co/timm/resnet51q.ra2_in1k)|256     |81.78|95.94|35.7       |6.4  |16.6 |2471   |\n|[resnet152.a2_in1k](https://huggingface.co/timm/resnet152.a2_in1k)|224     |81.77|95.22|60.2       |11.6 |22.6 |1485   |\n|[resnetaa50d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa50d.sw_in12k_ft_in1k)|224     |81.74|96.06|25.6       |5.4  |12.4 |2813   |\n|[ecaresnet50t.a2_in1k](https://huggingface.co/timm/ecaresnet50t.a2_in1k)|288     |81.65|95.54|25.6       |7.1  |19.6 |1703   |\n|[ecaresnet50d.miil_in1k](https://huggingface.co/timm/ecaresnet50d.miil_in1k)|288     |81.64|95.88|25.6       |7.2  |19.7 |1694   |\n|[resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k)|224     |81.62|96.04|88.8       |16.5 |31.2 |1101   |\n|[wide_resnet50_2.tv2_in1k](https://huggingface.co/timm/wide_resnet50_2.tv2_in1k)|224     |81.61|95.76|68.9       |11.4 |14.4 |1930   |\n|[resnetaa50.a1h_in1k](https://huggingface.co/timm/resnetaa50.a1h_in1k)|288     |81.61|95.83|25.6       |8.5  |19.2 |1868   |\n|[resnet101.a1_in1k](https://huggingface.co/timm/resnet101.a1_in1k)|224     |81.5 |95.16|44.6       |7.8  |16.2 |2125   |\n|[resnext50_32x4d.a1_in1k](https://huggingface.co/timm/resnext50_32x4d.a1_in1k)|288     |81.48|95.16|25.0       |7.0  |23.8 |1745   |\n|[gcresnet50t.ra2_in1k](https://huggingface.co/timm/gcresnet50t.ra2_in1k)|288     |81.47|95.71|25.9       |6.9  |18.6 |2071   |\n|[wide_resnet50_2.racm_in1k](https://huggingface.co/timm/wide_resnet50_2.racm_in1k)|224     |81.45|95.53|68.9       |11.4 |14.4 |1929   |\n|[resnet50d.a1_in1k](https://huggingface.co/timm/resnet50d.a1_in1k)|288     |81.44|95.22|25.6       |7.2  |19.7 |1908   |\n|[ecaresnet50t.ra2_in1k](https://huggingface.co/timm/ecaresnet50t.ra2_in1k)|256     |81.44|95.67|25.6       |5.6  |15.4 |2168   |\n|[ecaresnetlight.miil_in1k](https://huggingface.co/timm/ecaresnetlight.miil_in1k)|288     |81.4 |95.82|30.2       |6.8  |13.9 |2132   |\n|[resnet50d.ra2_in1k](https://huggingface.co/timm/resnet50d.ra2_in1k)|288     |81.37|95.74|25.6       |7.2  |19.7 |1910   |\n|[resnet101.a2_in1k](https://huggingface.co/timm/resnet101.a2_in1k)|224     |81.32|95.19|44.6       |7.8  |16.2 |2125   |\n|[seresnet50.ra2_in1k](https://huggingface.co/timm/seresnet50.ra2_in1k)|288     |81.3 |95.65|28.1       |6.8  |18.4 |1803   |\n|[resnext50_32x4d.a2_in1k](https://huggingface.co/timm/resnext50_32x4d.a2_in1k)|288     |81.3 |95.11|25.0       |7.0  |23.8 |1746   |\n|[seresnext50_32x4d.racm_in1k](https://huggingface.co/timm/seresnext50_32x4d.racm_in1k)|224     |81.27|95.62|27.6       |4.3  |14.4 |2591   |\n|[ecaresnet50t.a1_in1k](https://huggingface.co/timm/ecaresnet50t.a1_in1k)|224     |81.26|95.16|25.6       |4.3  |11.8 |2823   |\n|[gcresnext50ts.ch_in1k](https://huggingface.co/timm/gcresnext50ts.ch_in1k)|288     |81.23|95.54|15.7       |4.8  |19.6 |2117   |\n|[senet154.gluon_in1k](https://huggingface.co/timm/senet154.gluon_in1k)|224     |81.23|95.35|115.1      |20.8 |38.7 |545    |\n|[resnet50.a1_in1k](https://huggingface.co/timm/resnet50.a1_in1k)|288     |81.22|95.11|25.6       |6.8  |18.4 |2089   |\n|[resnet50_gn.a1h_in1k](https://huggingface.co/timm/resnet50_gn.a1h_in1k)|288     |81.22|95.63|25.6       |6.8  |18.4 |676    |\n|[resnet50d.a2_in1k](https://huggingface.co/timm/resnet50d.a2_in1k)|288     |81.18|95.09|25.6       |7.2  |19.7 |1908   |\n|[resnet50.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnet50.fb_swsl_ig1b_ft_in1k)|224     |81.18|95.98|25.6       |4.1  |11.1 |3455   |\n|[resnext50_32x4d.tv2_in1k](https://huggingface.co/timm/resnext50_32x4d.tv2_in1k)|224     |81.17|95.34|25.0       |4.3  |14.4 |2933   |\n|[resnext50_32x4d.a1h_in1k](https://huggingface.co/timm/resnext50_32x4d.a1h_in1k)|224     |81.1 |95.33|25.0       |4.3  |14.4 |2934   |\n|[seresnet50.a2_in1k](https://huggingface.co/timm/seresnet50.a2_in1k)|288     |81.1 |95.23|28.1       |6.8  |18.4 |1801   |\n|[seresnet50.a1_in1k](https://huggingface.co/timm/seresnet50.a1_in1k)|288     |81.1 |95.12|28.1       |6.8  |18.4 |1799   |\n|[resnet152s.gluon_in1k](https://huggingface.co/timm/resnet152s.gluon_in1k)|224     |81.02|95.41|60.3       |12.9 |25.0 |1347   |\n|[resnet50.d_in1k](https://huggingface.co/timm/resnet50.d_in1k)|288     |80.97|95.44|25.6       |6.8  |18.4 |2085   |\n|[gcresnet50t.ra2_in1k](https://huggingface.co/timm/gcresnet50t.ra2_in1k)|256     |80.94|95.45|25.9       |5.4  |14.7 |2571   |\n|[resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k)|224     |80.93|95.73|44.2       |8.0  |21.2 |1814   |\n|[resnet50.c1_in1k](https://huggingface.co/timm/resnet50.c1_in1k)|288     |80.91|95.55|25.6       |6.8  |18.4 |2084   |\n|[seresnext101_32x4d.gluon_in1k](https://huggingface.co/timm/seresnext101_32x4d.gluon_in1k)|224     |80.9 |95.31|49.0       |8.0  |21.3 |1585   |\n|[seresnext101_64x4d.gluon_in1k](https://huggingface.co/timm/seresnext101_64x4d.gluon_in1k)|224     |80.9 |95.3 |88.2       |15.5 |31.2 |918    |\n|[resnet50.c2_in1k](https://huggingface.co/timm/resnet50.c2_in1k)|288     |80.86|95.52|25.6       |6.8  |18.4 |2085   |\n|[resnet50.tv2_in1k](https://huggingface.co/timm/resnet50.tv2_in1k)|224     |80.85|95.43|25.6       |4.1  |11.1 |3450   |\n|[ecaresnet50t.a2_in1k](https://huggingface.co/timm/ecaresnet50t.a2_in1k)|224     |80.84|95.02|25.6       |4.3  |11.8 |2821   |\n|[ecaresnet101d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet101d_pruned.miil_in1k)|224     |80.79|95.62|24.9       |3.5  |7.7  |2961   |\n|[seresnet33ts.ra2_in1k](https://huggingface.co/timm/seresnet33ts.ra2_in1k)|288     |80.79|95.36|19.8       |6.0  |14.8 |2506   |\n|[ecaresnet50d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet50d_pruned.miil_in1k)|288     |80.79|95.58|19.9       |4.2  |10.6 |2349   |\n|[resnet50.a2_in1k](https://huggingface.co/timm/resnet50.a2_in1k)|288     |80.78|94.99|25.6       |6.8  |18.4 |2088   |\n|[resnet50.b1k_in1k](https://huggingface.co/timm/resnet50.b1k_in1k)|288     |80.71|95.43|25.6       |6.8  |18.4 |2087   |\n|[resnext50_32x4d.ra_in1k](https://huggingface.co/timm/resnext50_32x4d.ra_in1k)|288     |80.7 |95.39|25.0       |7.0  |23.8 |1749   |\n|[resnetrs101.tf_in1k](https://huggingface.co/timm/resnetrs101.tf_in1k)|192     |80.69|95.24|63.6       |6.0  |12.7 |2270   |\n|[resnet50d.a1_in1k](https://huggingface.co/timm/resnet50d.a1_in1k)|224     |80.68|94.71|25.6       |4.4  |11.9 |3162   |\n|[eca_resnet33ts.ra2_in1k](https://huggingface.co/timm/eca_resnet33ts.ra2_in1k)|288     |80.68|95.36|19.7       |6.0  |14.8 |2637   |\n|[resnet50.a1h_in1k](https://huggingface.co/timm/resnet50.a1h_in1k)|224     |80.67|95.3 |25.6       |4.1  |11.1 |3452   |\n|[resnext50d_32x4d.bt_in1k](https://huggingface.co/timm/resnext50d_32x4d.bt_in1k)|288     |80.67|95.42|25.0       |7.4  |25.1 |1626   |\n|[resnetaa50.a1h_in1k](https://huggingface.co/timm/resnetaa50.a1h_in1k)|224     |80.63|95.21|25.6       |5.2  |11.6 |3034   |\n|[ecaresnet50d.miil_in1k](https://huggingface.co/timm/ecaresnet50d.miil_in1k)|224     |80.61|95.32|25.6       |4.4  |11.9 |2813   |\n|[resnext101_64x4d.gluon_in1k](https://huggingface.co/timm/resnext101_64x4d.gluon_in1k)|224     |80.61|94.99|83.5       |15.5 |31.2 |989    |\n|[gcresnet33ts.ra2_in1k](https://huggingface.co/timm/gcresnet33ts.ra2_in1k)|288     |80.6 |95.31|19.9       |6.0  |14.8 |2578   |\n|[gcresnext50ts.ch_in1k](https://huggingface.co/timm/gcresnext50ts.ch_in1k)|256     |80.57|95.17|15.7       |3.8  |15.5 |2710   |\n|[resnet152.a3_in1k](https://huggingface.co/timm/resnet152.a3_in1k)|224     |80.56|95.0 |60.2       |11.6 |22.6 |1483   |\n|[resnet50d.ra2_in1k](https://huggingface.co/timm/resnet50d.ra2_in1k)|224     |80.53|95.16|25.6       |4.4  |11.9 |3164   |\n|[resnext50_32x4d.a1_in1k](https://huggingface.co/timm/resnext50_32x4d.a1_in1k)|224     |80.53|94.46|25.0       |4.3  |14.4 |2930   |\n|[wide_resnet101_2.tv2_in1k](https://huggingface.co/timm/wide_resnet101_2.tv2_in1k)|176     |80.48|94.98|126.9      |14.3 |13.2 |1719   |\n|[resnet152d.gluon_in1k](https://huggingface.co/timm/resnet152d.gluon_in1k)|224     |80.47|95.2 |60.2       |11.8 |23.4 |1428   |\n|[resnet50.b2k_in1k](https://huggingface.co/timm/resnet50.b2k_in1k)|288     |80.45|95.32|25.6       |6.8  |18.4 |2086   |\n|[ecaresnetlight.miil_in1k](https://huggingface.co/timm/ecaresnetlight.miil_in1k)|224     |80.45|95.24|30.2       |4.1  |8.4  |3530   |\n|[resnext50_32x4d.a2_in1k](https://huggingface.co/timm/resnext50_32x4d.a2_in1k)|224     |80.45|94.63|25.0       |4.3  |14.4 |2936   |\n|[wide_resnet50_2.tv2_in1k](https://huggingface.co/timm/wide_resnet50_2.tv2_in1k)|176     |80.43|95.09|68.9       |7.3  |9.0  |3015   |\n|[resnet101d.gluon_in1k](https://huggingface.co/timm/resnet101d.gluon_in1k)|224     |80.42|95.01|44.6       |8.1  |17.0 |2007   |\n|[resnet50.a1_in1k](https://huggingface.co/timm/resnet50.a1_in1k)|224     |80.38|94.6 |25.6       |4.1  |11.1 |3461   |\n|[seresnet33ts.ra2_in1k](https://huggingface.co/timm/seresnet33ts.ra2_in1k)|256     |80.36|95.1 |19.8       |4.8  |11.7 |3267   |\n|[resnext101_32x4d.gluon_in1k](https://huggingface.co/timm/resnext101_32x4d.gluon_in1k)|224     |80.34|94.93|44.2       |8.0  |21.2 |1814   |\n|[resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k)|224     |80.32|95.4 |25.0       |4.3  |14.4 |2941   |\n|[resnet101s.gluon_in1k](https://huggingface.co/timm/resnet101s.gluon_in1k)|224     |80.28|95.16|44.7       |9.2  |18.6 |1851   |\n|[seresnet50.ra2_in1k](https://huggingface.co/timm/seresnet50.ra2_in1k)|224     |80.26|95.08|28.1       |4.1  |11.1 |2972   |\n|[resnetblur50.bt_in1k](https://huggingface.co/timm/resnetblur50.bt_in1k)|288     |80.24|95.24|25.6       |8.5  |19.9 |1523   |\n|[resnet50d.a2_in1k](https://huggingface.co/timm/resnet50d.a2_in1k)|224     |80.22|94.63|25.6       |4.4  |11.9 |3162   |\n|[resnet152.tv2_in1k](https://huggingface.co/timm/resnet152.tv2_in1k)|176     |80.2 |94.64|60.2       |7.2  |14.0 |2346   |\n|[seresnet50.a2_in1k](https://huggingface.co/timm/seresnet50.a2_in1k)|224     |80.08|94.74|28.1       |4.1  |11.1 |2969   |\n|[eca_resnet33ts.ra2_in1k](https://huggingface.co/timm/eca_resnet33ts.ra2_in1k)|256     |80.08|94.97|19.7       |4.8  |11.7 |3284   |\n|[gcresnet33ts.ra2_in1k](https://huggingface.co/timm/gcresnet33ts.ra2_in1k)|256     |80.06|94.99|19.9       |4.8  |11.7 |3216   |\n|[resnet50_gn.a1h_in1k](https://huggingface.co/timm/resnet50_gn.a1h_in1k)|224     |80.06|94.95|25.6       |4.1  |11.1 |1109   |\n|[seresnet50.a1_in1k](https://huggingface.co/timm/seresnet50.a1_in1k)|224     |80.02|94.71|28.1       |4.1  |11.1 |2962   |\n|[resnet50.ram_in1k](https://huggingface.co/timm/resnet50.ram_in1k)|288     |79.97|95.05|25.6       |6.8  |18.4 |2086   |\n|[resnet152c.gluon_in1k](https://huggingface.co/timm/resnet152c.gluon_in1k)|224     |79.92|94.84|60.2       |11.8 |23.4 |1455   |\n|[seresnext50_32x4d.gluon_in1k](https://huggingface.co/timm/seresnext50_32x4d.gluon_in1k)|224     |79.91|94.82|27.6       |4.3  |14.4 |2591   |\n|[resnet50.d_in1k](https://huggingface.co/timm/resnet50.d_in1k)|224     |79.91|94.67|25.6       |4.1  |11.1 |3456   |\n|[resnet101.tv2_in1k](https://huggingface.co/timm/resnet101.tv2_in1k)|176     |79.9 |94.6 |44.6       |4.9  |10.1 |3341   |\n|[resnetrs50.tf_in1k](https://huggingface.co/timm/resnetrs50.tf_in1k)|224     |79.89|94.97|35.7       |4.5  |12.1 |2774   |\n|[resnet50.c2_in1k](https://huggingface.co/timm/resnet50.c2_in1k)|224     |79.88|94.87|25.6       |4.1  |11.1 |3455   |\n|[ecaresnet26t.ra2_in1k](https://huggingface.co/timm/ecaresnet26t.ra2_in1k)|320     |79.86|95.07|16.0       |5.2  |16.4 |2168   |\n|[resnet50.a2_in1k](https://huggingface.co/timm/resnet50.a2_in1k)|224     |79.85|94.56|25.6       |4.1  |11.1 |3460   |\n|[resnet50.ra_in1k](https://huggingface.co/timm/resnet50.ra_in1k)|288     |79.83|94.97|25.6       |6.8  |18.4 |2087   |\n|[resnet101.a3_in1k](https://huggingface.co/timm/resnet101.a3_in1k)|224     |79.82|94.62|44.6       |7.8  |16.2 |2114   |\n|[resnext50_32x4d.ra_in1k](https://huggingface.co/timm/resnext50_32x4d.ra_in1k)|224     |79.76|94.6 |25.0       |4.3  |14.4 |2943   |\n|[resnet50.c1_in1k](https://huggingface.co/timm/resnet50.c1_in1k)|224     |79.74|94.95|25.6       |4.1  |11.1 |3455   |\n|[ecaresnet50d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet50d_pruned.miil_in1k)|224     |79.74|94.87|19.9       |2.5  |6.4  |3929   |\n|[resnet33ts.ra2_in1k](https://huggingface.co/timm/resnet33ts.ra2_in1k)|288     |79.71|94.83|19.7       |6.0  |14.8 |2710   |\n|[resnet152.gluon_in1k](https://huggingface.co/timm/resnet152.gluon_in1k)|224     |79.68|94.74|60.2       |11.6 |22.6 |1486   |\n|[resnext50d_32x4d.bt_in1k](https://huggingface.co/timm/resnext50d_32x4d.bt_in1k)|224     |79.67|94.87|25.0       |4.5  |15.2 |2729   |\n|[resnet50.bt_in1k](https://huggingface.co/timm/resnet50.bt_in1k)|288     |79.63|94.91|25.6       |6.8  |18.4 |2086   |\n|[ecaresnet50t.a3_in1k](https://huggingface.co/timm/ecaresnet50t.a3_in1k)|224     |79.56|94.72|25.6       |4.3  |11.8 |2805   |\n|[resnet101c.gluon_in1k](https://huggingface.co/timm/resnet101c.gluon_in1k)|224     |79.53|94.58|44.6       |8.1  |17.0 |2062   |\n|[resnet50.b1k_in1k](https://huggingface.co/timm/resnet50.b1k_in1k)|224     |79.52|94.61|25.6       |4.1  |11.1 |3459   |\n|[resnet50.tv2_in1k](https://huggingface.co/timm/resnet50.tv2_in1k)|176     |79.42|94.64|25.6       |2.6  |6.9  |5397   |\n|[resnet32ts.ra2_in1k](https://huggingface.co/timm/resnet32ts.ra2_in1k)|288     |79.4 |94.66|18.0       |5.9  |14.6 |2752   |\n|[resnet50.b2k_in1k](https://huggingface.co/timm/resnet50.b2k_in1k)|224     |79.38|94.57|25.6       |4.1  |11.1 |3459   |\n|[resnext50_32x4d.tv2_in1k](https://huggingface.co/timm/resnext50_32x4d.tv2_in1k)|176     |79.37|94.3 |25.0       |2.7  |9.0  |4577   |\n|[resnext50_32x4d.gluon_in1k](https://huggingface.co/timm/resnext50_32x4d.gluon_in1k)|224     |79.36|94.43|25.0       |4.3  |14.4 |2942   |\n|[resnext101_32x8d.tv_in1k](https://huggingface.co/timm/resnext101_32x8d.tv_in1k)|224     |79.31|94.52|88.8       |16.5 |31.2 |1100   |\n|[resnet101.gluon_in1k](https://huggingface.co/timm/resnet101.gluon_in1k)|224     |79.31|94.53|44.6       |7.8  |16.2 |2125   |\n|[resnetblur50.bt_in1k](https://huggingface.co/timm/resnetblur50.bt_in1k)|224     |79.31|94.63|25.6       |5.2  |12.0 |2524   |\n|[resnet50.a1h_in1k](https://huggingface.co/timm/resnet50.a1h_in1k)|176     |79.27|94.49|25.6       |2.6  |6.9  |5404   |\n|[resnext50_32x4d.a3_in1k](https://huggingface.co/timm/resnext50_32x4d.a3_in1k)|224     |79.25|94.31|25.0       |4.3  |14.4 |2931   |\n|[resnet50.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnet50.fb_ssl_yfcc100m_ft_in1k)|224     |79.22|94.84|25.6       |4.1  |11.1 |3451   |\n|[resnet33ts.ra2_in1k](https://huggingface.co/timm/resnet33ts.ra2_in1k)|256     |79.21|94.56|19.7       |4.8  |11.7 |3392   |\n|[resnet50d.gluon_in1k](https://huggingface.co/timm/resnet50d.gluon_in1k)|224     |79.07|94.48|25.6       |4.4  |11.9 |3162   |\n|[resnet50.ram_in1k](https://huggingface.co/timm/resnet50.ram_in1k)|224     |79.03|94.38|25.6       |4.1  |11.1 |3453   |\n|[resnet50.am_in1k](https://huggingface.co/timm/resnet50.am_in1k)|224     |79.01|94.39|25.6       |4.1  |11.1 |3461   |\n|[resnet32ts.ra2_in1k](https://huggingface.co/timm/resnet32ts.ra2_in1k)|256     |79.01|94.37|18.0       |4.6  |11.6 |3440   |\n|[ecaresnet26t.ra2_in1k](https://huggingface.co/timm/ecaresnet26t.ra2_in1k)|256     |78.9 |94.54|16.0       |3.4  |10.5 |3421   |\n|[resnet152.a3_in1k](https://huggingface.co/timm/resnet152.a3_in1k)|160     |78.89|94.11|60.2       |5.9  |11.5 |2745   |\n|[wide_resnet101_2.tv_in1k](https://huggingface.co/timm/wide_resnet101_2.tv_in1k)|224     |78.84|94.28|126.9      |22.8 |21.2 |1079   |\n|[seresnext26d_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26d_32x4d.bt_in1k)|288     |78.83|94.24|16.8       |4.5  |16.8 |2251   |\n|[resnet50.ra_in1k](https://huggingface.co/timm/resnet50.ra_in1k)|224     |78.81|94.32|25.6       |4.1  |11.1 |3454   |\n|[seresnext26t_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26t_32x4d.bt_in1k)|288     |78.74|94.33|16.8       |4.5  |16.7 |2264   |\n|[resnet50s.gluon_in1k](https://huggingface.co/timm/resnet50s.gluon_in1k)|224     |78.72|94.23|25.7       |5.5  |13.5 |2796   |\n|[resnet50d.a3_in1k](https://huggingface.co/timm/resnet50d.a3_in1k)|224     |78.71|94.24|25.6       |4.4  |11.9 |3154   |\n|[wide_resnet50_2.tv_in1k](https://huggingface.co/timm/wide_resnet50_2.tv_in1k)|224     |78.47|94.09|68.9       |11.4 |14.4 |1934   |\n|[resnet50.bt_in1k](https://huggingface.co/timm/resnet50.bt_in1k)|224     |78.46|94.27|25.6       |4.1  |11.1 |3454   |\n|[resnet34d.ra2_in1k](https://huggingface.co/timm/resnet34d.ra2_in1k)|288     |78.43|94.35|21.8       |6.5  |7.5  |3291   |\n|[gcresnext26ts.ch_in1k](https://huggingface.co/timm/gcresnext26ts.ch_in1k)|288     |78.42|94.04|10.5       |3.1  |13.3 |3226   |\n|[resnet26t.ra2_in1k](https://huggingface.co/timm/resnet26t.ra2_in1k)|320     |78.33|94.13|16.0       |5.2  |16.4 |2391   |\n|[resnet152.tv_in1k](https://huggingface.co/timm/resnet152.tv_in1k)|224     |78.32|94.04|60.2       |11.6 |22.6 |1487   |\n|[seresnext26ts.ch_in1k](https://huggingface.co/timm/seresnext26ts.ch_in1k)|288     |78.28|94.1 |10.4       |3.1  |13.3 |3062   |\n|[bat_resnext26ts.ch_in1k](https://huggingface.co/timm/bat_resnext26ts.ch_in1k)|256     |78.25|94.1 |10.7       |2.5  |12.5 |3393   |\n|[resnet50.a3_in1k](https://huggingface.co/timm/resnet50.a3_in1k)|224     |78.06|93.78|25.6       |4.1  |11.1 |3450   |\n|[resnet50c.gluon_in1k](https://huggingface.co/timm/resnet50c.gluon_in1k)|224     |78.0 |93.99|25.6       |4.4  |11.9 |3286   |\n|[eca_resnext26ts.ch_in1k](https://huggingface.co/timm/eca_resnext26ts.ch_in1k)|288     |78.0 |93.91|10.3       |3.1  |13.3 |3297   |\n|[seresnext26t_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26t_32x4d.bt_in1k)|224     |77.98|93.75|16.8       |2.7  |10.1 |3841   |\n|[resnet34.a1_in1k](https://huggingface.co/timm/resnet34.a1_in1k)|288     |77.92|93.77|21.8       |6.1  |6.2  |3609   |\n|[resnet101.a3_in1k](https://huggingface.co/timm/resnet101.a3_in1k)|160     |77.88|93.71|44.6       |4.0  |8.3  |3926   |\n|[resnet26t.ra2_in1k](https://huggingface.co/timm/resnet26t.ra2_in1k)|256     |77.87|93.84|16.0       |3.4  |10.5 |3772   |\n|[seresnext26ts.ch_in1k](https://huggingface.co/timm/seresnext26ts.ch_in1k)|256     |77.86|93.79|10.4       |2.4  |10.5 |4263   |\n|[resnetrs50.tf_in1k](https://huggingface.co/timm/resnetrs50.tf_in1k)|160     |77.82|93.81|35.7       |2.3  |6.2  |5238   |\n|[gcresnext26ts.ch_in1k](https://huggingface.co/timm/gcresnext26ts.ch_in1k)|256     |77.81|93.82|10.5       |2.4  |10.5 |4183   |\n|[ecaresnet50t.a3_in1k](https://huggingface.co/timm/ecaresnet50t.a3_in1k)|160     |77.79|93.6 |25.6       |2.2  |6.0  |5329   |\n|[resnext50_32x4d.a3_in1k](https://huggingface.co/timm/resnext50_32x4d.a3_in1k)|160     |77.73|93.32|25.0       |2.2  |7.4  |5576   |\n|[resnext50_32x4d.tv_in1k](https://huggingface.co/timm/resnext50_32x4d.tv_in1k)|224     |77.61|93.7 |25.0       |4.3  |14.4 |2944   |\n|[seresnext26d_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26d_32x4d.bt_in1k)|224     |77.59|93.61|16.8       |2.7  |10.2 |3807   |\n|[resnet50.gluon_in1k](https://huggingface.co/timm/resnet50.gluon_in1k)|224     |77.58|93.72|25.6       |4.1  |11.1 |3455   |\n|[eca_resnext26ts.ch_in1k](https://huggingface.co/timm/eca_resnext26ts.ch_in1k)|256     |77.44|93.56|10.3       |2.4  |10.5 |4284   |\n|[resnet26d.bt_in1k](https://huggingface.co/timm/resnet26d.bt_in1k)|288     |77.41|93.63|16.0       |4.3  |13.5 |2907   |\n|[resnet101.tv_in1k](https://huggingface.co/timm/resnet101.tv_in1k)|224     |77.38|93.54|44.6       |7.8  |16.2 |2125   |\n|[resnet50d.a3_in1k](https://huggingface.co/timm/resnet50d.a3_in1k)|160     |77.22|93.27|25.6       |2.2  |6.1  |5982   |\n|[resnext26ts.ra2_in1k](https://huggingface.co/timm/resnext26ts.ra2_in1k)|288     |77.17|93.47|10.3       |3.1  |13.3 |3392   |\n|[resnet34.a2_in1k](https://huggingface.co/timm/resnet34.a2_in1k)|288     |77.15|93.27|21.8       |6.1  |6.2  |3615   |\n|[resnet34d.ra2_in1k](https://huggingface.co/timm/resnet34d.ra2_in1k)|224     |77.1 |93.37|21.8       |3.9  |4.5  |5436   |\n|[seresnet50.a3_in1k](https://huggingface.co/timm/seresnet50.a3_in1k)|224     |77.02|93.07|28.1       |4.1  |11.1 |2952   |\n|[resnext26ts.ra2_in1k](https://huggingface.co/timm/resnext26ts.ra2_in1k)|256     |76.78|93.13|10.3       |2.4  |10.5 |4410   |\n|[resnet26d.bt_in1k](https://huggingface.co/timm/resnet26d.bt_in1k)|224     |76.7 |93.17|16.0       |2.6  |8.2  |4859   |\n|[resnet34.bt_in1k](https://huggingface.co/timm/resnet34.bt_in1k)|288     |76.5 |93.35|21.8       |6.1  |6.2  |3617   |\n|[resnet34.a1_in1k](https://huggingface.co/timm/resnet34.a1_in1k)|224     |76.42|92.87|21.8       |3.7  |3.7  |5984   |\n|[resnet26.bt_in1k](https://huggingface.co/timm/resnet26.bt_in1k)|288     |76.35|93.18|16.0       |3.9  |12.2 |3331   |\n|[resnet50.tv_in1k](https://huggingface.co/timm/resnet50.tv_in1k)|224     |76.13|92.86|25.6       |4.1  |11.1 |3457   |\n|[resnet50.a3_in1k](https://huggingface.co/timm/resnet50.a3_in1k)|160     |75.96|92.5 |25.6       |2.1  |5.7  |6490   |\n|[resnet34.a2_in1k](https://huggingface.co/timm/resnet34.a2_in1k)|224     |75.52|92.44|21.8       |3.7  |3.7  |5991   |\n|[resnet26.bt_in1k](https://huggingface.co/timm/resnet26.bt_in1k)|224     |75.3 |92.58|16.0       |2.4  |7.4  |5583   |\n|[resnet34.bt_in1k](https://huggingface.co/timm/resnet34.bt_in1k)|224     |75.16|92.18|21.8       |3.7  |3.7  |5994   |\n|[seresnet50.a3_in1k](https://huggingface.co/timm/seresnet50.a3_in1k)|160     |75.1 |92.08|28.1       |2.1  |5.7  |5513   |\n|[resnet34.gluon_in1k](https://huggingface.co/timm/resnet34.gluon_in1k)|224     |74.57|91.98|21.8       |3.7  |3.7  |5984   |\n|[resnet18d.ra2_in1k](https://huggingface.co/timm/resnet18d.ra2_in1k)|288     |73.81|91.83|11.7       |3.4  |5.4  |5196   |\n|[resnet34.tv_in1k](https://huggingface.co/timm/resnet34.tv_in1k)|224     |73.32|91.42|21.8       |3.7  |3.7  |5979   |\n|[resnet18.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnet18.fb_swsl_ig1b_ft_in1k)|224     |73.28|91.73|11.7       |1.8  |2.5  |10213  |\n|[resnet18.a1_in1k](https://huggingface.co/timm/resnet18.a1_in1k)|288     |73.16|91.03|11.7       |3.0  |4.1  |6050   |\n|[resnet34.a3_in1k](https://huggingface.co/timm/resnet34.a3_in1k)|224     |72.98|91.11|21.8       |3.7  |3.7  |5967   |\n|[resnet18.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnet18.fb_ssl_yfcc100m_ft_in1k)|224     |72.6 |91.42|11.7       |1.8  |2.5  |10213  |\n|[resnet18.a2_in1k](https://huggingface.co/timm/resnet18.a2_in1k)|288     |72.37|90.59|11.7       |3.0  |4.1  |6051   |\n|[resnet14t.c3_in1k](https://huggingface.co/timm/resnet14t.c3_in1k)|224     |72.26|90.31|10.1       |1.7  |5.8  |7026   |\n|[resnet18d.ra2_in1k](https://huggingface.co/timm/resnet18d.ra2_in1k)|224     |72.26|90.68|11.7       |2.1  |3.3  |8707   |\n|[resnet18.a1_in1k](https://huggingface.co/timm/resnet18.a1_in1k)|224     |71.49|90.07|11.7       |1.8  |2.5  |10187  |\n|[resnet14t.c3_in1k](https://huggingface.co/timm/resnet14t.c3_in1k)|176     |71.31|89.69|10.1       |1.1  |3.6  |10970  |\n|[resnet18.gluon_in1k](https://huggingface.co/timm/resnet18.gluon_in1k)|224     |70.84|89.76|11.7       |1.8  |2.5  |10210  |\n|[resnet18.a2_in1k](https://huggingface.co/timm/resnet18.a2_in1k)|224     |70.64|89.47|11.7       |1.8  |2.5  |10194  |\n|[resnet34.a3_in1k](https://huggingface.co/timm/resnet34.a3_in1k)|160     |70.56|89.52|21.8       |1.9  |1.9  |10737  |\n|[resnet18.tv_in1k](https://huggingface.co/timm/resnet18.tv_in1k)|224     |69.76|89.07|11.7       |1.8  |2.5  |10205  |\n|[resnet10t.c3_in1k](https://huggingface.co/timm/resnet10t.c3_in1k)|224     |68.34|88.03|5.4        |1.1  |2.4  |13079  |\n|[resnet18.a3_in1k](https://huggingface.co/timm/resnet18.a3_in1k)|224     |68.25|88.17|11.7       |1.8  |2.5  |10167  |\n|[resnet10t.c3_in1k](https://huggingface.co/timm/resnet10t.c3_in1k)|176     |66.71|86.96|5.4        |0.7  |1.5  |20327  |\n|[resnet18.a3_in1k](https://huggingface.co/timm/resnet18.a3_in1k)|160     |65.66|86.26|11.7       |0.9  |1.3  |18229  |\n\n## Citation\n```bibtex\n@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}\n```\n```bibtex\n@article{He2015,\n  author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},\n  title = {Deep Residual Learning for Image Recognition},\n  journal = {arXiv preprint arXiv:1512.03385},\n  year = {2015}\n}\n```\n",
                  "extracted_code": "from urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model('resnet50.ram_in1k', pretrained=True)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\n\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnet50.ram_in1k',\n    pretrained=True,\n    features_only=True,\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\nfor o in output:\n    # print shape of each feature map in output\n    # e.g.:\n    #  torch.Size([1, 64, 112, 112])\n    #  torch.Size([1, 256, 56, 56])\n    #  torch.Size([1, 512, 28, 28])\n    #  torch.Size([1, 1024, 14, 14])\n    #  torch.Size([1, 2048, 7, 7])\n\n    print(o.shape)\n\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnet50.ram_in1k',\n    pretrained=True,\n    num_classes=0,  # remove classifier nn.Linear\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n\n# or equivalently (without needing to set num_classes=0)\n\noutput = model.forward_features(transforms(img).unsqueeze(0))\n# output is unpooled, a (1, 2048, 7, 7) shaped tensor\n\noutput = model.forward_head(output, pre_logits=True)\n# output is a (1, num_features) shaped tensor"
                }
              ],
              "datasets": [
                {
                  "id": "benjamin-paine/imagenet-1k-256x256",
                  "author": "benjamin-paine",
                  "sha": "1bd0400450249a7fe90c0aece37d0d03e7ea956a",
                  "created_at": "2024-09-13T13:50:22+00:00",
                  "last_modified": "2024-09-15T11:15:04+00:00",
                  "private": false,
                  "gated": false,
                  "disabled": false,
                  "downloads": 8693,
                  "likes": 12,
                  "siblings": [
                    {
                      "rfilename": ".gitattributes"
                    },
                    {
                      "rfilename": "LICENSE"
                    },
                    {
                      "rfilename": "README.md"
                    },
                    {
                      "rfilename": "data/test-00000-of-00003.parquet"
                    },
                    {
                      "rfilename": "data/test-00001-of-00003.parquet"
                    },
                    {
                      "rfilename": "data/test-00002-of-00003.parquet"
                    },
                    {
                      "rfilename": "data/train-00000-of-00040.parquet"
                    },
                    {
                      "rfilename": "data/train-00001-of-00040.parquet"
                    },
                    {
                      "rfilename": "data/train-00002-of-00040.parquet"
                    },
                    {
                      "rfilename": "data/train-00003-of-00040.parquet"
                    }
                  ],
                  "card_data": {
                    "license": [
                      "other"
                    ],
                    "language": [
                      "en"
                    ],
                    "tags": [
                      "imagenet",
                      "imagenet-1k",
                      "ilsvrc-2012"
                    ],
                    "datasets": [],
                    "task_categories": [
                      "image-classification"
                    ],
                    "size_categories": [
                      "1M<n<10M"
                    ],
                    "metrics": [],
                    "widget": []
                  },
                  "tags": [
                    "task_categories:image-classification",
                    "task_ids:multi-class-image-classification",
                    "annotations_creators:crowdsourced",
                    "language_creators:crowdsourced",
                    "multilinguality:monolingual",
                    "source_datasets:original",
                    "language:en",
                    "license:other",
                    "size_categories:1M<n<10M",
                    "format:parquet",
                    "modality:image",
                    "library:datasets",
                    "library:dask",
                    "library:mlcroissant",
                    "library:polars",
                    "arxiv:1409.0575",
                    "arxiv:1912.07726",
                    "arxiv:1811.12231",
                    "arxiv:2109.13228",
                    "region:us",
                    "imagenet",
                    "imagenet-1k",
                    "ilsvrc-2012"
                  ],
                  "readme": "---\nannotations_creators:\n- crowdsourced\nlanguage_creators:\n- crowdsourced\nlanguage:\n- en\nlicense:\n- other\nmultilinguality:\n- monolingual\nsize_categories:\n- 1M<n<10M\nsource_datasets:\n- original\ntask_categories:\n- image-classification\ntask_ids:\n- multi-class-image-classification\npaperswithcode_id: imagenet-1k-1\npretty_name: ImageNet\nlicense_details: imagenet-agreement\nextra_gated_prompt: 'By clicking on “Access repository” below, you also agree to ImageNet\n  Terms of Access:\n\n  [RESEARCHER_FULLNAME] (the \"Researcher\") has requested permission to use the ImageNet\n  database (the \"Database\") at Princeton University and Stanford University. In exchange\n  for such permission, Researcher hereby agrees to the following terms and conditions:\n\n  1. Researcher shall use the Database only for non-commercial research and educational\n  purposes.\n\n  2. Princeton University, Stanford University and Hugging Face make no representations\n  or warranties regarding the Database, including but not limited to warranties of\n  non-infringement or fitness for a particular purpose.\n\n  3. Researcher accepts full responsibility for his or her use of the Database and\n  shall defend and indemnify the ImageNet team, Princeton University, Stanford University\n  and Hugging Face, including their employees, Trustees, officers and agents, against\n  any and all claims arising from Researcher''s use of the Database, including but\n  not limited to Researcher''s use of any copies of copyrighted images that he or\n  she may create from the Database.\n\n  4. Researcher may provide research associates and colleagues with access to the\n  Database provided that they first agree to be bound by these terms and conditions.\n\n  5. Princeton University, Stanford University and Hugging Face reserve the right\n  to terminate Researcher''s access to the Database at any time.\n\n  6. If Researcher is employed by a for-profit, commercial entity, Researcher''s employer\n  shall also be bound by these terms and conditions, and Researcher hereby represents\n  that he or she is fully authorized to enter into this agreement on behalf of such\n  employer.\n\n  7. The law of the State of New Jersey shall apply to all disputes under this agreement.'\ntags:\n- imagenet\n- imagenet-1k\n- ilsvrc-2012\ndataset_info:\n  features:\n  - name: image\n    dtype: image\n  - name: label\n    dtype:\n      class_label:\n        names:\n          '0': tench, Tinca tinca\n          '1': goldfish, Carassius auratus\n          '2': great white shark, white shark, man-eater, man-eating shark, Carcharodon\n            carcharias\n          '3': tiger shark, Galeocerdo cuvieri\n          '4': hammerhead, hammerhead shark\n          '5': electric ray, crampfish, numbfish, torpedo\n          '6': stingray\n          '7': cock\n          '8': hen\n          '9': ostrich, Struthio camelus\n          '10': brambling, Fringilla montifringilla\n          '11': goldfinch, Carduelis carduelis\n          '12': house finch, linnet, Carpodacus mexicanus\n          '13': junco, snowbird\n          '14': indigo bunting, indigo finch, indigo bird, Passerina cyanea\n          '15': robin, American robin, Turdus migratorius\n          '16': bulbul\n          '17': jay\n          '18': magpie\n          '19': chickadee\n          '20': water ouzel, dipper\n          '21': kite\n          '22': bald eagle, American eagle, Haliaeetus leucocephalus\n          '23': vulture\n          '24': great grey owl, great gray owl, Strix nebulosa\n          '25': European fire salamander, Salamandra salamandra\n          '26': common newt, Triturus vulgaris\n          '27': eft\n          '28': spotted salamander, Ambystoma maculatum\n          '29': axolotl, mud puppy, Ambystoma mexicanum\n          '30': bullfrog, Rana catesbeiana\n          '31': tree frog, tree-frog\n          '32': tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui\n          '33': loggerhead, loggerhead turtle, Caretta caretta\n          '34': leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea\n          '35': mud turtle\n          '36': terrapin\n          '37': box turtle, box tortoise\n          '38': banded gecko\n          '39': common iguana, iguana, Iguana iguana\n          '40': American chameleon, anole, Anolis carolinensis\n          '41': whiptail, whiptail lizard\n          '42': agama\n          '43': frilled lizard, Chlamydosaurus kingi\n          '44': alligator lizard\n          '45': Gila monster, Heloderma suspectum\n          '46': green lizard, Lacerta viridis\n          '47': African chameleon, Chamaeleo chamaeleon\n          '48': Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus\n            komodoensis\n          '49': African crocodile, Nile crocodile, Crocodylus niloticus\n          '50': American alligator, Alligator mississipiensis\n          '51': triceratops\n          '52': thunder snake, worm snake, Carphophis amoenus\n          '53': ringneck snake, ring-necked snake, ring snake\n          '54': hognose snake, puff adder, sand viper\n          '55': green snake, grass snake\n          '56': king snake, kingsnake\n          '57': garter snake, grass snake\n          '58': water snake\n          '59': vine snake\n          '60': night snake, Hypsiglena torquata\n          '61': boa constrictor, Constrictor constrictor\n          '62': rock python, rock snake, Python sebae\n          '63': Indian cobra, Naja naja\n          '64': green mamba\n          '65': sea snake\n          '66': horned viper, cerastes, sand viper, horned asp, Cerastes cornutus\n          '67': diamondback, diamondback rattlesnake, Crotalus adamanteus\n          '68': sidewinder, horned rattlesnake, Crotalus cerastes\n          '69': trilobite\n          '70': harvestman, daddy longlegs, Phalangium opilio\n          '71': scorpion\n          '72': black and gold garden spider, Argiope aurantia\n          '73': barn spider, Araneus cavaticus\n          '74': garden spider, Aranea diademata\n          '75': black widow, Latrodectus mactans\n          '76': tarantula\n          '77': wolf spider, hunting spider\n          '78': tick\n          '79': centipede\n          '80': black grouse\n          '81': ptarmigan\n          '82': ruffed grouse, partridge, Bonasa umbellus\n          '83': prairie chicken, prairie grouse, prairie fowl\n          '84': peacock\n          '85': quail\n          '86': partridge\n          '87': African grey, African gray, Psittacus erithacus\n          '88': macaw\n          '89': sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita\n          '90': lorikeet\n          '91': coucal\n          '92': bee eater\n          '93': hornbill\n          '94': hummingbird\n          '95': jacamar\n          '96': toucan\n          '97': drake\n          '98': red-breasted merganser, Mergus serrator\n          '99': goose\n          '100': black swan, Cygnus atratus\n          '101': tusker\n          '102': echidna, spiny anteater, anteater\n          '103': platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus\n            anatinus\n          '104': wallaby, brush kangaroo\n          '105': koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus\n          '106': wombat\n          '107': jellyfish\n          '108': sea anemone, anemone\n          '109': brain coral\n          '110': flatworm, platyhelminth\n          '111': nematode, nematode worm, roundworm\n          '112': conch\n          '113': snail\n          '114': slug\n          '115': sea slug, nudibranch\n          '116': chiton, coat-of-mail shell, sea cradle, polyplacophore\n          '117': chambered nautilus, pearly nautilus, nautilus\n          '118': Dungeness crab, Cancer magister\n          '119': rock crab, Cancer irroratus\n          '120': fiddler crab\n          '121': king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes\n            camtschatica\n          '122': American lobster, Northern lobster, Maine lobster, Homarus americanus\n          '123': spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish\n          '124': crayfish, crawfish, crawdad, crawdaddy\n          '125': hermit crab\n          '126': isopod\n          '127': white stork, Ciconia ciconia\n          '128': black stork, Ciconia nigra\n          '129': spoonbill\n          '130': flamingo\n          '131': little blue heron, Egretta caerulea\n          '132': American egret, great white heron, Egretta albus\n          '133': bittern\n          '134': crane\n          '135': limpkin, Aramus pictus\n          '136': European gallinule, Porphyrio porphyrio\n          '137': American coot, marsh hen, mud hen, water hen, Fulica americana\n          '138': bustard\n          '139': ruddy turnstone, Arenaria interpres\n          '140': red-backed sandpiper, dunlin, Erolia alpina\n          '141': redshank, Tringa totanus\n          '142': dowitcher\n          '143': oystercatcher, oyster catcher\n          '144': pelican\n          '145': king penguin, Aptenodytes patagonica\n          '146': albatross, mollymawk\n          '147': grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius\n            robustus\n          '148': killer whale, killer, orca, grampus, sea wolf, Orcinus orca\n          '149': dugong, Dugong dugon\n          '150': sea lion\n          '151': Chihuahua\n          '152': Japanese spaniel\n          '153': Maltese dog, Maltese terrier, Maltese\n          '154': Pekinese, Pekingese, Peke\n          '155': Shih-Tzu\n          '156': Blenheim spaniel\n          '157': papillon\n          '158': toy terrier\n          '159': Rhodesian ridgeback\n          '160': Afghan hound, Afghan\n          '161': basset, basset hound\n          '162': beagle\n          '163': bloodhound, sleuthhound\n          '164': bluetick\n          '165': black-and-tan coonhound\n          '166': Walker hound, Walker foxhound\n          '167': English foxhound\n          '168': redbone\n          '169': borzoi, Russian wolfhound\n          '170': Irish wolfhound\n          '171': Italian greyhound\n          '172': whippet\n          '173': Ibizan hound, Ibizan Podenco\n          '174': Norwegian elkhound, elkhound\n          '175': otterhound, otter hound\n          '176': Saluki, gazelle hound\n          '177': Scottish deerhound, deerhound\n          '178': Weimaraner\n          '179': Staffordshire bullterrier, Staffordshire bull terrier\n          '180': American Staffordshire terrier, Staffordshire terrier, American pit\n            bull terrier, pit bull terrier\n          '181': Bedlington terrier\n          '182': Border terrier\n          '183': Kerry blue terrier\n          '184': Irish terrier\n          '185': Norfolk terrier\n          '186': Norwich terrier\n          '187': Yorkshire terrier\n          '188': wire-haired fox terrier\n          '189': Lakeland terrier\n          '190': Sealyham terrier, Sealyham\n          '191': Airedale, Airedale terrier\n          '192': cairn, cairn terrier\n          '193': Australian terrier\n          '194': Dandie Dinmont, Dandie Dinmont terrier\n          '195': Boston bull, Boston terrier\n          '196': miniature schnauzer\n          '197': giant schnauzer\n          '198': standard schnauzer\n          '199': Scotch terrier, Scottish terrier, Scottie\n          '200': Tibetan terrier, chrysanthemum dog\n          '201': silky terrier, Sydney silky\n          '202': soft-coated wheaten terrier\n          '203': West Highland white terrier\n          '204': Lhasa, Lhasa apso\n          '205': flat-coated retriever\n          '206': curly-coated retriever\n          '207': golden retriever\n          '208': Labrador retriever\n          '209': Chesapeake Bay retriever\n          '210': German short-haired pointer\n          '211': vizsla, Hungarian pointer\n          '212': English setter\n          '213': Irish setter, red setter\n          '214': Gordon setter\n          '215': Brittany spaniel\n          '216': clumber, clumber spaniel\n          '217': English springer, English springer spaniel\n          '218': Welsh springer spaniel\n          '219': cocker spaniel, English cocker spaniel, cocker\n          '220': Sussex spaniel\n          '221': Irish water spaniel\n          '222': kuvasz\n          '223': schipperke\n          '224': groenendael\n          '225': malinois\n          '226': briard\n          '227': kelpie\n          '228': komondor\n          '229': Old English sheepdog, bobtail\n          '230': Shetland sheepdog, Shetland sheep dog, Shetland\n          '231': collie\n          '232': Border collie\n          '233': Bouvier des Flandres, Bouviers des Flandres\n          '234': Rottweiler\n          '235': German shepherd, German shepherd dog, German police dog, alsatian\n          '236': Doberman, Doberman pinscher\n          '237': miniature pinscher\n          '238': Greater Swiss Mountain dog\n          '239': Bernese mountain dog\n          '240': Appenzeller\n          '241': EntleBucher\n          '242': boxer\n          '243': bull mastiff\n          '244': Tibetan mastiff\n          '245': French bulldog\n          '246': Great Dane\n          '247': Saint Bernard, St Bernard\n          '248': Eskimo dog, husky\n          '249': malamute, malemute, Alaskan malamute\n          '250': Siberian husky\n          '251': dalmatian, coach dog, carriage dog\n          '252': affenpinscher, monkey pinscher, monkey dog\n          '253': basenji\n          '254': pug, pug-dog\n          '255': Leonberg\n          '256': Newfoundland, Newfoundland dog\n          '257': Great Pyrenees\n          '258': Samoyed, Samoyede\n          '259': Pomeranian\n          '260': chow, chow chow\n          '261': keeshond\n          '262': Brabancon griffon\n          '263': Pembroke, Pembroke Welsh corgi\n          '264': Cardigan, Cardigan Welsh corgi\n          '265': toy poodle\n          '266': miniature poodle\n          '267': standard poodle\n          '268': Mexican hairless\n          '269': timber wolf, grey wolf, gray wolf, Canis lupus\n          '270': white wolf, Arctic wolf, Canis lupus tundrarum\n          '271': red wolf, maned wolf, Canis rufus, Canis niger\n          '272': coyote, prairie wolf, brush wolf, Canis latrans\n          '273': dingo, warrigal, warragal, Canis dingo\n          '274': dhole, Cuon alpinus\n          '275': African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus\n          '276': hyena, hyaena\n          '277': red fox, Vulpes vulpes\n          '278': kit fox, Vulpes macrotis\n          '279': Arctic fox, white fox, Alopex lagopus\n          '280': grey fox, gray fox, Urocyon cinereoargenteus\n          '281': tabby, tabby cat\n          '282': tiger cat\n          '283': Persian cat\n          '284': Siamese cat, Siamese\n          '285': Egyptian cat\n          '286': cougar, puma, catamount, mountain lion, painter, panther, Felis concolor\n          '287': lynx, catamount\n          '288': leopard, Panthera pardus\n          '289': snow leopard, ounce, Panthera uncia\n          '290': jaguar, panther, Panthera onca, Felis onca\n          '291': lion, king of beasts, Panthera leo\n          '292': tiger, Panthera tigris\n          '293': cheetah, chetah, Acinonyx jubatus\n          '294': brown bear, bruin, Ursus arctos\n          '295': American black bear, black bear, Ursus americanus, Euarctos americanus\n          '296': ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus\n          '297': sloth bear, Melursus ursinus, Ursus ursinus\n          '298': mongoose\n          '299': meerkat, mierkat\n          '300': tiger beetle\n          '301': ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle\n          '302': ground beetle, carabid beetle\n          '303': long-horned beetle, longicorn, longicorn beetle\n          '304': leaf beetle, chrysomelid\n          '305': dung beetle\n          '306': rhinoceros beetle\n          '307': weevil\n          '308': fly\n          '309': bee\n          '310': ant, emmet, pismire\n          '311': grasshopper, hopper\n          '312': cricket\n          '313': walking stick, walkingstick, stick insect\n          '314': cockroach, roach\n          '315': mantis, mantid\n          '316': cicada, cicala\n          '317': leafhopper\n          '318': lacewing, lacewing fly\n          '319': dragonfly, darning needle, devil's darning needle, sewing needle,\n            snake feeder, snake doctor, mosquito hawk, skeeter hawk\n          '320': damselfly\n          '321': admiral\n          '322': ringlet, ringlet butterfly\n          '323': monarch, monarch butterfly, milkweed butterfly, Danaus plexippus\n          '324': cabbage butterfly\n          '325': sulphur butterfly, sulfur butterfly\n          '326': lycaenid, lycaenid butterfly\n          '327': starfish, sea star\n          '328': sea urchin\n          '329': sea cucumber, holothurian\n          '330': wood rabbit, cottontail, cottontail rabbit\n          '331': hare\n          '332': Angora, Angora rabbit\n          '333': hamster\n          '334': porcupine, hedgehog\n          '335': fox squirrel, eastern fox squirrel, Sciurus niger\n          '336': marmot\n          '337': beaver\n          '338': guinea pig, Cavia cobaya\n          '339': sorrel\n          '340': zebra\n          '341': hog, pig, grunter, squealer, Sus scrofa\n          '342': wild boar, boar, Sus scrofa\n          '343': warthog\n          '344': hippopotamus, hippo, river horse, Hippopotamus amphibius\n          '345': ox\n          '346': water buffalo, water ox, Asiatic buffalo, Bubalus bubalis\n          '347': bison\n          '348': ram, tup\n          '349': bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain\n            sheep, Ovis canadensis\n          '350': ibex, Capra ibex\n          '351': hartebeest\n          '352': impala, Aepyceros melampus\n          '353': gazelle\n          '354': Arabian camel, dromedary, Camelus dromedarius\n          '355': llama\n          '356': weasel\n          '357': mink\n          '358': polecat, fitch, foulmart, foumart, Mustela putorius\n          '359': black-footed ferret, ferret, Mustela nigripes\n          '360': otter\n          '361': skunk, polecat, wood pussy\n          '362': badger\n          '363': armadillo\n          '364': three-toed sloth, ai, Bradypus tridactylus\n          '365': orangutan, orang, orangutang, Pongo pygmaeus\n          '366': gorilla, Gorilla gorilla\n          '367': chimpanzee, chimp, Pan troglodytes\n          '368': gibbon, Hylobates lar\n          '369': siamang, Hylobates syndactylus, Symphalangus syndactylus\n          '370': guenon, guenon monkey\n          '371': patas, hussar monkey, Erythrocebus patas\n          '372': baboon\n          '373': macaque\n          '374': langur\n          '375': colobus, colobus monkey\n          '376': proboscis monkey, Nasalis larvatus\n          '377': marmoset\n          '378': capuchin, ringtail, Cebus capucinus\n          '379': howler monkey, howler\n          '380': titi, titi monkey\n          '381': spider monkey, Ateles geoffroyi\n          '382': squirrel monkey, Saimiri sciureus\n          '383': Madagascar cat, ring-tailed lemur, Lemur catta\n          '384': indri, indris, Indri indri, Indri brevicaudatus\n          '385': Indian elephant, Elephas maximus\n          '386': African elephant, Loxodonta africana\n          '387': lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens\n          '388': giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca\n          '389': barracouta, snoek\n          '390': eel\n          '391': coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus\n            kisutch\n          '392': rock beauty, Holocanthus tricolor\n          '393': anemone fish\n          '394': sturgeon\n          '395': gar, garfish, garpike, billfish, Lepisosteus osseus\n          '396': lionfish\n          '397': puffer, pufferfish, blowfish, globefish\n          '398': abacus\n          '399': abaya\n          '400': academic gown, academic robe, judge's robe\n          '401': accordion, piano accordion, squeeze box\n          '402': acoustic guitar\n          '403': aircraft carrier, carrier, flattop, attack aircraft carrier\n          '404': airliner\n          '405': airship, dirigible\n          '406': altar\n          '407': ambulance\n          '408': amphibian, amphibious vehicle\n          '409': analog clock\n          '410': apiary, bee house\n          '411': apron\n          '412': ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin,\n            dustbin, trash barrel, trash bin\n          '413': assault rifle, assault gun\n          '414': backpack, back pack, knapsack, packsack, rucksack, haversack\n          '415': bakery, bakeshop, bakehouse\n          '416': balance beam, beam\n          '417': balloon\n          '418': ballpoint, ballpoint pen, ballpen, Biro\n          '419': Band Aid\n          '420': banjo\n          '421': bannister, banister, balustrade, balusters, handrail\n          '422': barbell\n          '423': barber chair\n          '424': barbershop\n          '425': barn\n          '426': barometer\n          '427': barrel, cask\n          '428': barrow, garden cart, lawn cart, wheelbarrow\n          '429': baseball\n          '430': basketball\n          '431': bassinet\n          '432': bassoon\n          '433': bathing cap, swimming cap\n          '434': bath towel\n          '435': bathtub, bathing tub, bath, tub\n          '436': beach wagon, station wagon, wagon, estate car, beach waggon, station\n            waggon, waggon\n          '437': beacon, lighthouse, beacon light, pharos\n          '438': beaker\n          '439': bearskin, busby, shako\n          '440': beer bottle\n          '441': beer glass\n          '442': bell cote, bell cot\n          '443': bib\n          '444': bicycle-built-for-two, tandem bicycle, tandem\n          '445': bikini, two-piece\n          '446': binder, ring-binder\n          '447': binoculars, field glasses, opera glasses\n          '448': birdhouse\n          '449': boathouse\n          '450': bobsled, bobsleigh, bob\n          '451': bolo tie, bolo, bola tie, bola\n          '452': bonnet, poke bonnet\n          '453': bookcase\n          '454': bookshop, bookstore, bookstall\n          '455': bottlecap\n          '456': bow\n          '457': bow tie, bow-tie, bowtie\n          '458': brass, memorial tablet, plaque\n          '459': brassiere, bra, bandeau\n          '460': breakwater, groin, groyne, mole, bulwark, seawall, jetty\n          '461': breastplate, aegis, egis\n          '462': broom\n          '463': bucket, pail\n          '464': buckle\n          '465': bulletproof vest\n          '466': bullet train, bullet\n          '467': butcher shop, meat market\n          '468': cab, hack, taxi, taxicab\n          '469': caldron, cauldron\n          '470': candle, taper, wax light\n          '471': cannon\n          '472': canoe\n          '473': can opener, tin opener\n          '474': cardigan\n          '475': car mirror\n          '476': carousel, carrousel, merry-go-round, roundabout, whirligig\n          '477': carpenter's kit, tool kit\n          '478': carton\n          '479': car wheel\n          '480': cash machine, cash dispenser, automated teller machine, automatic\n            teller machine, automated teller, automatic teller, ATM\n          '481': cassette\n          '482': cassette player\n          '483': castle\n          '484': catamaran\n          '485': CD player\n          '486': cello, violoncello\n          '487': cellular telephone, cellular phone, cellphone, cell, mobile phone\n          '488': chain\n          '489': chainlink fence\n          '490': chain mail, ring mail, mail, chain armor, chain armour, ring armor,\n            ring armour\n          '491': chain saw, chainsaw\n          '492': chest\n          '493': chiffonier, commode\n          '494': chime, bell, gong\n          '495': china cabinet, china closet\n          '496': Christmas stocking\n          '497': church, church building\n          '498': cinema, movie theater, movie theatre, movie house, picture palace\n          '499': cleaver, meat cleaver, chopper\n          '500': cliff dwelling\n          '501': cloak\n          '502': clog, geta, patten, sabot\n          '503': cocktail shaker\n          '504': coffee mug\n          '505': coffeepot\n          '506': coil, spiral, volute, whorl, helix\n          '507': combination lock\n          '508': computer keyboard, keypad\n          '509': confectionery, confectionary, candy store\n          '510': container ship, containership, container vessel\n          '511': convertible\n          '512': corkscrew, bottle screw\n          '513': cornet, horn, trumpet, trump\n          '514': cowboy boot\n          '515': cowboy hat, ten-gallon hat\n          '516': cradle\n          '517': crane2\n          '518': crash helmet\n          '519': crate\n          '520': crib, cot\n          '521': Crock Pot\n          '522': croquet ball\n          '523': crutch\n          '524': cuirass\n          '525': dam, dike, dyke\n          '526': desk\n          '527': desktop computer\n          '528': dial telephone, dial phone\n          '529': diaper, nappy, napkin\n          '530': digital clock\n          '531': digital watch\n          '532': dining table, board\n          '533': dishrag, dishcloth\n          '534': dishwasher, dish washer, dishwashing machine\n          '535': disk brake, disc brake\n          '536': dock, dockage, docking facility\n          '537': dogsled, dog sled, dog sleigh\n          '538': dome\n          '539': doormat, welcome mat\n          '540': drilling platform, offshore rig\n          '541': drum, membranophone, tympan\n          '542': drumstick\n          '543': dumbbell\n          '544': Dutch oven\n          '545': electric fan, blower\n          '546': electric guitar\n          '547': electric locomotive\n          '548': entertainment center\n          '549': envelope\n          '550': espresso maker\n          '551': face powder\n          '552': feather boa, boa\n          '553': file, file cabinet, filing cabinet\n          '554': fireboat\n          '555': fire engine, fire truck\n          '556': fire screen, fireguard\n          '557': flagpole, flagstaff\n          '558': flute, transverse flute\n          '559': folding chair\n          '560': football helmet\n          '561': forklift\n          '562': fountain\n          '563': fountain pen\n          '564': four-poster\n          '565': freight car\n          '566': French horn, horn\n          '567': frying pan, frypan, skillet\n          '568': fur coat\n          '569': garbage truck, dustcart\n          '570': gasmask, respirator, gas helmet\n          '571': gas pump, gasoline pump, petrol pump, island dispenser\n          '572': goblet\n          '573': go-kart\n          '574': golf ball\n          '575': golfcart, golf cart\n          '576': gondola\n          '577': gong, tam-tam\n          '578': gown\n          '579': grand piano, grand\n          '580': greenhouse, nursery, glasshouse\n          '581': grille, radiator grille\n          '582': grocery store, grocery, food market, market\n          '583': guillotine\n          '584': hair slide\n          '585': hair spray\n          '586': half track\n          '587': hammer\n          '588': hamper\n          '589': hand blower, blow dryer, blow drier, hair dryer, hair drier\n          '590': hand-held computer, hand-held microcomputer\n          '591': handkerchief, hankie, hanky, hankey\n          '592': hard disc, hard disk, fixed disk\n          '593': harmonica, mouth organ, harp, mouth harp\n          '594': harp\n          '595': harvester, reaper\n          '596': hatchet\n          '597': holster\n          '598': home theater, home theatre\n          '599': honeycomb\n          '600': hook, claw\n          '601': hoopskirt, crinoline\n          '602': horizontal bar, high bar\n          '603': horse cart, horse-cart\n          '604': hourglass\n          '605': iPod\n          '606': iron, smoothing iron\n          '607': jack-o'-lantern\n          '608': jean, blue jean, denim\n          '609': jeep, landrover\n          '610': jersey, T-shirt, tee shirt\n          '611': jigsaw puzzle\n          '612': jinrikisha, ricksha, rickshaw\n          '613': joystick\n          '614': kimono\n          '615': knee pad\n          '616': knot\n          '617': lab coat, laboratory coat\n          '618': ladle\n          '619': lampshade, lamp shade\n          '620': laptop, laptop computer\n          '621': lawn mower, mower\n          '622': lens cap, lens cover\n          '623': letter opener, paper knife, paperknife\n          '624': library\n          '625': lifeboat\n          '626': lighter, light, igniter, ignitor\n          '627': limousine, limo\n          '628': liner, ocean liner\n          '629': lipstick, lip rouge\n          '630': Loafer\n          '631': lotion\n          '632': loudspeaker, speaker, speaker unit, loudspeaker system, speaker system\n          '633': loupe, jeweler's loupe\n          '634': lumbermill, sawmill\n          '635': magnetic compass\n          '636': mailbag, postbag\n          '637': mailbox, letter box\n          '638': maillot\n          '639': maillot, tank suit\n          '640': manhole cover\n          '641': maraca\n          '642': marimba, xylophone\n          '643': mask\n          '644': matchstick\n          '645': maypole\n          '646': maze, labyrinth\n          '647': measuring cup\n          '648': medicine chest, medicine cabinet\n          '649': megalith, megalithic structure\n          '650': microphone, mike\n          '651': microwave, microwave oven\n          '652': military uniform\n          '653': milk can\n          '654': minibus\n          '655': miniskirt, mini\n          '656': minivan\n          '657': missile\n          '658': mitten\n          '659': mixing bowl\n          '660': mobile home, manufactured home\n          '661': Model T\n          '662': modem\n          '663': monastery\n          '664': monitor\n          '665': moped\n          '666': mortar\n          '667': mortarboard\n          '668': mosque\n          '669': mosquito net\n          '670': motor scooter, scooter\n          '671': mountain bike, all-terrain bike, off-roader\n          '672': mountain tent\n          '673': mouse, computer mouse\n          '674': mousetrap\n          '675': moving van\n          '676': muzzle\n          '677': nail\n          '678': neck brace\n          '679': necklace\n          '680': nipple\n          '681': notebook, notebook computer\n          '682': obelisk\n          '683': oboe, hautboy, hautbois\n          '684': ocarina, sweet potato\n          '685': odometer, hodometer, mileometer, milometer\n          '686': oil filter\n          '687': organ, pipe organ\n          '688': oscilloscope, scope, cathode-ray oscilloscope, CRO\n          '689': overskirt\n          '690': oxcart\n          '691': oxygen mask\n          '692': packet\n          '693': paddle, boat paddle\n          '694': paddlewheel, paddle wheel\n          '695': padlock\n          '696': paintbrush\n          '697': pajama, pyjama, pj's, jammies\n          '698': palace\n          '699': panpipe, pandean pipe, syrinx\n          '700': paper towel\n          '701': parachute, chute\n          '702': parallel bars, bars\n          '703': park bench\n          '704': parking meter\n          '705': passenger car, coach, carriage\n          '706': patio, terrace\n          '707': pay-phone, pay-station\n          '708': pedestal, plinth, footstall\n          '709': pencil box, pencil case\n          '710': pencil sharpener\n          '711': perfume, essence\n          '712': Petri dish\n          '713': photocopier\n          '714': pick, plectrum, plectron\n          '715': pickelhaube\n          '716': picket fence, paling\n          '717': pickup, pickup truck\n          '718': pier\n          '719': piggy bank, penny bank\n          '720': pill bottle\n          '721': pillow\n          '722': ping-pong ball\n          '723': pinwheel\n          '724': pirate, pirate ship\n          '725': pitcher, ewer\n          '726': plane, carpenter's plane, woodworking plane\n          '727': planetarium\n          '728': plastic bag\n          '729': plate rack\n          '730': plow, plough\n          '731': plunger, plumber's helper\n          '732': Polaroid camera, Polaroid Land camera\n          '733': pole\n          '734': police van, police wagon, paddy wagon, patrol wagon, wagon, black\n            Maria\n          '735': poncho\n          '736': pool table, billiard table, snooker table\n          '737': pop bottle, soda bottle\n          '738': pot, flowerpot\n          '739': potter's wheel\n          '740': power drill\n          '741': prayer rug, prayer mat\n          '742': printer\n          '743': prison, prison house\n          '744': projectile, missile\n          '745': projector\n          '746': puck, hockey puck\n          '747': punching bag, punch bag, punching ball, punchball\n          '748': purse\n          '749': quill, quill pen\n          '750': quilt, comforter, comfort, puff\n          '751': racer, race car, racing car\n          '752': racket, racquet\n          '753': radiator\n          '754': radio, wireless\n          '755': radio telescope, radio reflector\n          '756': rain barrel\n          '757': recreational vehicle, RV, R.V.\n          '758': reel\n          '759': reflex camera\n          '760': refrigerator, icebox\n          '761': remote control, remote\n          '762': restaurant, eating house, eating place, eatery\n          '763': revolver, six-gun, six-shooter\n          '764': rifle\n          '765': rocking chair, rocker\n          '766': rotisserie\n          '767': rubber eraser, rubber, pencil eraser\n          '768': rugby ball\n          '769': rule, ruler\n          '770': running shoe\n          '771': safe\n          '772': safety pin\n          '773': saltshaker, salt shaker\n          '774': sandal\n          '775': sarong\n          '776': sax, saxophone\n          '777': scabbard\n          '778': scale, weighing machine\n          '779': school bus\n          '780': schooner\n          '781': scoreboard\n          '782': screen, CRT screen\n          '783': screw\n          '784': screwdriver\n          '785': seat belt, seatbelt\n          '786': sewing machine\n          '787': shield, buckler\n          '788': shoe shop, shoe-shop, shoe store\n          '789': shoji\n          '790': shopping basket\n          '791': shopping cart\n          '792': shovel\n          '793': shower cap\n          '794': shower curtain\n          '795': ski\n          '796': ski mask\n          '797': sleeping bag\n          '798': slide rule, slipstick\n          '799': sliding door\n          '800': slot, one-armed bandit\n          '801': snorkel\n          '802': snowmobile\n          '803': snowplow, snowplough\n          '804': soap dispenser\n          '805': soccer ball\n          '806': sock\n          '807': solar dish, solar collector, solar furnace\n          '808': sombrero\n          '809': soup bowl\n          '810': space bar\n          '811': space heater\n          '812': space shuttle\n          '813': spatula\n          '814': speedboat\n          '815': spider web, spider's web\n          '816': spindle\n          '817': sports car, sport car\n          '818': spotlight, spot\n          '819': stage\n          '820': steam locomotive\n          '821': steel arch bridge\n          '822': steel drum\n          '823': stethoscope\n          '824': stole\n          '825': stone wall\n          '826': stopwatch, stop watch\n          '827': stove\n          '828': strainer\n          '829': streetcar, tram, tramcar, trolley, trolley car\n          '830': stretcher\n          '831': studio couch, day bed\n          '832': stupa, tope\n          '833': submarine, pigboat, sub, U-boat\n          '834': suit, suit of clothes\n          '835': sundial\n          '836': sunglass\n          '837': sunglasses, dark glasses, shades\n          '838': sunscreen, sunblock, sun blocker\n          '839': suspension bridge\n          '840': swab, swob, mop\n          '841': sweatshirt\n          '842': swimming trunks, bathing trunks\n          '843': swing\n          '844': switch, electric switch, electrical switch\n          '845': syringe\n          '846': table lamp\n          '847': tank, army tank, armored combat vehicle, armoured combat vehicle\n          '848': tape player\n          '849': teapot\n          '850': teddy, teddy bear\n          '851': television, television system\n          '852': tennis ball\n          '853': thatch, thatched roof\n          '854': theater curtain, theatre curtain\n          '855': thimble\n          '856': thresher, thrasher, threshing machine\n          '857': throne\n          '858': tile roof\n          '859': toaster\n          '860': tobacco shop, tobacconist shop, tobacconist\n          '861': toilet seat\n          '862': torch\n          '863': totem pole\n          '864': tow truck, tow car, wrecker\n          '865': toyshop\n          '866': tractor\n          '867': trailer truck, tractor trailer, trucking rig, rig, articulated lorry,\n            semi\n          '868': tray\n          '869': trench coat\n          '870': tricycle, trike, velocipede\n          '871': trimaran\n          '872': tripod\n          '873': triumphal arch\n          '874': trolleybus, trolley coach, trackless trolley\n          '875': trombone\n          '876': tub, vat\n          '877': turnstile\n          '878': typewriter keyboard\n          '879': umbrella\n          '880': unicycle, monocycle\n          '881': upright, upright piano\n          '882': vacuum, vacuum cleaner\n          '883': vase\n          '884': vault\n          '885': velvet\n          '886': vending machine\n          '887': vestment\n          '888': viaduct\n          '889': violin, fiddle\n          '890': volleyball\n          '891': waffle iron\n          '892': wall clock\n          '893': wallet, billfold, notecase, pocketbook\n          '894': wardrobe, closet, press\n          '895': warplane, military plane\n          '896': washbasin, handbasin, washbowl, lavabo, wash-hand basin\n          '897': washer, automatic washer, washing machine\n          '898': water bottle\n          '899': water jug\n          '900': water tower\n          '901': whiskey jug\n          '902': whistle\n          '903': wig\n          '904': window screen\n          '905': window shade\n          '906': Windsor tie\n          '907': wine bottle\n          '908': wing\n          '909': wok\n          '910': wooden spoon\n          '911': wool, woolen, woollen\n          '912': worm fence, snake fence, snake-rail fence, Virginia fence\n          '913': wreck\n          '914': yawl\n          '915': yurt\n          '916': web site, website, internet site, site\n          '917': comic book\n          '918': crossword puzzle, crossword\n          '919': street sign\n          '920': traffic light, traffic signal, stoplight\n          '921': book jacket, dust cover, dust jacket, dust wrapper\n          '922': menu\n          '923': plate\n          '924': guacamole\n          '925': consomme\n          '926': hot pot, hotpot\n          '927': trifle\n          '928': ice cream, icecream\n          '929': ice lolly, lolly, lollipop, popsicle\n          '930': French loaf\n          '931': bagel, beigel\n          '932': pretzel\n          '933': cheeseburger\n          '934': hotdog, hot dog, red hot\n          '935': mashed potato\n          '936': head cabbage\n          '937': broccoli\n          '938': cauliflower\n          '939': zucchini, courgette\n          '940': spaghetti squash\n          '941': acorn squash\n          '942': butternut squash\n          '943': cucumber, cuke\n          '944': artichoke, globe artichoke\n          '945': bell pepper\n          '946': cardoon\n          '947': mushroom\n          '948': Granny Smith\n          '949': strawberry\n          '950': orange\n          '951': lemon\n          '952': fig\n          '953': pineapple, ananas\n          '954': banana\n          '955': jackfruit, jak, jack\n          '956': custard apple\n          '957': pomegranate\n          '958': hay\n          '959': carbonara\n          '960': chocolate sauce, chocolate syrup\n          '961': dough\n          '962': meat loaf, meatloaf\n          '963': pizza, pizza pie\n          '964': potpie\n          '965': burrito\n          '966': red wine\n          '967': espresso\n          '968': cup\n          '969': eggnog\n          '970': alp\n          '971': bubble\n          '972': cliff, drop, drop-off\n          '973': coral reef\n          '974': geyser\n          '975': lakeside, lakeshore\n          '976': promontory, headland, head, foreland\n          '977': sandbar, sand bar\n          '978': seashore, coast, seacoast, sea-coast\n          '979': valley, vale\n          '980': volcano\n          '981': ballplayer, baseball player\n          '982': groom, bridegroom\n          '983': scuba diver\n          '984': rapeseed\n          '985': daisy\n          '986': yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus,\n            Cypripedium parviflorum\n          '987': corn\n          '988': acorn\n          '989': hip, rose hip, rosehip\n          '990': buckeye, horse chestnut, conker\n          '991': coral fungus\n          '992': agaric\n          '993': gyromitra\n          '994': stinkhorn, carrion fungus\n          '995': earthstar\n          '996': hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola\n            frondosa\n          '997': bolete\n          '998': ear, spike, capitulum\n          '999': toilet tissue, toilet paper, bathroom tissue\n          '1000': none\n  splits:\n  - name: train\n    num_bytes: 19891500949.711\n    num_examples: 1281167\n  - name: validation\n    num_bytes: 708730650.0\n    num_examples: 50000\n  - name: test\n    num_bytes: 1418148200.0\n    num_examples: 100000\n  download_size: 19759304051\n  dataset_size: 22018379799.711\nconfigs:\n- config_name: default\n  data_files:\n  - split: train\n    path: data/train-*\n  - split: validation\n    path: data/validation-*\n  - split: test\n    path: data/test-*\n---\n\n# Repack Information\n\nThis repository contains a complete repack of [ILSVRC/imagenet-1k](https://huggingface.co/datasets/ILSVRC/imagenet-1k/) in Parquet format with the following data transformations:\n\n1. Images were center-cropped to square to the minimum height/width dimension.\n2. Images were then rescaled to 256x256 using Lanczos resampling.\n\n# Dataset Card for ImageNet\n\n## Table of Contents\n- [Dataset Description](#dataset-description)\n  - [Dataset Summary](#dataset-summary)\n  - [Supported Tasks](#supported-tasks-and-leaderboards)\n  - [Languages](#languages)\n- [Dataset Structure](#dataset-structure)\n  - [Data Instances](#data-instances)\n  - [Data Fields](#data-fields)\n  - [Data Splits](#data-splits)\n- [Dataset Creation](#dataset-creation)\n  - [Curation Rationale](#curation-rationale)\n  - [Source Data](#source-data)\n  - [Annotations](#annotations)\n  - [Personal and Sensitive Information](#personal-and-sensitive-information)\n- [Considerations for Using the Data](#considerations-for-using-the-data)\n  - [Social Impact of Dataset](#social-impact-of-dataset)\n  - [Discussion of Biases](#discussion-of-biases)\n  - [Other Known Limitations](#other-known-limitations)\n- [Additional Information](#additional-information)\n  - [Dataset Curators](#dataset-curators)\n  - [Licensing Information](#licensing-information)\n  - [Citation Information](#citation-information)\n  - [Contributions](#contributions)\n\n## Dataset Description\n\n- **Homepage:** https://image-net.org/index.php\n- **Repository:**\n- **Paper:** https://arxiv.org/abs/1409.0575\n- **Leaderboard:** https://paperswithcode.com/sota/image-classification-on-imagenet?tag_filter=171\n- **Point of Contact:** mailto: imagenet.help.desk@gmail.com  \n\n### Dataset Summary\n\nILSVRC 2012, commonly known as 'ImageNet' is an image dataset organized according to the WordNet hierarchy. Each meaningful concept in WordNet, possibly described by multiple words or word phrases, is called a \"synonym set\" or \"synset\". There are more than 100,000 synsets in WordNet, majority of them are nouns (80,000+). ImageNet aims to provide on average 1000 images to illustrate each synset. Images of each concept are quality-controlled and human-annotated.\n\n💡 This dataset provides access to ImageNet (ILSVRC) 2012 which is the most commonly used **subset** of ImageNet. This dataset spans 1000 object classes and contains 1,281,167 training images, 50,000 validation images and 100,000 test images. The version also has the [patch](https://drive.google.com/file/d/16RYnHpVOW0XKCsn3G3S9GTHUyoV2-4WX/view) which fixes some of the corrupted test set images already applied. For full ImageNet dataset presented in [[2]](https://ieeexplore.ieee.org/abstract/document/5206848), please check the download section of the [main website](https://image-net.org/download-images.php).\n\n### Supported Tasks and Leaderboards\n\n- `image-classification`: The goal of this task is to classify a given image into one of 1000 ImageNet classes. The leaderboard is available [here](https://paperswithcode.com/sota/image-classification-on-imagenet?tag_filter=171).\n\nTo evaluate the `imagenet-classification` accuracy on the test split, one must first create an account at https://image-net.org. This account must be approved by the site administrator. After the account is created, one can submit the results to the test server at https://image-net.org/challenges/LSVRC/eval_server.php The submission consists of several ASCII text files corresponding to multiple tasks. The task of interest is \"Classification submission (top-5 cls error)\". A sample of an exported text file looks like the following:\n\n```\n670 778 794 387 650\n217 691 564 909 364\n737 369 430 531 124\n755 930 755 512 152\n```\n\nThe export format is described in full in \"readme.txt\" within the 2013 development kit available here: https://image-net.org/data/ILSVRC/2013/ILSVRC2013_devkit.tgz. Please see the section entitled \"3.3 CLS-LOC submission format\". Briefly, the format of the text file is 100,000 lines corresponding to each image in the test split. Each line of integers correspond to the rank-ordered, top 5 predictions for each test image. The integers are 1-indexed corresponding to the line number in the corresponding labels file. See `imagenet2012_labels.txt`.\n\n### Languages\n\nThe class labels in the dataset are in English.\n\n## Dataset Structure\n\n### Data Instances\n\nAn example looks like below: \n\n```\n{\n  'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=384x512 at 0x276021C5EB8>,\n  'label': 23\n}\n```\n\n### Data Fields\n\nThe data instances have the following fields:\n\n- `image`: A `PIL.Image.Image` object containing the image. Note that when accessing the image column: `dataset[0][\"image\"]` the image file is automatically decoded. Decoding of a large number of image files might take a significant amount of time. Thus it is important to first query the sample index before the `\"image\"` column, *i.e.* `dataset[0][\"image\"]` should **always** be preferred over `dataset[\"image\"][0]`.\n- `label`: an `int` classification label. -1 for `test` set as the labels are missing.\n\nThe labels are indexed based on a sorted list of synset ids such as `n07565083` which we automatically map to original class names. The original dataset is divided into folders based on these synset ids. To get a mapping from original synset names, use the file [LOC_synset_mapping.txt](https://www.kaggle.com/competitions/imagenet-object-localization-challenge/data?select=LOC_synset_mapping.txt) available on Kaggle challenge page. You can also use `dataset_instance.features[\"labels\"].int2str` function to get the class for a particular label index. Also note that, labels for test set are returned as -1 as they are missing.\n\n<details>\n  <summary>\n  Click here to see the full list of ImageNet class labels mapping:\n  </summary>\n\n  |id|Class|\n  |--|-----|\n  |0 | tench, Tinca tinca|\n  |1 | goldfish, Carassius auratus|\n  |2 | great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias|\n  |3 | tiger shark, Galeocerdo cuvieri|\n  |4 | hammerhead, hammerhead shark|\n  |5 | electric ray, crampfish, numbfish, torpedo|\n  |6 | stingray|\n  |7 | cock|\n  |8 | hen|\n  |9 | ostrich, Struthio camelus|\n  |10 | brambling, Fringilla montifringilla|\n  |11 | goldfinch, Carduelis carduelis|\n  |12 | house finch, linnet, Carpodacus mexicanus|\n  |13 | junco, snowbird|\n  |14 | indigo bunting, indigo finch, indigo bird, Passerina cyanea|\n  |15 | robin, American robin, Turdus migratorius|\n  |16 | bulbul|\n  |17 | jay|\n  |18 | magpie|\n  |19 | chickadee|\n  |20 | water ouzel, dipper|\n  |21 | kite|\n  |22 | bald eagle, American eagle, Haliaeetus leucocephalus|\n  |23 | vulture|\n  |24 | great grey owl, great gray owl, Strix nebulosa|\n  |25 | European fire salamander, Salamandra salamandra|\n  |26 | common newt, Triturus vulgaris|\n  |27 | eft|\n  |28 | spotted salamander, Ambystoma maculatum|\n  |29 | axolotl, mud puppy, Ambystoma mexicanum|\n  |30 | bullfrog, Rana catesbeiana|\n  |31 | tree frog, tree-frog|\n  |32 | tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui|\n  |33 | loggerhead, loggerhead turtle, Caretta caretta|\n  |34 | leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea|\n  |35 | mud turtle|\n  |36 | terrapin|\n  |37 | box turtle, box tortoise|\n  |38 | banded gecko|\n  |39 | common iguana, iguana, Iguana iguana|\n  |40 | American chameleon, anole, Anolis carolinensis|\n  |41 | whiptail, whiptail lizard|\n  |42 | agama|\n  |43 | frilled lizard, Chlamydosaurus kingi|\n  |44 | alligator lizard|\n  |45 | Gila monster, Heloderma suspectum|\n  |46 | green lizard, Lacerta viridis|\n  |47 | African chameleon, Chamaeleo chamaeleon|\n  |48 | Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis|\n  |49 | African crocodile, Nile crocodile, Crocodylus niloticus|\n  |50 | American alligator, Alligator mississipiensis|\n  |51 | triceratops|\n  |52 | thunder snake, worm snake, Carphophis amoenus|\n  |53 | ringneck snake, ring-necked snake, ring snake|\n  |54 | hognose snake, puff adder, sand viper|\n  |55 | green snake, grass snake|\n  |56 | king snake, kingsnake|\n  |57 | garter snake, grass snake|\n  |58 | water snake|\n  |59 | vine snake|\n  |60 | night snake, Hypsiglena torquata|\n  |61 | boa constrictor, Constrictor constrictor|\n  |62 | rock python, rock snake, Python sebae|\n  |63 | Indian cobra, Naja naja|\n  |64 | green mamba|\n  |65 | sea snake|\n  |66 | horned viper, cerastes, sand viper, horned asp, Cerastes cornutus|\n  |67 | diamondback, diamondback rattlesnake, Crotalus adamanteus|\n  |68 | sidewinder, horned rattlesnake, Crotalus cerastes|\n  |69 | trilobite|\n  |70 | harvestman, daddy longlegs, Phalangium opilio|\n  |71 | scorpion|\n  |72 | black and gold garden spider, Argiope aurantia|\n  |73 | barn spider, Araneus cavaticus|\n  |74 | garden spider, Aranea diademata|\n  |75 | black widow, Latrodectus mactans|\n  |76 | tarantula|\n  |77 | wolf spider, hunting spider|\n  |78 | tick|\n  |79 | centipede|\n  |80 | black grouse|\n  |81 | ptarmigan|\n  |82 | ruffed grouse, partridge, Bonasa umbellus|\n  |83 | prairie chicken, prairie grouse, prairie fowl|\n  |84 | peacock|\n  |85 | quail|\n  |86 | partridge|\n  |87 | African grey, African gray, Psittacus erithacus|\n  |88 | macaw|\n  |89 | sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita|\n  |90 | lorikeet|\n  |91 | coucal|\n  |92 | bee eater|\n  |93 | hornbill|\n  |94 | hummingbird|\n  |95 | jacamar|\n  |96 | toucan|\n  |97 | drake|\n  |98 | red-breasted merganser, Mergus serrator|\n  |99 | goose|\n  |100 | black swan, Cygnus atratus|\n  |101 | tusker|\n  |102 | echidna, spiny anteater, anteater|\n  |103 | platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus|\n  |104 | wallaby, brush kangaroo|\n  |105 | koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus|\n  |106 | wombat|\n  |107 | jellyfish|\n  |108 | sea anemone, anemone|\n  |109 | brain coral|\n  |110 | flatworm, platyhelminth|\n  |111 | nematode, nematode worm, roundworm|\n  |112 | conch|\n  |113 | snail|\n  |114 | slug|\n  |115 | sea slug, nudibranch|\n  |116 | chiton, coat-of-mail shell, sea cradle, polyplacophore|\n  |117 | chambered nautilus, pearly nautilus, nautilus|\n  |118 | Dungeness crab, Cancer magister|\n  |119 | rock crab, Cancer irroratus|\n  |120 | fiddler crab|\n  |121 | king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica|\n  |122 | American lobster, Northern lobster, Maine lobster, Homarus americanus|\n  |123 | spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish|\n  |124 | crayfish, crawfish, crawdad, crawdaddy|\n  |125 | hermit crab|\n  |126 | isopod|\n  |127 | white stork, Ciconia ciconia|\n  |128 | black stork, Ciconia nigra|\n  |129 | spoonbill|\n  |130 | flamingo|\n  |131 | little blue heron, Egretta caerulea|\n  |132 | American egret, great white heron, Egretta albus|\n  |133 | bittern|\n  |134 | crane|\n  |135 | limpkin, Aramus pictus|\n  |136 | European gallinule, Porphyrio porphyrio|\n  |137 | American coot, marsh hen, mud hen, water hen, Fulica americana|\n  |138 | bustard|\n  |139 | ruddy turnstone, Arenaria interpres|\n  |140 | red-backed sandpiper, dunlin, Erolia alpina|\n  |141 | redshank, Tringa totanus|\n  |142 | dowitcher|\n  |143 | oystercatcher, oyster catcher|\n  |144 | pelican|\n  |145 | king penguin, Aptenodytes patagonica|\n  |146 | albatross, mollymawk|\n  |147 | grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus|\n  |148 | killer whale, killer, orca, grampus, sea wolf, Orcinus orca|\n  |149 | dugong, Dugong dugon|\n  |150 | sea lion|\n  |151 | Chihuahua|\n  |152 | Japanese spaniel|\n  |153 | Maltese dog, Maltese terrier, Maltese|\n  |154 | Pekinese, Pekingese, Peke|\n  |155 | Shih-Tzu|\n  |156 | Blenheim spaniel|\n  |157 | papillon|\n  |158 | toy terrier|\n  |159 | Rhodesian ridgeback|\n  |160 | Afghan hound, Afghan|\n  |161 | basset, basset hound|\n  |162 | beagle|\n  |163 | bloodhound, sleuthhound|\n  |164 | bluetick|\n  |165 | black-and-tan coonhound|\n  |166 | Walker hound, Walker foxhound|\n  |167 | English foxhound|\n  |168 | redbone|\n  |169 | borzoi, Russian wolfhound|\n  |170 | Irish wolfhound|\n  |171 | Italian greyhound|\n  |172 | whippet|\n  |173 | Ibizan hound, Ibizan Podenco|\n  |174 | Norwegian elkhound, elkhound|\n  |175 | otterhound, otter hound|\n  |176 | Saluki, gazelle hound|\n  |177 | Scottish deerhound, deerhound|\n  |178 | Weimaraner|\n  |179 | Staffordshire bullterrier, Staffordshire bull terrier|\n  |180 | American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier|\n  |181 | Bedlington terrier|\n  |182 | Border terrier|\n  |183 | Kerry blue terrier|\n  |184 | Irish terrier|\n  |185 | Norfolk terrier|\n  |186 | Norwich terrier|\n  |187 | Yorkshire terrier|\n  |188 | wire-haired fox terrier|\n  |189 | Lakeland terrier|\n  |190 | Sealyham terrier, Sealyham|\n  |191 | Airedale, Airedale terrier|\n  |192 | cairn, cairn terrier|\n  |193 | Australian terrier|\n  |194 | Dandie Dinmont, Dandie Dinmont terrier|\n  |195 | Boston bull, Boston terrier|\n  |196 | miniature schnauzer|\n  |197 | giant schnauzer|\n  |198 | standard schnauzer|\n  |199 | Scotch terrier, Scottish terrier, Scottie|\n  |200 | Tibetan terrier, chrysanthemum dog|\n  |201 | silky terrier, Sydney silky|\n  |202 | soft-coated wheaten terrier|\n  |203 | West Highland white terrier|\n  |204 | Lhasa, Lhasa apso|\n  |205 | flat-coated retriever|\n  |206 | curly-coated retriever|\n  |207 | golden retriever|\n  |208 | Labrador retriever|\n  |209 | Chesapeake Bay retriever|\n  |210 | German short-haired pointer|\n  |211 | vizsla, Hungarian pointer|\n  |212 | English setter|\n  |213 | Irish setter, red setter|\n  |214 | Gordon setter|\n  |215 | Brittany spaniel|\n  |216 | clumber, clumber spaniel|\n  |217 | English springer, English springer spaniel|\n  |218 | Welsh springer spaniel|\n  |219 | cocker spaniel, English cocker spaniel, cocker|\n  |220 | Sussex spaniel|\n  |221 | Irish water spaniel|\n  |222 | kuvasz|\n  |223 | schipperke|\n  |224 | groenendael|\n  |225 | malinois|\n  |226 | briard|\n  |227 | kelpie|\n  |228 | komondor|\n  |229 | Old English sheepdog, bobtail|\n  |230 | Shetland sheepdog, Shetland sheep dog, Shetland|\n  |231 | collie|\n  |232 | Border collie|\n  |233 | Bouvier des Flandres, Bouviers des Flandres|\n  |234 | Rottweiler|\n  |235 | German shepherd, German shepherd dog, German police dog, alsatian|\n  |236 | Doberman, Doberman pinscher|\n  |237 | miniature pinscher|\n  |238 | Greater Swiss Mountain dog|\n  |239 | Bernese mountain dog|\n  |240 | Appenzeller|\n  |241 | EntleBucher|\n  |242 | boxer|\n  |243 | bull mastiff|\n  |244 | Tibetan mastiff|\n  |245 | French bulldog|\n  |246 | Great Dane|\n  |247 | Saint Bernard, St Bernard|\n  |248 | Eskimo dog, husky|\n  |249 | malamute, malemute, Alaskan malamute|\n  |250 | Siberian husky|\n  |251 | dalmatian, coach dog, carriage dog|\n  |252 | affenpinscher, monkey pinscher, monkey dog|\n  |253 | basenji|\n  |254 | pug, pug-dog|\n  |255 | Leonberg|\n  |256 | Newfoundland, Newfoundland dog|\n  |257 | Great Pyrenees|\n  |258 | Samoyed, Samoyede|\n  |259 | Pomeranian|\n  |260 | chow, chow chow|\n  |261 | keeshond|\n  |262 | Brabancon griffon|\n  |263 | Pembroke, Pembroke Welsh corgi|\n  |264 | Cardigan, Cardigan Welsh corgi|\n  |265 | toy poodle|\n  |266 | miniature poodle|\n  |267 | standard poodle|\n  |268 | Mexican hairless|\n  |269 | timber wolf, grey wolf, gray wolf, Canis lupus|\n  |270 | white wolf, Arctic wolf, Canis lupus tundrarum|\n  |271 | red wolf, maned wolf, Canis rufus, Canis niger|\n  |272 | coyote, prairie wolf, brush wolf, Canis latrans|\n  |273 | dingo, warrigal, warragal, Canis dingo|\n  |274 | dhole, Cuon alpinus|\n  |275 | African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus|\n  |276 | hyena, hyaena|\n  |277 | red fox, Vulpes vulpes|\n  |278 | kit fox, Vulpes macrotis|\n  |279 | Arctic fox, white fox, Alopex lagopus|\n  |280 | grey fox, gray fox, Urocyon cinereoargenteus|\n  |281 | tabby, tabby cat|\n  |282 | tiger cat|\n  |283 | Persian cat|\n  |284 | Siamese cat, Siamese|\n  |285 | Egyptian cat|\n  |286 | cougar, puma, catamount, mountain lion, painter, panther, Felis concolor|\n  |287 | lynx, catamount|\n  |288 | leopard, Panthera pardus|\n  |289 | snow leopard, ounce, Panthera uncia|\n  |290 | jaguar, panther, Panthera onca, Felis onca|\n  |291 | lion, king of beasts, Panthera leo|\n  |292 | tiger, Panthera tigris|\n  |293 | cheetah, chetah, Acinonyx jubatus|\n  |294 | brown bear, bruin, Ursus arctos|\n  |295 | American black bear, black bear, Ursus americanus, Euarctos americanus|\n  |296 | ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus|\n  |297 | sloth bear, Melursus ursinus, Ursus ursinus|\n  |298 | mongoose|\n  |299 | meerkat, mierkat|\n  |300 | tiger beetle|\n  |301 | ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle|\n  |302 | ground beetle, carabid beetle|\n  |303 | long-horned beetle, longicorn, longicorn beetle|\n  |304 | leaf beetle, chrysomelid|\n  |305 | dung beetle|\n  |306 | rhinoceros beetle|\n  |307 | weevil|\n  |308 | fly|\n  |309 | bee|\n  |310 | ant, emmet, pismire|\n  |311 | grasshopper, hopper|\n  |312 | cricket|\n  |313 | walking stick, walkingstick, stick insect|\n  |314 | cockroach, roach|\n  |315 | mantis, mantid|\n  |316 | cicada, cicala|\n  |317 | leafhopper|\n  |318 | lacewing, lacewing fly|\n  |319 | dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk|\n  |320 | damselfly|\n  |321 | admiral|\n  |322 | ringlet, ringlet butterfly|\n  |323 | monarch, monarch butterfly, milkweed butterfly, Danaus plexippus|\n  |324 | cabbage butterfly|\n  |325 | sulphur butterfly, sulfur butterfly|\n  |326 | lycaenid, lycaenid butterfly|\n  |327 | starfish, sea star|\n  |328 | sea urchin|\n  |329 | sea cucumber, holothurian|\n  |330 | wood rabbit, cottontail, cottontail rabbit|\n  |331 | hare|\n  |332 | Angora, Angora rabbit|\n  |333 | hamster|\n  |334 | porcupine, hedgehog|\n  |335 | fox squirrel, eastern fox squirrel, Sciurus niger|\n  |336 | marmot|\n  |337 | beaver|\n  |338 | guinea pig, Cavia cobaya|\n  |339 | sorrel|\n  |340 | zebra|\n  |341 | hog, pig, grunter, squealer, Sus scrofa|\n  |342 | wild boar, boar, Sus scrofa|\n  |343 | warthog|\n  |344 | hippopotamus, hippo, river horse, Hippopotamus amphibius|\n  |345 | ox|\n  |346 | water buffalo, water ox, Asiatic buffalo, Bubalus bubalis|\n  |347 | bison|\n  |348 | ram, tup|\n  |349 | bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis|\n  |350 | ibex, Capra ibex|\n  |351 | hartebeest|\n  |352 | impala, Aepyceros melampus|\n  |353 | gazelle|\n  |354 | Arabian camel, dromedary, Camelus dromedarius|\n  |355 | llama|\n  |356 | weasel|\n  |357 | mink|\n  |358 | polecat, fitch, foulmart, foumart, Mustela putorius|\n  |359 | black-footed ferret, ferret, Mustela nigripes|\n  |360 | otter|\n  |361 | skunk, polecat, wood pussy|\n  |362 | badger|\n  |363 | armadillo|\n  |364 | three-toed sloth, ai, Bradypus tridactylus|\n  |365 | orangutan, orang, orangutang, Pongo pygmaeus|\n  |366 | gorilla, Gorilla gorilla|\n  |367 | chimpanzee, chimp, Pan troglodytes|\n  |368 | gibbon, Hylobates lar|\n  |369 | siamang, Hylobates syndactylus, Symphalangus syndactylus|\n  |370 | guenon, guenon monkey|\n  |371 | patas, hussar monkey, Erythrocebus patas|\n  |372 | baboon|\n  |373 | macaque|\n  |374 | langur|\n  |375 | colobus, colobus monkey|\n  |376 | proboscis monkey, Nasalis larvatus|\n  |377 | marmoset|\n  |378 | capuchin, ringtail, Cebus capucinus|\n  |379 | howler monkey, howler|\n  |380 | titi, titi monkey|\n  |381 | spider monkey, Ateles geoffroyi|\n  |382 | squirrel monkey, Saimiri sciureus|\n  |383 | Madagascar cat, ring-tailed lemur, Lemur catta|\n  |384 | indri, indris, Indri indri, Indri brevicaudatus|\n  |385 | Indian elephant, Elephas maximus|\n  |386 | African elephant, Loxodonta africana|\n  |387 | lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens|\n  |388 | giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca|\n  |389 | barracouta, snoek|\n  |390 | eel|\n  |391 | coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch|\n  |392 | rock beauty, Holocanthus tricolor|\n  |393 | anemone fish|\n  |394 | sturgeon|\n  |395 | gar, garfish, garpike, billfish, Lepisosteus osseus|\n  |396 | lionfish|\n  |397 | puffer, pufferfish, blowfish, globefish|\n  |398 | abacus|\n  |399 | abaya|\n  |400 | academic gown, academic robe, judge's robe|\n  |401 | accordion, piano accordion, squeeze box|\n  |402 | acoustic guitar|\n  |403 | aircraft carrier, carrier, flattop, attack aircraft carrier|\n  |404 | airliner|\n  |405 | airship, dirigible|\n  |406 | altar|\n  |407 | ambulance|\n  |408 | amphibian, amphibious vehicle|\n  |409 | analog clock|\n  |410 | apiary, bee house|\n  |411 | apron|\n  |412 | ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin|\n  |413 | assault rifle, assault gun|\n  |414 | backpack, back pack, knapsack, packsack, rucksack, haversack|\n  |415 | bakery, bakeshop, bakehouse|\n  |416 | balance beam, beam|\n  |417 | balloon|\n  |418 | ballpoint, ballpoint pen, ballpen, Biro|\n  |419 | Band Aid|\n  |420 | banjo|\n  |421 | bannister, banister, balustrade, balusters, handrail|\n  |422 | barbell|\n  |423 | barber chair|\n  |424 | barbershop|\n  |425 | barn|\n  |426 | barometer|\n  |427 | barrel, cask|\n  |428 | barrow, garden cart, lawn cart, wheelbarrow|\n  |429 | baseball|\n  |430 | basketball|\n  |431 | bassinet|\n  |432 | bassoon|\n  |433 | bathing cap, swimming cap|\n  |434 | bath towel|\n  |435 | bathtub, bathing tub, bath, tub|\n  |436 | beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon|\n  |437 | beacon, lighthouse, beacon light, pharos|\n  |438 | beaker|\n  |439 | bearskin, busby, shako|\n  |440 | beer bottle|\n  |441 | beer glass|\n  |442 | bell cote, bell cot|\n  |443 | bib|\n  |444 | bicycle-built-for-two, tandem bicycle, tandem|\n  |445 | bikini, two-piece|\n  |446 | binder, ring-binder|\n  |447 | binoculars, field glasses, opera glasses|\n  |448 | birdhouse|\n  |449 | boathouse|\n  |450 | bobsled, bobsleigh, bob|\n  |451 | bolo tie, bolo, bola tie, bola|\n  |452 | bonnet, poke bonnet|\n  |453 | bookcase|\n  |454 | bookshop, bookstore, bookstall|\n  |455 | bottlecap|\n  |456 | bow|\n  |457 | bow tie, bow-tie, bowtie|\n  |458 | brass, memorial tablet, plaque|\n  |459 | brassiere, bra, bandeau|\n  |460 | breakwater, groin, groyne, mole, bulwark, seawall, jetty|\n  |461 | breastplate, aegis, egis|\n  |462 | broom|\n  |463 | bucket, pail|\n  |464 | buckle|\n  |465 | bulletproof vest|\n  |466 | bullet train, bullet|\n  |467 | butcher shop, meat market|\n  |468 | cab, hack, taxi, taxicab|\n  |469 | caldron, cauldron|\n  |470 | candle, taper, wax light|\n  |471 | cannon|\n  |472 | canoe|\n  |473 | can opener, tin opener|\n  |474 | cardigan|\n  |475 | car mirror|\n  |476 | carousel, carrousel, merry-go-round, roundabout, whirligig|\n  |477 | carpenter's kit, tool kit|\n  |478 | carton|\n  |479 | car wheel|\n  |480 | cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM|\n  |481 | cassette|\n  |482 | cassette player|\n  |483 | castle|\n  |484 | catamaran|\n  |485 | CD player|\n  |486 | cello, violoncello|\n  |487 | cellular telephone, cellular phone, cellphone, cell, mobile phone|\n  |488 | chain|\n  |489 | chainlink fence|\n  |490 | chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour|\n  |491 | chain saw, chainsaw|\n  |492 | chest|\n  |493 | chiffonier, commode|\n  |494 | chime, bell, gong|\n  |495 | china cabinet, china closet|\n  |496 | Christmas stocking|\n  |497 | church, church building|\n  |498 | cinema, movie theater, movie theatre, movie house, picture palace|\n  |499 | cleaver, meat cleaver, chopper|\n  |500 | cliff dwelling|\n  |501 | cloak|\n  |502 | clog, geta, patten, sabot|\n  |503 | cocktail shaker|\n  |504 | coffee mug|\n  |505 | coffeepot|\n  |506 | coil, spiral, volute, whorl, helix|\n  |507 | combination lock|\n  |508 | computer keyboard, keypad|\n  |509 | confectionery, confectionary, candy store|\n  |510 | container ship, containership, container vessel|\n  |511 | convertible|\n  |512 | corkscrew, bottle screw|\n  |513 | cornet, horn, trumpet, trump|\n  |514 | cowboy boot|\n  |515 | cowboy hat, ten-gallon hat|\n  |516 | cradle|\n  |517 | crane_1|\n  |518 | crash helmet|\n  |519 | crate|\n  |520 | crib, cot|\n  |521 | Crock Pot|\n  |522 | croquet ball|\n  |523 | crutch|\n  |524 | cuirass|\n  |525 | dam, dike, dyke|\n  |526 | desk|\n  |527 | desktop computer|\n  |528 | dial telephone, dial phone|\n  |529 | diaper, nappy, napkin|\n  |530 | digital clock|\n  |531 | digital watch|\n  |532 | dining table, board|\n  |533 | dishrag, dishcloth|\n  |534 | dishwasher, dish washer, dishwashing machine|\n  |535 | disk brake, disc brake|\n  |536 | dock, dockage, docking facility|\n  |537 | dogsled, dog sled, dog sleigh|\n  |538 | dome|\n  |539 | doormat, welcome mat|\n  |540 | drilling platform, offshore rig|\n  |541 | drum, membranophone, tympan|\n  |542 | drumstick|\n  |543 | dumbbell|\n  |544 | Dutch oven|\n  |545 | electric fan, blower|\n  |546 | electric guitar|\n  |547 | electric locomotive|\n  |548 | entertainment center|\n  |549 | envelope|\n  |550 | espresso maker|\n  |551 | face powder|\n  |552 | feather boa, boa|\n  |553 | file, file cabinet, filing cabinet|\n  |554 | fireboat|\n  |555 | fire engine, fire truck|\n  |556 | fire screen, fireguard|\n  |557 | flagpole, flagstaff|\n  |558 | flute, transverse flute|\n  |559 | folding chair|\n  |560 | football helmet|\n  |561 | forklift|\n  |562 | fountain|\n  |563 | fountain pen|\n  |564 | four-poster|\n  |565 | freight car|\n  |566 | French horn, horn|\n  |567 | frying pan, frypan, skillet|\n  |568 | fur coat|\n  |569 | garbage truck, dustcart|\n  |570 | gasmask, respirator, gas helmet|\n  |571 | gas pump, gasoline pump, petrol pump, island dispenser|\n  |572 | goblet|\n  |573 | go-kart|\n  |574 | golf ball|\n  |575 | golfcart, golf cart|\n  |576 | gondola|\n  |577 | gong, tam-tam|\n  |578 | gown|\n  |579 | grand piano, grand|\n  |580 | greenhouse, nursery, glasshouse|\n  |581 | grille, radiator grille|\n  |582 | grocery store, grocery, food market, market|\n  |583 | guillotine|\n  |584 | hair slide|\n  |585 | hair spray|\n  |586 | half track|\n  |587 | hammer|\n  |588 | hamper|\n  |589 | hand blower, blow dryer, blow drier, hair dryer, hair drier|\n  |590 | hand-held computer, hand-held microcomputer|\n  |591 | handkerchief, hankie, hanky, hankey|\n  |592 | hard disc, hard disk, fixed disk|\n  |593 | harmonica, mouth organ, harp, mouth harp|\n  |594 | harp|\n  |595 | harvester, reaper|\n  |596 | hatchet|\n  |597 | holster|\n  |598 | home theater, home theatre|\n  |599 | honeycomb|\n  |600 | hook, claw|\n  |601 | hoopskirt, crinoline|\n  |602 | horizontal bar, high bar|\n  |603 | horse cart, horse-cart|\n  |604 | hourglass|\n  |605 | iPod|\n  |606 | iron, smoothing iron|\n  |607 | jack-o'-lantern|\n  |608 | jean, blue jean, denim|\n  |609 | jeep, landrover|\n  |610 | jersey, T-shirt, tee shirt|\n  |611 | jigsaw puzzle|\n  |612 | jinrikisha, ricksha, rickshaw|\n  |613 | joystick|\n  |614 | kimono|\n  |615 | knee pad|\n  |616 | knot|\n  |617 | lab coat, laboratory coat|\n  |618 | ladle|\n  |619 | lampshade, lamp shade|\n  |620 | laptop, laptop computer|\n  |621 | lawn mower, mower|\n  |622 | lens cap, lens cover|\n  |623 | letter opener, paper knife, paperknife|\n  |624 | library|\n  |625 | lifeboat|\n  |626 | lighter, light, igniter, ignitor|\n  |627 | limousine, limo|\n  |628 | liner, ocean liner|\n  |629 | lipstick, lip rouge|\n  |630 | Loafer|\n  |631 | lotion|\n  |632 | loudspeaker, speaker, speaker unit, loudspeaker system, speaker system|\n  |633 | loupe, jeweler's loupe|\n  |634 | lumbermill, sawmill|\n  |635 | magnetic compass|\n  |636 | mailbag, postbag|\n  |637 | mailbox, letter box|\n  |638 | maillot|\n  |639 | maillot, tank suit|\n  |640 | manhole cover|\n  |641 | maraca|\n  |642 | marimba, xylophone|\n  |643 | mask|\n  |644 | matchstick|\n  |645 | maypole|\n  |646 | maze, labyrinth|\n  |647 | measuring cup|\n  |648 | medicine chest, medicine cabinet|\n  |649 | megalith, megalithic structure|\n  |650 | microphone, mike|\n  |651 | microwave, microwave oven|\n  |652 | military uniform|\n  |653 | milk can|\n  |654 | minibus|\n  |655 | miniskirt, mini|\n  |656 | minivan|\n  |657 | missile|\n  |658 | mitten|\n  |659 | mixing bowl|\n  |660 | mobile home, manufactured home|\n  |661 | Model T|\n  |662 | modem|\n  |663 | monastery|\n  |664 | monitor|\n  |665 | moped|\n  |666 | mortar|\n  |667 | mortarboard|\n  |668 | mosque|\n  |669 | mosquito net|\n  |670 | motor scooter, scooter|\n  |671 | mountain bike, all-terrain bike, off-roader|\n  |672 | mountain tent|\n  |673 | mouse, computer mouse|\n  |674 | mousetrap|\n  |675 | moving van|\n  |676 | muzzle|\n  |677 | nail|\n  |678 | neck brace|\n  |679 | necklace|\n  |680 | nipple|\n  |681 | notebook, notebook computer|\n  |682 | obelisk|\n  |683 | oboe, hautboy, hautbois|\n  |684 | ocarina, sweet potato|\n  |685 | odometer, hodometer, mileometer, milometer|\n  |686 | oil filter|\n  |687 | organ, pipe organ|\n  |688 | oscilloscope, scope, cathode-ray oscilloscope, CRO|\n  |689 | overskirt|\n  |690 | oxcart|\n  |691 | oxygen mask|\n  |692 | packet|\n  |693 | paddle, boat paddle|\n  |694 | paddlewheel, paddle wheel|\n  |695 | padlock|\n  |696 | paintbrush|\n  |697 | pajama, pyjama, pj's, jammies|\n  |698 | palace|\n  |699 | panpipe, pandean pipe, syrinx|\n  |700 | paper towel|\n  |701 | parachute, chute|\n  |702 | parallel bars, bars|\n  |703 | park bench|\n  |704 | parking meter|\n  |705 | passenger car, coach, carriage|\n  |706 | patio, terrace|\n  |707 | pay-phone, pay-station|\n  |708 | pedestal, plinth, footstall|\n  |709 | pencil box, pencil case|\n  |710 | pencil sharpener|\n  |711 | perfume, essence|\n  |712 | Petri dish|\n  |713 | photocopier|\n  |714 | pick, plectrum, plectron|\n  |715 | pickelhaube|\n  |716 | picket fence, paling|\n  |717 | pickup, pickup truck|\n  |718 | pier|\n  |719 | piggy bank, penny bank|\n  |720 | pill bottle|\n  |721 | pillow|\n  |722 | ping-pong ball|\n  |723 | pinwheel|\n  |724 | pirate, pirate ship|\n  |725 | pitcher, ewer|\n  |726 | plane, carpenter's plane, woodworking plane|\n  |727 | planetarium|\n  |728 | plastic bag|\n  |729 | plate rack|\n  |730 | plow, plough|\n  |731 | plunger, plumber's helper|\n  |732 | Polaroid camera, Polaroid Land camera|\n  |733 | pole|\n  |734 | police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria|\n  |735 | poncho|\n  |736 | pool table, billiard table, snooker table|\n  |737 | pop bottle, soda bottle|\n  |738 | pot, flowerpot|\n  |739 | potter's wheel|\n  |740 | power drill|\n  |741 | prayer rug, prayer mat|\n  |742 | printer|\n  |743 | prison, prison house|\n  |744 | projectile, missile|\n  |745 | projector|\n  |746 | puck, hockey puck|\n  |747 | punching bag, punch bag, punching ball, punchball|\n  |748 | purse|\n  |749 | quill, quill pen|\n  |750 | quilt, comforter, comfort, puff|\n  |751 | racer, race car, racing car|\n  |752 | racket, racquet|\n  |753 | radiator|\n  |754 | radio, wireless|\n  |755 | radio telescope, radio reflector|\n  |756 | rain barrel|\n  |757 | recreational vehicle, RV, R.V.|\n  |758 | reel|\n  |759 | reflex camera|\n  |760 | refrigerator, icebox|\n  |761 | remote control, remote|\n  |762 | restaurant, eating house, eating place, eatery|\n  |763 | revolver, six-gun, six-shooter|\n  |764 | rifle|\n  |765 | rocking chair, rocker|\n  |766 | rotisserie|\n  |767 | rubber eraser, rubber, pencil eraser|\n  |768 | rugby ball|\n  |769 | rule, ruler|\n  |770 | running shoe|\n  |771 | safe|\n  |772 | safety pin|\n  |773 | saltshaker, salt shaker|\n  |774 | sandal|\n  |775 | sarong|\n  |776 | sax, saxophone|\n  |777 | scabbard|\n  |778 | scale, weighing machine|\n  |779 | school bus|\n  |780 | schooner|\n  |781 | scoreboard|\n  |782 | screen, CRT screen|\n  |783 | screw|\n  |784 | screwdriver|\n  |785 | seat belt, seatbelt|\n  |786 | sewing machine|\n  |787 | shield, buckler|\n  |788 | shoe shop, shoe-shop, shoe store|\n  |789 | shoji|\n  |790 | shopping basket|\n  |791 | shopping cart|\n  |792 | shovel|\n  |793 | shower cap|\n  |794 | shower curtain|\n  |795 | ski|\n  |796 | ski mask|\n  |797 | sleeping bag|\n  |798 | slide rule, slipstick|\n  |799 | sliding door|\n  |800 | slot, one-armed bandit|\n  |801 | snorkel|\n  |802 | snowmobile|\n  |803 | snowplow, snowplough|\n  |804 | soap dispenser|\n  |805 | soccer ball|\n  |806 | sock|\n  |807 | solar dish, solar collector, solar furnace|\n  |808 | sombrero|\n  |809 | soup bowl|\n  |810 | space bar|\n  |811 | space heater|\n  |812 | space shuttle|\n  |813 | spatula|\n  |814 | speedboat|\n  |815 | spider web, spider's web|\n  |816 | spindle|\n  |817 | sports car, sport car|\n  |818 | spotlight, spot|\n  |819 | stage|\n  |820 | steam locomotive|\n  |821 | steel arch bridge|\n  |822 | steel drum|\n  |823 | stethoscope|\n  |824 | stole|\n  |825 | stone wall|\n  |826 | stopwatch, stop watch|\n  |827 | stove|\n  |828 | strainer|\n  |829 | streetcar, tram, tramcar, trolley, trolley car|\n  |830 | stretcher|\n  |831 | studio couch, day bed|\n  |832 | stupa, tope|\n  |833 | submarine, pigboat, sub, U-boat|\n  |834 | suit, suit of clothes|\n  |835 | sundial|\n  |836 | sunglass|\n  |837 | sunglasses, dark glasses, shades|\n  |838 | sunscreen, sunblock, sun blocker|\n  |839 | suspension bridge|\n  |840 | swab, swob, mop|\n  |841 | sweatshirt|\n  |842 | swimming trunks, bathing trunks|\n  |843 | swing|\n  |844 | switch, electric switch, electrical switch|\n  |845 | syringe|\n  |846 | table lamp|\n  |847 | tank, army tank, armored combat vehicle, armoured combat vehicle|\n  |848 | tape player|\n  |849 | teapot|\n  |850 | teddy, teddy bear|\n  |851 | television, television system|\n  |852 | tennis ball|\n  |853 | thatch, thatched roof|\n  |854 | theater curtain, theatre curtain|\n  |855 | thimble|\n  |856 | thresher, thrasher, threshing machine|\n  |857 | throne|\n  |858 | tile roof|\n  |859 | toaster|\n  |860 | tobacco shop, tobacconist shop, tobacconist|\n  |861 | toilet seat|\n  |862 | torch|\n  |863 | totem pole|\n  |864 | tow truck, tow car, wrecker|\n  |865 | toyshop|\n  |866 | tractor|\n  |867 | trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi|\n  |868 | tray|\n  |869 | trench coat|\n  |870 | tricycle, trike, velocipede|\n  |871 | trimaran|\n  |872 | tripod|\n  |873 | triumphal arch|\n  |874 | trolleybus, trolley coach, trackless trolley|\n  |875 | trombone|\n  |876 | tub, vat|\n  |877 | turnstile|\n  |878 | typewriter keyboard|\n  |879 | umbrella|\n  |880 | unicycle, monocycle|\n  |881 | upright, upright piano|\n  |882 | vacuum, vacuum cleaner|\n  |883 | vase|\n  |884 | vault|\n  |885 | velvet|\n  |886 | vending machine|\n  |887 | vestment|\n  |888 | viaduct|\n  |889 | violin, fiddle|\n  |890 | volleyball|\n  |891 | waffle iron|\n  |892 | wall clock|\n  |893 | wallet, billfold, notecase, pocketbook|\n  |894 | wardrobe, closet, press|\n  |895 | warplane, military plane|\n  |896 | washbasin, handbasin, washbowl, lavabo, wash-hand basin|\n  |897 | washer, automatic washer, washing machine|\n  |898 | water bottle|\n  |899 | water jug|\n  |900 | water tower|\n  |901 | whiskey jug|\n  |902 | whistle|\n  |903 | wig|\n  |904 | window screen|\n  |905 | window shade|\n  |906 | Windsor tie|\n  |907 | wine bottle|\n  |908 | wing|\n  |909 | wok|\n  |910 | wooden spoon|\n  |911 | wool, woolen, woollen|\n  |912 | worm fence, snake fence, snake-rail fence, Virginia fence|\n  |913 | wreck|\n  |914 | yawl|\n  |915 | yurt|\n  |916 | web site, website, internet site, site|\n  |917 | comic book|\n  |918 | crossword puzzle, crossword|\n  |919 | street sign|\n  |920 | traffic light, traffic signal, stoplight|\n  |921 | book jacket, dust cover, dust jacket, dust wrapper|\n  |922 | menu|\n  |923 | plate|\n  |924 | guacamole|\n  |925 | consomme|\n  |926 | hot pot, hotpot|\n  |927 | trifle|\n  |928 | ice cream, icecream|\n  |929 | ice lolly, lolly, lollipop, popsicle|\n  |930 | French loaf|\n  |931 | bagel, beigel|\n  |932 | pretzel|\n  |933 | cheeseburger|\n  |934 | hotdog, hot dog, red hot|\n  |935 | mashed potato|\n  |936 | head cabbage|\n  |937 | broccoli|\n  |938 | cauliflower|\n  |939 | zucchini, courgette|\n  |940 | spaghetti squash|\n  |941 | acorn squash|\n  |942 | butternut squash|\n  |943 | cucumber, cuke|\n  |944 | artichoke, globe artichoke|\n  |945 | bell pepper|\n  |946 | cardoon|\n  |947 | mushroom|\n  |948 | Granny Smith|\n  |949 | strawberry|\n  |950 | orange|\n  |951 | lemon|\n  |952 | fig|\n  |953 | pineapple, ananas|\n  |954 | banana|\n  |955 | jackfruit, jak, jack|\n  |956 | custard apple|\n  |957 | pomegranate|\n  |958 | hay|\n  |959 | carbonara|\n  |960 | chocolate sauce, chocolate syrup|\n  |961 | dough|\n  |962 | meat loaf, meatloaf|\n  |963 | pizza, pizza pie|\n  |964 | potpie|\n  |965 | burrito|\n  |966 | red wine|\n  |967 | espresso|\n  |968 | cup|\n  |969 | eggnog|\n  |970 | alp|\n  |971 | bubble|\n  |972 | cliff, drop, drop-off|\n  |973 | coral reef|\n  |974 | geyser|\n  |975 | lakeside, lakeshore|\n  |976 | promontory, headland, head, foreland|\n  |977 | sandbar, sand bar|\n  |978 | seashore, coast, seacoast, sea-coast|\n  |979 | valley, vale|\n  |980 | volcano|\n  |981 | ballplayer, baseball player|\n  |982 | groom, bridegroom|\n  |983 | scuba diver|\n  |984 | rapeseed|\n  |985 | daisy|\n  |986 | yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum|\n  |987 | corn|\n  |988 | acorn|\n  |989 | hip, rose hip, rosehip|\n  |990 | buckeye, horse chestnut, conker|\n  |991 | coral fungus|\n  |992 | agaric|\n  |993 | gyromitra|\n  |994 | stinkhorn, carrion fungus|\n  |995 | earthstar|\n  |996 | hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa|\n  |997 | bolete|\n  |998 | ear, spike, capitulum|\n  |999 | toilet tissue, toilet paper, bathroom tissue|\n</details>\n\n### Data Splits\n\n|             |train  |validation| test  |\n|-------------|------:|---------:|------:|\n|# of examples|1281167|50000     |100000 |\n\n## Dataset Creation\n\n### Curation Rationale\n\nThe ImageNet project was inspired by two important needs in computer vision research. The first was the need to establish a clear North Star problem in computer vision. While the field enjoyed an abundance of important tasks to work on, from stereo vision to image retrieval, from 3D reconstruction to image segmentation, object categorization was recognized to be one of the most fundamental capabilities of both human and machine vision. Hence there was a growing demand for a high quality object categorization benchmark with clearly established evaluation metrics. Second, there was a critical need for more data to enable more generalizable machine learning methods. Ever since the birth of the digital era and the availability of web-scale data exchanges, researchers in these fields have been working hard to design more and more sophisticated algorithms to index, retrieve, organize and annotate multimedia data. But good research requires good resources. To tackle this problem at scale (think of your growing personal collection of digital images, or videos, or a commercial web search engine’s database), it was critical to provide researchers with a large-scale image database for both training and testing. The convergence of these two intellectual reasons motivated us to build ImageNet.\n\n### Source Data\n\n#### Initial Data Collection and Normalization\n\nInitial data for ImageNet image classification task consists of photographs collected from [Flickr](https://www.flickr.com) and other search engines, manually labeled with the presence of one of 1000 object categories. Constructing ImageNet was an effort to scale up an image classification dataset to cover most nouns in English using tens of millions of manually verified photographs [1](https://ieeexplore.ieee.org/abstract/document/5206848). The image classification task of ILSVRC came as a direct extension of this effort. A subset of categories and images was chosen and fixed to provide a standardized benchmark while the rest of ImageNet continued to grow.\n\n#### Who are the source language producers?\n\nWordNet synsets further quality controlled by human annotators. The images are from Flickr.\n\n### Annotations\n\n#### Annotation process\n\nThe annotation process of collecting ImageNet for image classification task is a three step process.\n\n1. Defining the 1000 object categories for the image classification task. These categories have evolved over the years.\n1. Collecting the candidate image for these object categories using a search engine.\n1. Quality control on the candidate images by using human annotators on Amazon Mechanical Turk (AMT) to make sure the image has the synset it was collected for.\n\nSee the section 3.1 in [1](https://arxiv.org/abs/1409.0575) for more details on data collection procedure and [2](https://ieeexplore.ieee.org/abstract/document/5206848) for general information on ImageNet.\n\n#### Who are the annotators?\n\nImages are automatically fetched from an image search engine based on the synsets and filtered using human annotators on Amazon Mechanical Turk. See [1](https://arxiv.org/abs/1409.0575) for more details.\n\n### Personal and Sensitive Information\n\nThe 1,000 categories selected for this subset contain only 3 people categories (scuba diver, bridegroom, and baseball player) while the full ImageNet contains 2,832 people categories under the person subtree (accounting for roughly 8.3% of the total images). This subset does contain the images of people without their consent. Though, the study in [[1]](https://image-net.org/face-obfuscation/) on obfuscating faces of the people in the ImageNet 2012 subset shows that blurring people's faces causes a very minor decrease in accuracy (~0.6%) suggesting that privacy-aware models can be trained on ImageNet. On larger ImageNet, there has been [an attempt](https://arxiv.org/abs/1912.07726) at filtering and balancing the people subtree in the larger ImageNet.\n\n## Considerations for Using the Data\n\n### Social Impact of Dataset\n\nThe ImageNet dataset has been very crucial in advancement of deep learning technology as being the standard benchmark for the computer vision models. The dataset aims to probe models on their understanding of the objects and has become the de-facto dataset for this purpose. ImageNet is still one of the major datasets on which models are evaluated for their generalization in computer vision capabilities as the field moves towards self-supervised algorithms. Please see the future section in [1](https://arxiv.org/abs/1409.0575) for a discussion on social impact of the dataset.\n\n### Discussion of Biases\n\n1. A [study](https://image-net.org/update-sep-17-2019.php) of the history of the multiple layers (taxonomy, object classes and labeling) of ImageNet and WordNet in 2019 described how bias is deeply embedded in most classification approaches for of all sorts of images.\n1. A [study](https://arxiv.org/abs/1811.12231) has also shown that ImageNet trained models are biased towards texture rather than shapes which in contrast with how humans do object classification. Increasing the shape bias improves the accuracy and robustness.\n1. Another [study](https://arxiv.org/abs/2109.13228) more potential issues and biases with the ImageNet dataset and provides an alternative benchmark for image classification task. The data collected contains humans without their consent.\n1. ImageNet data with face obfuscation is also provided at [this link](https://image-net.org/face-obfuscation/)\n1. A study on genealogy of ImageNet is can be found at [this link](https://journals.sagepub.com/doi/full/10.1177/20539517211035955) about the \"norms, values, and assumptions\" in ImageNet.\n1. See [this study](https://arxiv.org/abs/1912.07726) on filtering and balancing the distribution of people subtree in the larger complete ImageNet.\n\n### Other Known Limitations\n\n1. Since most of the images were collected from internet, keep in mind that some images in ImageNet might be subject to copyrights. See the following papers for more details: [[1]](https://arxiv.org/abs/2109.13228) [[2]](https://arxiv.org/abs/1409.0575) [[3]](https://ieeexplore.ieee.org/abstract/document/5206848).\n\n## Additional Information\n\n### Dataset Curators\n\nAuthors of [[1]](https://arxiv.org/abs/1409.0575) and [[2]](https://ieeexplore.ieee.org/abstract/document/5206848):\n\n- Olga Russakovsky\n- Jia Deng\n- Hao Su\n- Jonathan Krause\n- Sanjeev Satheesh\n- Wei Dong\n- Richard Socher\n- Li-Jia Li\n- Kai Li\n- Sean Ma\n- Zhiheng Huang\n- Andrej Karpathy\n- Aditya Khosla\n- Michael Bernstein\n- Alexander C Berg\n- Li Fei-Fei\n\n### Licensing Information\n\nIn exchange for permission to use the ImageNet database (the \"Database\") at Princeton University and Stanford University, Researcher hereby agrees to the following terms and conditions:\n\n1. Researcher shall use the Database only for non-commercial research and educational purposes.\n1. Princeton University and Stanford University make no representations or warranties regarding the Database, including but not limited to warranties of non-infringement or fitness for a particular purpose.\n1. Researcher accepts full responsibility for his or her use of the Database and shall defend and indemnify the ImageNet team, Princeton University, and Stanford University, including their employees, Trustees, officers and agents, against any and all claims arising from Researcher's use of the Database, including but not limited to Researcher's use of any copies of copyrighted images that he or she may create from the Database.\n1. Researcher may provide research associates and colleagues with access to the Database provided that they first agree to be bound by these terms and conditions.\n1. Princeton University and Stanford University reserve the right to terminate Researcher's access to the Database at any time.\n1. If Researcher is employed by a for-profit, commercial entity, Researcher's employer shall also be bound by these terms and conditions, and Researcher hereby represents that he or she is fully authorized to enter into this agreement on behalf of such employer.\n1. The law of the State of New Jersey shall apply to all disputes under this agreement.\n\n### Citation Information\n\n```bibtex\n@article{imagenet15russakovsky,\n    Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},\n    Title = { {ImageNet Large Scale Visual Recognition Challenge} },\n    Year = {2015},\n    journal   = {International Journal of Computer Vision (IJCV)},\n    doi = {10.1007/s11263-015-0816-y},\n    volume={115},\n    number={3},\n    pages={211-252}\n}\n```\n\n### Contributions\n\nThanks to [@apsdehal](https://github.com/apsdehal) for adding this dataset."
                }
              ]
            }
          }
        },
        "experiment_runs": [
          {
            "run_id": "proposed-iter1-ResNet-50-25M-ImageNet-1k",
            "method_name": "proposed",
            "model_name": "ResNet-50 (25M)",
            "dataset_name": "ImageNet-1k"
          },
          {
            "run_id": "comparative-1-iter1-ResNet-50-25M-ImageNet-1k",
            "method_name": "comparative-1",
            "model_name": "ResNet-50 (25M)",
            "dataset_name": "ImageNet-1k"
          }
        ]
      }
    ]
  }
}